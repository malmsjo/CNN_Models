{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten, LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv2D, Conv3D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image \n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "import tqdm\n",
    "from numpy import loadtxt\n",
    "from os import *\n",
    "from sklearn.utils import class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split('(\\d+)', text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(true,preds):\n",
    "    conf_matx = confusion_matrix(true, preds)\n",
    "    sns.heatmap(conf_matx, annot=True,annot_kws={\"size\": 12},fmt='g', cbar=False, cmap=plt.cm.Blues) #'viridis'\n",
    "    #plt.savefig('/home/jovyan/conf_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return conf_matx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model_history, model_name):\n",
    "    fig = plt.figure(figsize=(15,5), facecolor='w')\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(model_history.history['loss'])\n",
    "    ax.plot(model_history.history['val_loss'])\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Val'], loc='upper left')\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(model_history.history['accuracy'])\n",
    "    ax.plot(model_history.history['val_accuracy'])\n",
    "    ax.set(title=model_name + ': Model Accuracy; test='+ str(np.round(model_history.history['val_accuracy'][-1], 3)),\n",
    "           ylabel='Accuracy', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Val'], loc='upper left')\n",
    "    #plt.savefig('/home/jovyan/curve.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(path_data):\n",
    "    \n",
    "    p = '/home/jovyan/DATA_MASTER_PROJECT/IMG_A549_high_con/'\n",
    "    \n",
    "    \n",
    "    \n",
    "    pa_adr = p + 'ADR_tile/'\n",
    "    \n",
    "    pa_control = p + 'CONTROL_cropped/'\n",
    "    \n",
    "    pa_hrh = p + 'HRH_tile/'\n",
    "    \n",
    "    pa_dmso = p + 'DMSO_tile/'\n",
    "    \n",
    "    image_list = []\n",
    "    \n",
    "    \n",
    "       \n",
    "\n",
    "\n",
    "    for filename in sorted(path_data, key=natural_keys): \n",
    "        \n",
    "        if 'adr' in filename:\n",
    "            \n",
    "            im=cv2.imread(pa_adr + filename,1)\n",
    "\n",
    "            imarray = np.array(im)\n",
    "            \n",
    "\n",
    "            image_list.append(imarray)\n",
    "        \n",
    "            \n",
    "        if 'hrh' in filename:\n",
    "            \n",
    "            im=cv2.imread(pa_hrh + filename,1)\n",
    "\n",
    "            imarray = np.array(im)\n",
    "            \n",
    "\n",
    "            image_list.append(imarray)\n",
    "            \n",
    "        if 'dmso' in filename:\n",
    "            \n",
    "            im=cv2.imread(pa_dmso + filename,1)\n",
    "\n",
    "            imarray = np.array(im)\n",
    "            \n",
    "\n",
    "            image_list.append(imarray)\n",
    "\n",
    "\n",
    "\n",
    "    x_orig = np.reshape(image_list, (len(image_list), 256, 256, 3))\n",
    "\n",
    "    return x_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Images_path(path_data):\n",
    "    \n",
    "    p = '/scratch-shared/victor/IMG_A549/'\n",
    "    \n",
    "    \n",
    "    \n",
    "    pa_adr = p + 'ADR_tile/'\n",
    "    pa_hrh = p + 'HRH_tile/'\n",
    "    pa_dmso = p + 'DMSO_tile/'\n",
    "    \n",
    "    image_list = []\n",
    "    \n",
    "    \n",
    "       \n",
    "\n",
    "\n",
    "    for filename in path_data: \n",
    "        \n",
    "        if 'adr' in filename:\n",
    "            \n",
    "\n",
    "            image_list.append(pa_adr + filename)\n",
    "        \n",
    "            \n",
    "        if 'hrh' in filename:\n",
    "\n",
    "            image_list.append(pa_hrh + filename)\n",
    "            \n",
    "        if 'dmso' in filename:\n",
    "            \n",
    "\n",
    "            image_list.append(pa_dmso + filename)\n",
    "\n",
    "    return image_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label(y):\n",
    "    lab = []\n",
    "    for i in y:\n",
    "        if 'adr' in i:\n",
    "            lab.append(0)\n",
    "        if 'hrh' in i:\n",
    "            lab.append(1)\n",
    "        if 'dmso' in i:\n",
    "            lab.append(2)\n",
    "    return lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_count(x):\n",
    "    name_wel = []\n",
    "    for i in sorted(x, key = natural_keys):\n",
    "        name_wel.append(i.split('_')[0])\n",
    "\n",
    "    z = sorted(list(set(name_wel)))\n",
    "    r = list(range(len(z)))\n",
    "\n",
    "    num = []\n",
    "    for iz in range(len(z)):\n",
    "        count = 0\n",
    "        for i in sorted(x, key=natural_keys):\n",
    "            if z[iz] in i:\n",
    "                count += 1\n",
    "        num.append(count)\n",
    "    return list(zip(z, r, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages_LSTM(path_data,len_t_points):\n",
    "    \n",
    "\n",
    "    feat_list = []\n",
    "\n",
    "\n",
    "    for filename in sorted(glob.glob(path_data), key=natural_keys): \n",
    "        feat_list.append(np.load(filename))\n",
    "\n",
    "    x_orig = np.reshape(feat_list, (len(feat_list),len_t_points, 64))\n",
    "\n",
    "    return x_orig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(data_set):\n",
    "    fe = return_count(data_set)\n",
    "    leb = creat_label(fe)\n",
    "    y = np.array(list(leb))\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels_LSTM(data_set):\n",
    "    fe = return_count_LSTM(data_set)\n",
    "    leb = creat_label(fe)\n",
    "    y = np.array(list(leb))\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_count_LSTM(x):\n",
    "    name_wel = []\n",
    "    for _,_,i in os.walk(x):\n",
    "        for f in i:\n",
    "            name_wel.append(f.split('_')[2])\n",
    "\n",
    "    z = sorted(list(set(name_wel)), key=natural_keys)\n",
    "    r = list(range(len(z)))\n",
    "\n",
    "    num = []\n",
    "    for iz in range(len(z)):\n",
    "        count = 0\n",
    "        for i in sorted(name_wel, key=natural_keys):\n",
    "            if z[iz] in i:\n",
    "                count += 1\n",
    "        num.append(count)\n",
    "    return list(zip(z, r, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_label(y):\n",
    "    labels = []\n",
    "    for ix, _ in enumerate(y):\n",
    "        \n",
    "        if y[ix][0] == 'adr':\n",
    "        \n",
    "            labels.append([[y[ix][0],0]] * y[ix][2])\n",
    "        \n",
    "        if y[ix][0] == 'hrh':\n",
    "            \n",
    "            labels.append([[y[ix][0],1]] * y[ix][2])\n",
    "        \n",
    "            \n",
    "        if y[ix][0] == 'dmso':\n",
    "            \n",
    "            labels.append([[y[ix][0],2]] * y[ix][2])\n",
    "    \n",
    "    ler = [i for sub in labels for i in sub ]\n",
    "    \n",
    "    _, lab= zip(*ler)\n",
    "\n",
    "    \n",
    "    return lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_step_acc(tes_data, x):\n",
    "\n",
    "    results = []            \n",
    "\n",
    "    x_test = loadImages(tes_data)\n",
    "    y_test = make_labels(tes_data)\n",
    "    \n",
    "    y_test_1 = keras.utils.to_categorical(y_test,num_classes=3)\n",
    "    \n",
    "    #x_test = resize(x_test)\n",
    "    x_test = preprocess_input(x_test)\n",
    "\n",
    "    scores = x.evaluate(x_test, y_test_1, verbose = 1)\n",
    "    results.append(scores[1]*100)\n",
    "    \n",
    "    test_preds = m4.predict(x_test)\n",
    "\n",
    "    preds_df = pd.DataFrame(test_preds)\n",
    "    predicted_labels = preds_df.idxmax(axis=1)\n",
    "    draw_confusion_matrix(y_test, predicted_labels)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_mean_acc(result_cv, string_well):\n",
    "    \n",
    "    l_drug = string_well*3\n",
    "\n",
    "    acc_mean_cv = []\n",
    "\n",
    "    for i in result_cv:\n",
    "        acc_mean_cv.append(np.mean(i))\n",
    "        \n",
    "    cv_drug = list(zip(acc_mean_cv, l_drug))\n",
    "    \n",
    "    res = sorted(cv_drug, key = lambda x: x[1])\n",
    "    a , b = zip(*res)\n",
    "    \n",
    "    a = list(a)\n",
    "    \n",
    "    s = list(np.array_split(a, len(string_well)))\n",
    "    \n",
    "    cv_score_acc = []\n",
    "    \n",
    "    for ix, i in enumerate(s):\n",
    "        s1 = list(s[ix])\n",
    "        \n",
    "        cv_score_acc.append(np.mean(s1))\n",
    "        \n",
    "    return list(zip(cv_score_acc, string_well))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA FOR LSTM PART\n",
    "\n",
    "p_feat = '/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/'\n",
    "train_data = p_feat + 'features_train/*.npy'\n",
    "val_data = p_feat + 'features_validation/*.npy'\n",
    "tes_data= p_feat + 'features_test/*.npy'\n",
    "\n",
    "y_tra_path = '/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/features_train/'\n",
    "y_tes_path = '/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/features_test/'\n",
    "y_val_path = '/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/features_validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "met = ['F10']\n",
    "mid = ['D5']\n",
    "oxy = ['F6']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cycl = ['C4']\n",
    "dime =  ['F7']\n",
    "cypr  = ['G9']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_well_adr = [met,mid,oxy]\n",
    "\n",
    "tot_well_hrh = [cycl, dime, cypr]\n",
    "\n",
    "string_well_adr = ['met', 'mid', 'oxy']\n",
    "\n",
    "string_well_hrh = ['cycl', 'dime', 'cypr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_well = []\n",
    "string_well = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'ADR' # FOR TEST SET\n",
    "b = 'HRH' # FOR REST\n",
    "\n",
    "\n",
    "if a == 'HRH':\n",
    "    tot_well = tot_well_hrh\n",
    "    string_well = string_well_hrh\n",
    "    \n",
    "if a == 'ADR':\n",
    "    tot_well = tot_well_adr\n",
    "    string_well = string_well_adr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_results_accuracy = []\n",
    "\n",
    "results_lstm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_points = list(map(str, range(0,34)))\n",
    "\n",
    "new_time = []\n",
    "for i in time_points:\n",
    "    r = '_' + i + '.'\n",
    "    new_time.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = list(np.random.randint(1,1000,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|█████     | 1/2 [00:02<00:02,  2.42s/it]\u001b[A\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.99s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22848 validated image filenames belonging to 3 classes.\n",
      "Found 15232 validated image filenames belonging to 3 classes.\n",
      "Model_loaded\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 357 steps, validate for 238 steps\n",
      "Epoch 1/50\n",
      "357/357 [==============================] - 109s 306ms/step - loss: 0.8468 - accuracy: 0.5699 - val_loss: 1.0493 - val_accuracy: 0.5315\n",
      "Epoch 2/50\n",
      "357/357 [==============================] - 104s 292ms/step - loss: 0.6209 - accuracy: 0.6876 - val_loss: 1.3111 - val_accuracy: 0.4426\n",
      "Epoch 3/50\n",
      "357/357 [==============================] - 105s 293ms/step - loss: 0.5025 - accuracy: 0.7483 - val_loss: 1.5476 - val_accuracy: 0.4412\n",
      "Epoch 4/50\n",
      "357/357 [==============================] - 105s 293ms/step - loss: 0.4287 - accuracy: 0.7901 - val_loss: 1.6733 - val_accuracy: 0.4626\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 357 steps, validate for 238 steps\n",
      "Epoch 1/300\n",
      "357/357 [==============================] - 222s 623ms/step - loss: 0.4045 - accuracy: 0.8027 - val_loss: 5.2527 - val_accuracy: 0.2015\n",
      "Epoch 2/300\n",
      "357/357 [==============================] - 218s 612ms/step - loss: 0.2632 - accuracy: 0.8694 - val_loss: 3.9863 - val_accuracy: 0.4506\n",
      "Epoch 3/300\n",
      "357/357 [==============================] - 219s 613ms/step - loss: 0.2080 - accuracy: 0.9034 - val_loss: 2.2914 - val_accuracy: 0.4988\n",
      "Epoch 4/300\n",
      "357/357 [==============================] - 219s 612ms/step - loss: 0.1613 - accuracy: 0.9280 - val_loss: 2.8042 - val_accuracy: 0.2992\n",
      "Epoch 5/300\n",
      "357/357 [==============================] - 219s 614ms/step - loss: 0.1287 - accuracy: 0.9470 - val_loss: 2.0427 - val_accuracy: 0.4982\n",
      "Epoch 6/300\n",
      " 17/357 [>.............................] - ETA: 2:49 - loss: 0.0963 - accuracy: 0.9648"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.6/site-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-1143e18d7959>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    162\u001b[0m         m4_h = m4.fit(train_generator, steps_per_epoch = STEP_SIZE_TRAIN, validation_data = val_generator, \n\u001b[1;32m    163\u001b[0m                       \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTEP_SIZE_VALID\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                       class_weight = weights, epochs=epochs, verbose = 1)\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    820\u001b[0m   \"\"\"\n\u001b[1;32m    821\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \"\"\"\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for num_ix, rand_num in enumerate(rand):\n",
    "    for index_t_well, _ in tqdm.tqdm(enumerate(tot_well)):\n",
    "\n",
    "\n",
    "\n",
    "        path_test = '/scratch-shared/victor/IMG_A549/{}_tile/'.format(a)\n",
    "\n",
    "        # NAME OF THE WELLS CORRESPONDING TO THE DRUG THAT YOU WANT IN THE TEST SET \n",
    "\n",
    "        wells_drug = [tot_well[index_t_well][0]] \n",
    "\n",
    "        test = []\n",
    "\n",
    "        for _,_, filenames in os.walk(path_test):\n",
    "\n",
    "            for filename in sorted(filenames, key = natural_keys):\n",
    "\n",
    "                for w in wells_drug:\n",
    "                    for t in new_time:\n",
    "                        if '{}'.format(w) in filename and '{}tiff'.format(t) in filename:\n",
    "                            test.append(filename)\n",
    "\n",
    "        groups_list = ['{}'.format(a), '{}'.format(b)]\n",
    "\n",
    "        fileds_of_view = ['1','2','3','4','5']\n",
    "\n",
    "        field_train, field_val = train_test_split(fileds_of_view, test_size=0.4, random_state=rand_num)\n",
    "\n",
    "\n",
    "        train = []\n",
    "\n",
    "        validation = []\n",
    "\n",
    "        group_compounds = []\n",
    "        \n",
    "        all_wells = tot_well_adr + tot_well_hrh\n",
    "        all_wells.remove(wells_drug)\n",
    "        allw = [j for i in all_wells for j in i]\n",
    "\n",
    "        for group in tqdm.tqdm(groups_list):\n",
    "\n",
    "            pa = '/scratch-shared/victor/IMG_A549/{}_tile/'.format(group)\n",
    "\n",
    "            for _,_, filenames in os.walk(pa):\n",
    "\n",
    "                for filename in sorted(filenames, key = natural_keys):\n",
    "\n",
    "                    for t in new_time:\n",
    "                        \n",
    "                        for al in allw:\n",
    "\n",
    "                            if '_{}-'.format(al) in filename  and '{}tiff'.format(t) in filename:\n",
    "\n",
    "                                group_compounds.append(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        pp = '/scratch-shared/victor/IMG_A549/DMSO_tile/'\n",
    "\n",
    "        dm = []\n",
    "        for _,_, filenames in os.walk(pp): \n",
    "\n",
    "            for f in filenames:\n",
    "                dm.append(f)\n",
    "\n",
    "\n",
    "        group_compounds = group_compounds + dm\n",
    "\n",
    "        for i in group_compounds:\n",
    "\n",
    "            for f in field_train:\n",
    "                if '-{}_'.format(f) in i:\n",
    "                    train.append(i)\n",
    "\n",
    "\n",
    "            for v in field_val:\n",
    "                if '-{}_'.format(v) in i:\n",
    "                    validation.append(i)\n",
    "                    \n",
    "                    \n",
    "        \n",
    "        train_leb = label(train)\n",
    "        val_leb = label(validation)\n",
    "        test_leb = label(test)\n",
    "        train_cnn = Images_path(train)\n",
    "        validation_cnn = Images_path(validation)\n",
    "        \n",
    "\n",
    "        data_train = {'id': train_cnn, 'labels':train_leb}\n",
    "        data_val = {'id': validation_cnn, 'labels':val_leb}\n",
    "\n",
    "        df_train = pd.DataFrame(data_train, columns = ['id', 'labels'])\n",
    "        df_val = pd.DataFrame(data_val, columns = ['id', 'labels'])\n",
    "\n",
    "        df_train['labels'] = df_train['labels'].astype(str)\n",
    "        df_val['labels'] = df_val['labels'].astype(str)\n",
    "\n",
    "        datagen=ImageDataGenerator(preprocessing_function = preprocess_input)\n",
    "\n",
    "        train_generator=datagen.flow_from_dataframe(dataframe=df_train, directory = None , x_col=\"id\", y_col=\"labels\",color_mode=\"rgb\", \n",
    "                                                    class_mode=\"categorical\", target_size=(256, 256), batch_size=64)\n",
    "\n",
    "        val_generator=datagen.flow_from_dataframe(dataframe=df_val, directory = None , x_col=\"id\", y_col=\"labels\",color_mode=\"rgb\", \n",
    "                                                    class_mode=\"categorical\", target_size=(256, 256), batch_size=64)\n",
    "\n",
    "\n",
    "        weights = class_weight.compute_class_weight('balanced', np.unique(train_leb), train_leb)\n",
    "        \n",
    "        weights = dict(enumerate(weights))\n",
    "        \n",
    "\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=3)\n",
    "\n",
    "        pretrained_model = VGG16(weights='imagenet',include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "        base_model = Model(inputs=pretrained_model.input, outputs=pretrained_model.get_layer('block3_pool').output)\n",
    "        \n",
    "        print('Model_loaded')\n",
    "\n",
    "        m4 = Sequential()\n",
    "        m4.add(base_model)\n",
    "\n",
    "\n",
    "        m4.add(BatchNormalization())\n",
    "        m4.add(GlobalAveragePooling2D())\n",
    "        m4.add(Dense(128, activation='relu'))\n",
    "        m4.add(BatchNormalization())\n",
    "        m4.add(Dense(64, activation='relu'))\n",
    "        m4.add(BatchNormalization())\n",
    "        m4.add(Activation('relu'))\n",
    "        m4.add(Dense(3,activation='softmax'))\n",
    "\n",
    "\n",
    "        base_model.trainable = False\n",
    "\n",
    "        opt = keras.optimizers.Adam(lr=1e-3)\n",
    "\n",
    "        m4.compile(loss= keras.losses.categorical_crossentropy, optimizer=opt, metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "        epochs = 50\n",
    "        \n",
    "        STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "        STEP_SIZE_VALID=val_generator.n//val_generator.batch_size\n",
    "\n",
    "        m4_h = m4.fit(train_generator, steps_per_epoch = STEP_SIZE_TRAIN, validation_data = val_generator, \n",
    "                      validation_steps = STEP_SIZE_VALID , callbacks = [es],\n",
    "                      class_weight = weights, epochs=epochs, verbose = 1)\n",
    "\n",
    "\n",
    "        base_model.trainable = True\n",
    "\n",
    "        opt = keras.optimizers.Adam(lr=1e-4)\n",
    "\n",
    "        m4.compile(loss= keras.losses.categorical_crossentropy, optimizer=opt, metrics = ['accuracy'])\n",
    "\n",
    "        epochs = 300\n",
    "\n",
    "        m4_h = m4.fit(train_generator, steps_per_epoch = STEP_SIZE_TRAIN, validation_data = val_generator, \n",
    "                      validation_steps = STEP_SIZE_VALID , callbacks = [es],\n",
    "                      class_weight = weights, epochs=epochs, verbose = 1)\n",
    "\n",
    "        l = []\n",
    "        for t in new_time:\n",
    "            for i in test:\n",
    "                if t in i:\n",
    "                    l.append((i))\n",
    "\n",
    "\n",
    "        grouped = {}\n",
    "        for elem in l:\n",
    "            key = elem.split('.tiff')[0].split('_')[5]\n",
    "            grouped.setdefault(key, []).append(elem)\n",
    "        grouped = grouped.values()\n",
    "\n",
    "        test_data = list(grouped)\n",
    "\n",
    "        r = []\n",
    "\n",
    "        for ix ,_ in enumerate(test_data):\n",
    "            r.append(time_step_acc(test_data[ix],m4))\n",
    "\n",
    "        plt.plot(time_points,r)\n",
    "        plt.savefig('/home/jovyan/IMG_CNN_FINAL/{}_accuracy.png'.format(string_well[index_t_well]))\n",
    "\n",
    "        tot_results_accuracy.append(r)\n",
    "        \n",
    "        for i, layer in enumerate(m4.layers):\n",
    "            layer._name = 'layer_' + str(i)\n",
    "\n",
    "\n",
    "\n",
    "        lstm_model = Model(inputs=m4.input, outputs=m4.get_layer('layer_6').output)\n",
    "\n",
    "        del m4\n",
    "        K.clear_session()\n",
    "        \n",
    "        data_name = [train,test,validation]\n",
    "\n",
    "        feat_name = ['train', 'test', 'validation']\n",
    "\n",
    "        for index_name, _ in enumerate(data_name):\n",
    "\n",
    "            path =  data_name[index_name]\n",
    "\n",
    "            name_well = []\n",
    "\n",
    "            for i in path:\n",
    "                name_well.append(i.split('_id')[0])\n",
    "\n",
    "            wells = list(set(name_well))\n",
    "            wells\n",
    "\n",
    "            for w in wells:\n",
    "\n",
    "                time = []\n",
    "\n",
    "\n",
    "                for filename in sorted(path, key = natural_keys):\n",
    "                    if w in filename: #PAY ATTENTION ID THE IMAGE IS A TIFF OR PNG IMAGE #########\n",
    "                        time.append(filename)\n",
    "\n",
    "                data_id = {}\n",
    "                n_id = []\n",
    "                w_n = []\n",
    "\n",
    "                for i in time:\n",
    "                    t = i.split('_id_')[1].split('time_')[0]\n",
    "                    f = i.split('_id_')[0].split('time_')[0]\n",
    "                    n_id.append(t)\n",
    "                    w_n.append(f)\n",
    "\n",
    "                id_cell = set(n_id)\n",
    "\n",
    "\n",
    "                for ix, i in enumerate(sorted(id_cell, key = natural_keys)):\n",
    "\n",
    "                    id_name = []\n",
    "                    dict_1 = {}\n",
    "\n",
    "                    for t in time:\n",
    "                        if 'id_{}'.format(i) in t:\n",
    "                            id_name.append(t)\n",
    "\n",
    "                    d = {'id':id_name}\n",
    "                    data = pd.DataFrame(d)\n",
    "\n",
    "                    dict_1[ix]=data \n",
    "                    data_id.update(dict_1) \n",
    "\n",
    "\n",
    "                len_id = [i for i, j in data_id.items()]\n",
    "\n",
    "                for le in len_id:    \n",
    "\n",
    "\n",
    "                    e = pd.DataFrame(data_id[le])\n",
    "\n",
    "                    coords = e.values.tolist()\n",
    "                    id_cells = []\n",
    "                    for i in coords:\n",
    "                        for j in i:\n",
    "                            id_cells.append(j)\n",
    "                            \n",
    "                    \n",
    "\n",
    "                    x_orig = loadImages(id_cells)\n",
    "                    \n",
    "                    x_orig = preprocess_input(x_orig)\n",
    "                    output = lstm_model.predict(x_orig)\n",
    "                    np.save('/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/features_{}/features_well_{}_id_{}.npy'.format(feat_name[index_name],w_n[0], le), output)\n",
    "            print('Saved_feature_{}'.format(feat_name[index_name]))\n",
    "\n",
    "\n",
    "        x_train_lstm = loadImages_LSTM(train_data, len(time_points))\n",
    "        y_train_lstm = make_labels_LSTM(y_tra_path)\n",
    "\n",
    "        x_test_lstm = loadImages_LSTM(tes_data, len(time_points))\n",
    "        y_test_lstm = make_labels_LSTM(y_tes_path)\n",
    "\n",
    "        x_val_lstm = loadImages_LSTM(val_data, len(time_points))\n",
    "        y_val_lstm = make_labels_LSTM(y_val_path)\n",
    "\n",
    "        weights_lstm = class_weight.compute_class_weight('balanced', np.unique(y_train_lstm),y_train_lstm)\n",
    "        \n",
    "        weights_lstm = dict(enumerate(weights_lstm))\n",
    "        \n",
    "        y_train_lstm = keras.utils.to_categorical(y_train_lstm, num_classes = 3)\n",
    "        y_test_1_lstm = keras.utils.to_categorical(y_test_lstm, num_classes= 3)\n",
    "        y_val_lstm = keras.utils.to_categorical(y_val_lstm, num_classes = 3)\n",
    "\n",
    "\n",
    "        m = Sequential()\n",
    "        m.add(LSTM(32, input_shape = (x_train_lstm.shape[1],x_train_lstm.shape[2])))\n",
    "        m.add(Dropout(0.2))\n",
    "        m.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "        opt_lstm = keras.optimizers.Adam(lr=1e-4)\n",
    "\n",
    "        m.compile(loss= keras.losses.categorical_crossentropy, optimizer=opt, metrics = ['accuracy'])\n",
    "\n",
    "        epochs = 300\n",
    "\n",
    "        m_h = m.fit(x_train_lstm,y_train_lstm,\n",
    "\n",
    "                         callbacks = [es],\n",
    "\n",
    "                        epochs=epochs,\n",
    "                        validation_data = (x_val_lstm,y_val_lstm), \n",
    "\n",
    "                        class_weight = weights_lstm)\n",
    "\n",
    "\n",
    "        scores_lstm = m.evaluate(x_test_lstm, y_test_1_lstm)\n",
    "        results_lstm.append([scores_lstm[1]*100, string_well[index_t_well]])\n",
    "        \n",
    "        del m\n",
    "        K.clear_session()\n",
    "\n",
    "        # DELITE FILES IN FEATURE VECTOR FOLDERS\n",
    "\n",
    "        folders = glob.glob('/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/*')\n",
    "\n",
    "        for fo in folders:\n",
    "            file = glob.glob(f'{fo}/*')\n",
    "            for f in file:\n",
    "                os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ACCURACY SCORE AVERAGE FOR CNN\n",
    "cv_s = cv_mean_acc(tot_results_accuracy, string_well)\n",
    "cv_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_s_mean,_ = zip(*cv_s)\n",
    "\n",
    "m_cv = np.mean(list(cv_s_mean))\n",
    "m_cv    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT OF MEAN ACCURACY FOR EVERY TIME POINT CNN\n",
    "\n",
    "l_drug = string_well*3\n",
    "\n",
    "acc_plot = []\n",
    "\n",
    "for i in tot_results_accuracy:\n",
    "    acc_plot.append(i)\n",
    "\n",
    "cv_plot = list(zip(acc_plot, l_drug))\n",
    "\n",
    "res_plot = sorted(cv_plot, key = lambda x: x[1])\n",
    "\n",
    "a , b = zip(*res_plot)\n",
    "    \n",
    "a = list(a)\n",
    "\n",
    "s = list(np.array_split(a, len(string_well)))\n",
    "\n",
    "cv_plot = []\n",
    "\n",
    "for ix, i in enumerate(s):\n",
    "    s1 = list(s[ix])\n",
    "    \n",
    "    cv_plot.append(np.average(s1, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "for i in cv_plot:\n",
    "    \n",
    "    plt.plot(time_points, i)\n",
    "    plt.show\n",
    "    plt.savefig('/home/jovyan/cv_score.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lstm = sorted(results_lstm, key=lambda x: x[1])\n",
    "r_lstm , _ = zip(*results_lstm)\n",
    "    \n",
    "r_lstm = list(r_lstm)\n",
    "\n",
    "re_lstm = list(np.array_split(r_lstm, len(string_well)))\n",
    "\n",
    "cv_lstm = []\n",
    "\n",
    "for ix, i in enumerate(re_lstm):\n",
    "    r1 = list(re_lstm[ix])\n",
    "    cv_lstm.append(np.mean(r1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lstm = list(zip(cv_lstm, string_well))\n",
    "cv_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_l_mean,_ = zip(*cv_lstm)\n",
    "\n",
    "m_cv_l = np.mean(list(cv_l_mean))\n",
    "m_cv_l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE RUNNING AGAIN\n",
    "\n",
    "folders = glob.glob('/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/*')\n",
    "\n",
    "for fo in folders:\n",
    "    file = glob.glob(f'{fo}/*')\n",
    "    for f in file:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_l_mean,_ = zip(*cv_lstm)\n",
    "\n",
    "m_cv_l = np.mean(list(cv_l_mean))\n",
    "m_cv_l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_cv_l =  np.std(list(cv_l_mean))\n",
    "sd_cv_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = np.mean(cv_plot, axis = 0)\n",
    "me = me.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = np.std(cv_plot, axis = 0)\n",
    "sd = sd.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('ytick', labelsize=18)\n",
    "ax = plt.figure(figsize=(13,8), facecolor='w').gca()\n",
    "ax.plot(me)\n",
    "ax.fill_between(time_points, me - sd, me + sd, alpha = 0.5)\n",
    "\n",
    "plt.errorbar(31.5, m_cv_l, sd_cv_l, linestyle='None', marker='^', markersize=12)\n",
    "\n",
    "plt.savefig('/home/jovyan/md_sd_score.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_stat = [i for i in cv_s_mean]\n",
    "lstm_stat = [i for i in cv_l_mean]\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "stat, p = wilcoxon(cnn_stat, lstm_stat)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "\tprint('Same distribution (fail to reject H0)')\n",
    "else:\n",
    "\tprint('Different distribution (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
