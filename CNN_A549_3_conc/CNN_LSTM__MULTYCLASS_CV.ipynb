{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten, LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv2D, Conv3D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image \n",
    "import seaborn as sns\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "import tqdm\n",
    "from numpy import loadtxt\n",
    "from os import *\n",
    "from sklearn.utils import class_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "def natural_keys(text):\n",
    "    return [atoi(c) for c in re.split('(\\d+)', text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_confusion_matrix(true,preds,a):\n",
    "    conf_matx = confusion_matrix(true, preds)\n",
    "    sns.heatmap(conf_matx, annot=True,annot_kws={\"size\": 12},fmt='g', cbar=False, cmap=plt.cm.Blues) #'viridis'\n",
    "    plt.savefig('/home/jovyan/conf_matrix_{}.png'.format(a))\n",
    "    plt.show()\n",
    "    \n",
    "    return conf_matx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(model_history, model_name):\n",
    "    fig = plt.figure(figsize=(15,5), facecolor='w')\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.plot(model_history.history['loss'])\n",
    "    ax.plot(model_history.history['val_loss'])\n",
    "    ax.set(title=model_name + ': Model loss', ylabel='Loss', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Val'], loc='upper left')\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.plot(model_history.history['accuracy'])\n",
    "    ax.plot(model_history.history['val_accuracy'])\n",
    "    ax.set(title=model_name + ': Model Accuracy; test='+ str(np.round(model_history.history['val_accuracy'][-1], 3)),\n",
    "           ylabel='Accuracy', xlabel='Epoch')\n",
    "    ax.legend(['Train', 'Val'], loc='upper left')\n",
    "    #plt.savefig('/home/jovyan/curve.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(x):\n",
    "    rescaled = []\n",
    "\n",
    "    for i in x:\n",
    "\n",
    "        scale_percent = 140 # percent of original size\n",
    "        width = int(i.shape[1] / (scale_percent / 100))\n",
    "        height = int(i.shape[0] / (scale_percent / 100))\n",
    "        dim = (width, height)\n",
    "        resized = cv2.resize(i, dim, interpolation = cv2.INTER_LANCZOS4)\n",
    "        rescaled.append(resized)\n",
    "\n",
    "    x_orig = np.reshape( rescaled, (len( rescaled), resized.shape[1], resized.shape[1], 3))\n",
    "\n",
    "    return x_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages(path_data):\n",
    "    \n",
    "    p = '/home/jovyan/DATA_MASTER_PROJECT/Check_DIFF_T0_T97/'\n",
    "    \n",
    "    \n",
    "    \n",
    "    pa_adr = p + 'ADR_cropped/'\n",
    "    \n",
    "    pa_control = p + 'CONTROL_cropped/'\n",
    "    \n",
    "    pa_hrh = p + 'HRH_cropped/'\n",
    "    \n",
    "    image_list = []\n",
    "    \n",
    "    \n",
    "       \n",
    "\n",
    "\n",
    "    for filename in sorted(path_data, key=natural_keys): \n",
    "        \n",
    "        if 'adr' in filename:\n",
    "            \n",
    "            im=cv2.imread(pa_adr + filename)\n",
    "\n",
    "            imarray = np.array(im)\n",
    "            \n",
    "\n",
    "            image_list.append(imarray)\n",
    "            \n",
    "        if 'control' in filename:\n",
    "            \n",
    "            im=cv2.imread(pa_control + filename)\n",
    "\n",
    "            imarray = np.array(im)\n",
    "            \n",
    "\n",
    "            image_list.append(imarray)\n",
    "            \n",
    "        if 'hrh' in filename:\n",
    "            \n",
    "            im=cv2.imread(pa_hrh + filename)\n",
    "\n",
    "            imarray = np.array(im)\n",
    "            \n",
    "\n",
    "            image_list.append(imarray)\n",
    "\n",
    "\n",
    "\n",
    "    x_orig = np.reshape(image_list, (len(image_list), 90, 90, 3))\n",
    "\n",
    "    return x_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_count(x):\n",
    "    name_wel = []\n",
    "    for i in sorted(x, key = natural_keys):\n",
    "        name_wel.append(i.split('_')[0])\n",
    "\n",
    "    z = sorted(list(set(name_wel)))\n",
    "    r = list(range(len(z)))\n",
    "\n",
    "    num = []\n",
    "    for iz in range(len(z)):\n",
    "        count = 0\n",
    "        for i in sorted(x, key=natural_keys):\n",
    "            if z[iz] in i:\n",
    "                count += 1\n",
    "        num.append(count)\n",
    "    return list(zip(z, r, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadImages_LSTM(path_data):\n",
    "    \n",
    "\n",
    "    feat_list = []\n",
    "\n",
    "\n",
    "    for filename in sorted(glob.glob(path_data), key=natural_keys): \n",
    "        feat_list.append(np.load(filename))\n",
    "\n",
    "    x_orig = np.reshape(feat_list, (len(feat_list),32, 64))\n",
    "\n",
    "    return x_orig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels(data_set):\n",
    "    fe = return_count(data_set)\n",
    "    leb = creat_label(fe)\n",
    "    y = np.array(list(leb))\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_labels_LSTM(data_set):\n",
    "    fe = return_count_LSTM(data_set)\n",
    "    leb = creat_label(fe)\n",
    "    y = np.array(list(leb))\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_count_LSTM(x):\n",
    "    name_wel = []\n",
    "    for _,_,i in os.walk(x):\n",
    "        for f in i:\n",
    "            name_wel.append(f.split('_')[2])\n",
    "\n",
    "    z = sorted(list(set(name_wel)), key=natural_keys)\n",
    "    r = list(range(len(z)))\n",
    "\n",
    "    num = []\n",
    "    for iz in range(len(z)):\n",
    "        count = 0\n",
    "        for i in sorted(name_wel, key=natural_keys):\n",
    "            if z[iz] in i:\n",
    "                count += 1\n",
    "        num.append(count)\n",
    "    return list(zip(z, r, num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creat_label(y):\n",
    "    labels = []\n",
    "    for ix, _ in enumerate(y):\n",
    "        \n",
    "        if y[ix][0] == 'adr':\n",
    "        \n",
    "            labels.append([[y[ix][0],0]] * y[ix][2])\n",
    "        \n",
    "        if y[ix][0] == 'hrh':\n",
    "            \n",
    "            labels.append([[y[ix][0],1]] * y[ix][2])\n",
    "        \n",
    "        if y[ix][0] == 'control':\n",
    "            \n",
    "            labels.append([[y[ix][0],2]] * y[ix][2])\n",
    "    \n",
    "    ler = [i for sub in labels for i in sub ]\n",
    "    \n",
    "    _, lab= zip(*ler)\n",
    "\n",
    "    \n",
    "    return lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_step_acc(t_data, x):\n",
    "\n",
    "    results = []            \n",
    "\n",
    "    x_test = loadImages(t_data)\n",
    "    y_test = make_labels(t_data)\n",
    "    \n",
    "    y_test_1 = keras.utils.to_categorical(y_test,num_classes=3)\n",
    "\n",
    "    x_test = resize(x_test)\n",
    "    x_test = preprocess_input(x_test)\n",
    "\n",
    "    scores = x.evaluate(x_test, y_test_1, verbose = 1)\n",
    "    results.append(scores[1]*100)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_mean_acc(result_cv, string_well):\n",
    "    \n",
    "    l_drug = string_well*3\n",
    "\n",
    "    acc_mean_cv = []\n",
    "\n",
    "    for i in result_cv:\n",
    "        acc_mean_cv.append(np.mean(i))\n",
    "        \n",
    "    cv_drug = list(zip(acc_mean_cv, l_drug))\n",
    "    \n",
    "    res = sorted(cv_drug, key = lambda x: x[1])\n",
    "    a , b = zip(*res)\n",
    "    \n",
    "    a = list(a)\n",
    "    \n",
    "    s = list(np.array_split(a, 5))\n",
    "    \n",
    "    cv_score_acc = []\n",
    "    \n",
    "    for ix, i in enumerate(s):\n",
    "        s1 = list(s[ix])\n",
    "        \n",
    "        cv_score_acc.append(np.mean(s1))\n",
    "        \n",
    "    return list(zip(cv_score_acc, string_well))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA FOR LSTM PART\n",
    "\n",
    "p_feat = '/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/'\n",
    "train_data = p_feat + 'features_train/*.npy'\n",
    "val_data = p_feat + 'features_validation/*.npy'\n",
    "tes_data= p_feat + 'features_test/*.npy'\n",
    "\n",
    "y_tra_path = '/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/features_train/'\n",
    "y_tes_path = '/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/features_test/'\n",
    "y_val_path = '/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/features_validation/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = ['C6', 'F11']\n",
    "leb = ['D11', 'G4']\n",
    "mep = ['F2', 'G10']\n",
    "met = ['G5', 'B10']\n",
    "oxy = ['G3', 'B8']\n",
    "\n",
    "cyc = ['E4', 'G6']\n",
    "dox = ['G8', 'D10']\n",
    "olo = ['E7', 'B7']\n",
    "ket = ['E10', 'B11']\n",
    "orp = ['D8', 'B2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_results_accuracy = []\n",
    "\n",
    "results_lstm = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_well_adr = [mid, leb, mep, met, oxy]\n",
    "tot_well_hrh = [cyc, dox, olo, ket, orp]\n",
    "\n",
    "string_well_adr = ['mid', 'leb', 'mep', 'met', 'oxy']\n",
    "string_well_hrh = ['cyc', 'dox', 'olo', 'ket', 'orp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_well = []\n",
    "string_well = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'HRH' # FOR TEST SET\n",
    "b = 'ADR' # FOR REST\n",
    "c = 'CONTROL'\n",
    "\n",
    "if a == 'HRH':\n",
    "    tot_well = tot_well_hrh\n",
    "    string_well = string_well_hrh\n",
    "    \n",
    "if a == 'ADR':\n",
    "    tot_well = tot_well_adr\n",
    "    string_well = string_well_adr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = list(np.random.randint(1,1000,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:05<00:11,  5.52s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:11<00:05,  5.59s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:13<00:00,  4.65s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 618.1015625 steps, validate for 180.4453125 steps\n",
      "Epoch 1/300\n",
      "619/618 [==============================] - 21s 34ms/step - loss: 1.1251 - accuracy: 0.4211 - val_loss: 1.0695 - val_accuracy: 0.4423\n",
      "Epoch 2/300\n",
      "619/618 [==============================] - 17s 28ms/step - loss: 1.0006 - accuracy: 0.5032 - val_loss: 1.0653 - val_accuracy: 0.4427\n",
      "Epoch 3/300\n",
      "619/618 [==============================] - 17s 28ms/step - loss: 0.9728 - accuracy: 0.5247 - val_loss: 1.0657 - val_accuracy: 0.4444\n",
      "Epoch 4/300\n",
      "619/618 [==============================] - 18s 29ms/step - loss: 0.9536 - accuracy: 0.5400 - val_loss: 1.0639 - val_accuracy: 0.4478\n",
      "Epoch 5/300\n",
      "619/618 [==============================] - 18s 28ms/step - loss: 0.9382 - accuracy: 0.5520 - val_loss: 1.0671 - val_accuracy: 0.4505\n",
      "Epoch 6/300\n",
      "619/618 [==============================] - 18s 28ms/step - loss: 0.9249 - accuracy: 0.5611 - val_loss: 1.0682 - val_accuracy: 0.4496\n",
      "Epoch 7/300\n",
      "619/618 [==============================] - 18s 29ms/step - loss: 0.9135 - accuracy: 0.5686 - val_loss: 1.0708 - val_accuracy: 0.4506\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 618.1015625 steps, validate for 180.4453125 steps\n",
      "Epoch 1/300\n",
      "619/618 [==============================] - 42s 67ms/step - loss: 0.8935 - accuracy: 0.5828 - val_loss: 1.0703 - val_accuracy: 0.4521\n",
      "Epoch 2/300\n",
      "619/618 [==============================] - 40s 65ms/step - loss: 0.8644 - accuracy: 0.6031 - val_loss: 1.0929 - val_accuracy: 0.4507\n",
      "Epoch 3/300\n",
      "619/618 [==============================] - 40s 65ms/step - loss: 0.8417 - accuracy: 0.6178 - val_loss: 1.1047 - val_accuracy: 0.4556\n",
      "Epoch 4/300\n",
      "619/618 [==============================] - 40s 65ms/step - loss: 0.8210 - accuracy: 0.6309 - val_loss: 1.0855 - val_accuracy: 0.4541\n",
      "Epoch 00004: early stopping\n",
      "380/380 [==============================] - 0s 1ms/sample - loss: 1.6903 - accuracy: 0.1526\n",
      "369/369 [==============================] - 0s 466us/sample - loss: 1.7250 - accuracy: 0.1057\n",
      "364/364 [==============================] - 0s 437us/sample - loss: 1.7204 - accuracy: 0.1429\n",
      "360/360 [==============================] - 0s 405us/sample - loss: 1.7114 - accuracy: 0.1306\n",
      "355/355 [==============================] - 0s 424us/sample - loss: 1.7326 - accuracy: 0.1521\n",
      "346/346 [==============================] - 0s 495us/sample - loss: 1.7140 - accuracy: 0.1503\n",
      "343/343 [==============================] - 0s 499us/sample - loss: 1.6962 - accuracy: 0.1545\n",
      "336/336 [==============================] - 0s 463us/sample - loss: 1.6897 - accuracy: 0.1756\n",
      "332/332 [==============================] - 0s 209us/sample - loss: 1.7066 - accuracy: 0.1506\n",
      "328/328 [==============================] - 0s 224us/sample - loss: 1.8072 - accuracy: 0.1402\n",
      "323/323 [==============================] - 0s 238us/sample - loss: 1.7298 - accuracy: 0.1517\n",
      "322/322 [==============================] - 0s 410us/sample - loss: 1.8396 - accuracy: 0.1584\n",
      "313/313 [==============================] - 0s 530us/sample - loss: 1.6622 - accuracy: 0.1661\n",
      "312/312 [==============================] - 0s 519us/sample - loss: 1.7148 - accuracy: 0.1571\n",
      "310/310 [==============================] - 0s 530us/sample - loss: 1.7981 - accuracy: 0.1613\n",
      "308/308 [==============================] - 0s 513us/sample - loss: 1.7942 - accuracy: 0.1591\n",
      "305/305 [==============================] - 0s 211us/sample - loss: 1.7834 - accuracy: 0.1738\n",
      "301/301 [==============================] - 0s 337us/sample - loss: 1.7936 - accuracy: 0.1927\n",
      "299/299 [==============================] - 0s 480us/sample - loss: 1.8033 - accuracy: 0.1672\n",
      "298/298 [==============================] - 0s 451us/sample - loss: 1.7934 - accuracy: 0.1342\n",
      "293/293 [==============================] - 0s 457us/sample - loss: 1.6989 - accuracy: 0.1843\n",
      "291/291 [==============================] - 0s 226us/sample - loss: 1.7483 - accuracy: 0.1615\n",
      "291/291 [==============================] - 0s 222us/sample - loss: 1.7118 - accuracy: 0.1546\n",
      "291/291 [==============================] - 0s 222us/sample - loss: 1.7168 - accuracy: 0.1753\n",
      "292/292 [==============================] - 0s 433us/sample - loss: 1.7370 - accuracy: 0.1473\n",
      "287/287 [==============================] - 0s 564us/sample - loss: 1.7750 - accuracy: 0.1603\n",
      "286/286 [==============================] - 0s 581us/sample - loss: 1.7400 - accuracy: 0.1783\n",
      "286/286 [==============================] - 0s 203us/sample - loss: 1.7452 - accuracy: 0.1678\n",
      "279/279 [==============================] - 0s 213us/sample - loss: 1.6636 - accuracy: 0.2043\n",
      "277/277 [==============================] - 0s 557us/sample - loss: 1.8332 - accuracy: 0.1805\n",
      "277/277 [==============================] - 0s 224us/sample - loss: 1.7676 - accuracy: 0.1697\n",
      "275/275 [==============================] - 0s 546us/sample - loss: 1.8532 - accuracy: 0.1600\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 1955 samples, validate on 554 samples\n",
      "Epoch 1/300\n",
      "1955/1955 [==============================] - 2s 1ms/sample - loss: 1.1633 - accuracy: 0.3688 - val_loss: 1.2486 - val_accuracy: 0.2798\n",
      "Epoch 2/300\n",
      "1955/1955 [==============================] - 0s 187us/sample - loss: 1.1222 - accuracy: 0.3949 - val_loss: 1.2284 - val_accuracy: 0.2906\n",
      "Epoch 3/300\n",
      "1955/1955 [==============================] - 0s 181us/sample - loss: 1.1057 - accuracy: 0.4061 - val_loss: 1.2146 - val_accuracy: 0.3087\n",
      "Epoch 4/300\n",
      "1955/1955 [==============================] - 0s 182us/sample - loss: 1.0909 - accuracy: 0.4251 - val_loss: 1.2040 - val_accuracy: 0.3177\n",
      "Epoch 5/300\n",
      "1955/1955 [==============================] - 0s 191us/sample - loss: 1.0752 - accuracy: 0.4379 - val_loss: 1.1958 - val_accuracy: 0.3213\n",
      "Epoch 6/300\n",
      "1955/1955 [==============================] - 0s 181us/sample - loss: 1.0684 - accuracy: 0.4404 - val_loss: 1.1884 - val_accuracy: 0.3231\n",
      "Epoch 7/300\n",
      "1955/1955 [==============================] - 0s 183us/sample - loss: 1.0606 - accuracy: 0.4512 - val_loss: 1.1823 - val_accuracy: 0.3339\n",
      "Epoch 8/300\n",
      "1955/1955 [==============================] - 0s 183us/sample - loss: 1.0503 - accuracy: 0.4685 - val_loss: 1.1766 - val_accuracy: 0.3394\n",
      "Epoch 9/300\n",
      "1955/1955 [==============================] - 0s 192us/sample - loss: 1.0440 - accuracy: 0.4726 - val_loss: 1.1717 - val_accuracy: 0.3394\n",
      "Epoch 10/300\n",
      "1955/1955 [==============================] - 0s 173us/sample - loss: 1.0352 - accuracy: 0.4824 - val_loss: 1.1673 - val_accuracy: 0.3412\n",
      "Epoch 11/300\n",
      "1955/1955 [==============================] - 0s 185us/sample - loss: 1.0308 - accuracy: 0.4870 - val_loss: 1.1634 - val_accuracy: 0.3412\n",
      "Epoch 12/300\n",
      "1955/1955 [==============================] - 0s 177us/sample - loss: 1.0293 - accuracy: 0.4931 - val_loss: 1.1597 - val_accuracy: 0.3556\n",
      "Epoch 13/300\n",
      "1955/1955 [==============================] - 0s 177us/sample - loss: 1.0256 - accuracy: 0.5018 - val_loss: 1.1563 - val_accuracy: 0.3628\n",
      "Epoch 14/300\n",
      "1955/1955 [==============================] - 0s 171us/sample - loss: 1.0166 - accuracy: 0.5023 - val_loss: 1.1531 - val_accuracy: 0.3682\n",
      "Epoch 15/300\n",
      "1955/1955 [==============================] - 0s 174us/sample - loss: 1.0185 - accuracy: 0.5013 - val_loss: 1.1504 - val_accuracy: 0.3755\n",
      "Epoch 16/300\n",
      "1955/1955 [==============================] - 0s 173us/sample - loss: 1.0104 - accuracy: 0.5253 - val_loss: 1.1475 - val_accuracy: 0.3863\n",
      "Epoch 17/300\n",
      "1955/1955 [==============================] - 0s 174us/sample - loss: 1.0089 - accuracy: 0.5100 - val_loss: 1.1451 - val_accuracy: 0.3863\n",
      "Epoch 18/300\n",
      "1955/1955 [==============================] - 0s 178us/sample - loss: 1.0065 - accuracy: 0.5176 - val_loss: 1.1430 - val_accuracy: 0.3845\n",
      "Epoch 19/300\n",
      "1955/1955 [==============================] - 0s 197us/sample - loss: 1.0010 - accuracy: 0.5166 - val_loss: 1.1408 - val_accuracy: 0.3809\n",
      "Epoch 20/300\n",
      "1955/1955 [==============================] - 0s 178us/sample - loss: 0.9979 - accuracy: 0.5223 - val_loss: 1.1387 - val_accuracy: 0.3863\n",
      "Epoch 21/300\n",
      "1955/1955 [==============================] - 0s 168us/sample - loss: 0.9951 - accuracy: 0.5289 - val_loss: 1.1369 - val_accuracy: 0.3917\n",
      "Epoch 22/300\n",
      "1955/1955 [==============================] - 0s 172us/sample - loss: 0.9831 - accuracy: 0.5442 - val_loss: 1.1350 - val_accuracy: 0.3899\n",
      "Epoch 23/300\n",
      "1955/1955 [==============================] - 0s 164us/sample - loss: 0.9911 - accuracy: 0.5315 - val_loss: 1.1332 - val_accuracy: 0.3917\n",
      "Epoch 24/300\n",
      "1955/1955 [==============================] - 0s 176us/sample - loss: 0.9901 - accuracy: 0.5141 - val_loss: 1.1317 - val_accuracy: 0.3899\n",
      "Epoch 25/300\n",
      "1955/1955 [==============================] - 0s 188us/sample - loss: 0.9806 - accuracy: 0.5320 - val_loss: 1.1300 - val_accuracy: 0.3917\n",
      "Epoch 26/300\n",
      "1955/1955 [==============================] - 0s 181us/sample - loss: 0.9737 - accuracy: 0.5458 - val_loss: 1.1285 - val_accuracy: 0.3935\n",
      "Epoch 27/300\n",
      "1955/1955 [==============================] - 0s 180us/sample - loss: 0.9776 - accuracy: 0.5437 - val_loss: 1.1270 - val_accuracy: 0.3899\n",
      "Epoch 28/300\n",
      "1955/1955 [==============================] - 0s 195us/sample - loss: 0.9691 - accuracy: 0.5535 - val_loss: 1.1257 - val_accuracy: 0.3953\n",
      "Epoch 29/300\n",
      "1955/1955 [==============================] - 0s 193us/sample - loss: 0.9687 - accuracy: 0.5407 - val_loss: 1.1246 - val_accuracy: 0.3935\n",
      "Epoch 30/300\n",
      "1955/1955 [==============================] - 0s 179us/sample - loss: 0.9701 - accuracy: 0.5494 - val_loss: 1.1234 - val_accuracy: 0.4025\n",
      "Epoch 31/300\n",
      "1955/1955 [==============================] - 0s 190us/sample - loss: 0.9626 - accuracy: 0.5545 - val_loss: 1.1222 - val_accuracy: 0.4007\n",
      "Epoch 32/300\n",
      "1955/1955 [==============================] - 0s 178us/sample - loss: 0.9611 - accuracy: 0.5473 - val_loss: 1.1213 - val_accuracy: 0.3989\n",
      "Epoch 33/300\n",
      "1955/1955 [==============================] - 0s 175us/sample - loss: 0.9593 - accuracy: 0.5591 - val_loss: 1.1203 - val_accuracy: 0.4007\n",
      "Epoch 34/300\n",
      "1955/1955 [==============================] - 0s 190us/sample - loss: 0.9567 - accuracy: 0.5555 - val_loss: 1.1193 - val_accuracy: 0.4025\n",
      "Epoch 35/300\n",
      "1955/1955 [==============================] - 0s 182us/sample - loss: 0.9526 - accuracy: 0.5606 - val_loss: 1.1185 - val_accuracy: 0.4025\n",
      "Epoch 36/300\n",
      "1955/1955 [==============================] - 0s 177us/sample - loss: 0.9562 - accuracy: 0.5519 - val_loss: 1.1177 - val_accuracy: 0.4025\n",
      "Epoch 37/300\n",
      "1955/1955 [==============================] - 0s 165us/sample - loss: 0.9557 - accuracy: 0.5560 - val_loss: 1.1168 - val_accuracy: 0.4007\n",
      "Epoch 38/300\n",
      "1955/1955 [==============================] - 0s 173us/sample - loss: 0.9485 - accuracy: 0.5657 - val_loss: 1.1160 - val_accuracy: 0.4025\n",
      "Epoch 39/300\n",
      "1955/1955 [==============================] - 0s 172us/sample - loss: 0.9540 - accuracy: 0.5673 - val_loss: 1.1151 - val_accuracy: 0.4007\n",
      "Epoch 40/300\n",
      "1955/1955 [==============================] - 0s 181us/sample - loss: 0.9386 - accuracy: 0.5693 - val_loss: 1.1143 - val_accuracy: 0.4025\n",
      "Epoch 41/300\n",
      "1955/1955 [==============================] - 0s 175us/sample - loss: 0.9369 - accuracy: 0.5575 - val_loss: 1.1135 - val_accuracy: 0.4007\n",
      "Epoch 42/300\n",
      "1955/1955 [==============================] - 0s 173us/sample - loss: 0.9383 - accuracy: 0.5621 - val_loss: 1.1128 - val_accuracy: 0.4007\n",
      "Epoch 43/300\n",
      "1955/1955 [==============================] - 0s 180us/sample - loss: 0.9413 - accuracy: 0.5637 - val_loss: 1.1119 - val_accuracy: 0.4007\n",
      "Epoch 44/300\n",
      "1955/1955 [==============================] - 0s 174us/sample - loss: 0.9302 - accuracy: 0.5719 - val_loss: 1.1113 - val_accuracy: 0.4007\n",
      "Epoch 45/300\n",
      "1955/1955 [==============================] - 0s 169us/sample - loss: 0.9324 - accuracy: 0.5754 - val_loss: 1.1106 - val_accuracy: 0.4061\n",
      "Epoch 46/300\n",
      "1955/1955 [==============================] - 0s 172us/sample - loss: 0.9289 - accuracy: 0.5795 - val_loss: 1.1101 - val_accuracy: 0.4097\n",
      "Epoch 47/300\n",
      "1955/1955 [==============================] - 0s 167us/sample - loss: 0.9203 - accuracy: 0.5790 - val_loss: 1.1095 - val_accuracy: 0.4097\n",
      "Epoch 48/300\n",
      "1955/1955 [==============================] - 0s 163us/sample - loss: 0.9268 - accuracy: 0.5847 - val_loss: 1.1089 - val_accuracy: 0.4079\n",
      "Epoch 49/300\n",
      "1955/1955 [==============================] - 0s 173us/sample - loss: 0.9270 - accuracy: 0.5806 - val_loss: 1.1085 - val_accuracy: 0.4097\n",
      "Epoch 50/300\n",
      "1955/1955 [==============================] - 0s 179us/sample - loss: 0.9270 - accuracy: 0.5708 - val_loss: 1.1080 - val_accuracy: 0.4097\n",
      "Epoch 51/300\n",
      "1955/1955 [==============================] - 0s 185us/sample - loss: 0.9226 - accuracy: 0.5698 - val_loss: 1.1076 - val_accuracy: 0.4097\n",
      "Epoch 52/300\n",
      "1955/1955 [==============================] - 0s 177us/sample - loss: 0.9208 - accuracy: 0.5790 - val_loss: 1.1070 - val_accuracy: 0.4116\n",
      "Epoch 53/300\n",
      "1955/1955 [==============================] - 0s 184us/sample - loss: 0.9186 - accuracy: 0.5847 - val_loss: 1.1066 - val_accuracy: 0.4097\n",
      "Epoch 54/300\n",
      "1955/1955 [==============================] - 0s 178us/sample - loss: 0.9158 - accuracy: 0.5862 - val_loss: 1.1063 - val_accuracy: 0.4097\n",
      "Epoch 55/300\n",
      "1955/1955 [==============================] - 0s 185us/sample - loss: 0.9068 - accuracy: 0.5831 - val_loss: 1.1059 - val_accuracy: 0.4097\n",
      "Epoch 56/300\n",
      "1955/1955 [==============================] - 0s 182us/sample - loss: 0.9081 - accuracy: 0.5821 - val_loss: 1.1054 - val_accuracy: 0.4079\n",
      "Epoch 57/300\n",
      "1955/1955 [==============================] - 0s 177us/sample - loss: 0.9150 - accuracy: 0.5739 - val_loss: 1.1050 - val_accuracy: 0.4097\n",
      "Epoch 58/300\n",
      "1955/1955 [==============================] - 0s 174us/sample - loss: 0.9119 - accuracy: 0.5898 - val_loss: 1.1048 - val_accuracy: 0.4116\n",
      "Epoch 59/300\n",
      "1955/1955 [==============================] - 0s 180us/sample - loss: 0.9072 - accuracy: 0.5841 - val_loss: 1.1044 - val_accuracy: 0.4116\n",
      "Epoch 60/300\n",
      "1955/1955 [==============================] - 0s 191us/sample - loss: 0.9042 - accuracy: 0.5908 - val_loss: 1.1041 - val_accuracy: 0.4116\n",
      "Epoch 61/300\n",
      "1955/1955 [==============================] - 0s 169us/sample - loss: 0.9022 - accuracy: 0.6010 - val_loss: 1.1039 - val_accuracy: 0.4134\n",
      "Epoch 62/300\n",
      "1955/1955 [==============================] - 0s 188us/sample - loss: 0.9020 - accuracy: 0.5913 - val_loss: 1.1035 - val_accuracy: 0.4134\n",
      "Epoch 63/300\n",
      "1955/1955 [==============================] - 0s 171us/sample - loss: 0.9045 - accuracy: 0.5882 - val_loss: 1.1032 - val_accuracy: 0.4152\n",
      "Epoch 64/300\n",
      "1955/1955 [==============================] - 0s 179us/sample - loss: 0.8918 - accuracy: 0.5995 - val_loss: 1.1029 - val_accuracy: 0.4134\n",
      "Epoch 65/300\n",
      "1955/1955 [==============================] - 0s 176us/sample - loss: 0.8959 - accuracy: 0.6000 - val_loss: 1.1028 - val_accuracy: 0.4152\n",
      "Epoch 66/300\n",
      "1955/1955 [==============================] - 0s 179us/sample - loss: 0.8927 - accuracy: 0.6005 - val_loss: 1.1024 - val_accuracy: 0.4152\n",
      "Epoch 67/300\n",
      "1955/1955 [==============================] - 0s 179us/sample - loss: 0.8853 - accuracy: 0.6010 - val_loss: 1.1020 - val_accuracy: 0.4134\n",
      "Epoch 68/300\n",
      "1955/1955 [==============================] - 0s 173us/sample - loss: 0.8837 - accuracy: 0.6010 - val_loss: 1.1018 - val_accuracy: 0.4134\n",
      "Epoch 69/300\n",
      "1955/1955 [==============================] - 0s 179us/sample - loss: 0.8885 - accuracy: 0.5964 - val_loss: 1.1015 - val_accuracy: 0.4170\n",
      "Epoch 70/300\n",
      "1955/1955 [==============================] - 0s 165us/sample - loss: 0.8901 - accuracy: 0.5964 - val_loss: 1.1014 - val_accuracy: 0.4170\n",
      "Epoch 71/300\n",
      "1955/1955 [==============================] - 0s 177us/sample - loss: 0.8784 - accuracy: 0.6031 - val_loss: 1.1011 - val_accuracy: 0.4152\n",
      "Epoch 72/300\n",
      "1955/1955 [==============================] - 0s 166us/sample - loss: 0.8837 - accuracy: 0.5949 - val_loss: 1.1010 - val_accuracy: 0.4152\n",
      "Epoch 73/300\n",
      "1955/1955 [==============================] - 0s 173us/sample - loss: 0.8862 - accuracy: 0.5990 - val_loss: 1.1010 - val_accuracy: 0.4152\n",
      "Epoch 74/300\n",
      "1955/1955 [==============================] - 0s 177us/sample - loss: 0.8816 - accuracy: 0.6077 - val_loss: 1.1007 - val_accuracy: 0.4152\n",
      "Epoch 75/300\n",
      "1955/1955 [==============================] - 0s 167us/sample - loss: 0.8792 - accuracy: 0.6087 - val_loss: 1.1006 - val_accuracy: 0.4134\n",
      "Epoch 76/300\n",
      "1955/1955 [==============================] - 0s 171us/sample - loss: 0.8759 - accuracy: 0.6133 - val_loss: 1.1005 - val_accuracy: 0.4134\n",
      "Epoch 77/300\n",
      "1955/1955 [==============================] - 0s 179us/sample - loss: 0.8744 - accuracy: 0.6184 - val_loss: 1.1005 - val_accuracy: 0.4152\n",
      "Epoch 78/300\n",
      "1955/1955 [==============================] - 0s 173us/sample - loss: 0.8755 - accuracy: 0.6005 - val_loss: 1.1003 - val_accuracy: 0.4134\n",
      "Epoch 79/300\n",
      "1955/1955 [==============================] - 0s 172us/sample - loss: 0.8668 - accuracy: 0.6133 - val_loss: 1.1001 - val_accuracy: 0.4152\n",
      "Epoch 80/300\n",
      "1955/1955 [==============================] - 0s 182us/sample - loss: 0.8743 - accuracy: 0.6153 - val_loss: 1.1001 - val_accuracy: 0.4170\n",
      "Epoch 81/300\n",
      "1955/1955 [==============================] - 0s 179us/sample - loss: 0.8661 - accuracy: 0.6184 - val_loss: 1.0999 - val_accuracy: 0.4170\n",
      "Epoch 82/300\n",
      "1955/1955 [==============================] - 0s 183us/sample - loss: 0.8650 - accuracy: 0.6205 - val_loss: 1.0995 - val_accuracy: 0.4188\n",
      "Epoch 83/300\n",
      "1955/1955 [==============================] - 0s 181us/sample - loss: 0.8689 - accuracy: 0.6036 - val_loss: 1.0994 - val_accuracy: 0.4188\n",
      "Epoch 84/300\n",
      "1955/1955 [==============================] - 0s 184us/sample - loss: 0.8617 - accuracy: 0.6189 - val_loss: 1.0995 - val_accuracy: 0.4188\n",
      "Epoch 85/300\n",
      "1955/1955 [==============================] - 0s 181us/sample - loss: 0.8611 - accuracy: 0.6246 - val_loss: 1.0993 - val_accuracy: 0.4188\n",
      "Epoch 86/300\n",
      "1955/1955 [==============================] - 0s 184us/sample - loss: 0.8611 - accuracy: 0.6061 - val_loss: 1.0991 - val_accuracy: 0.4170\n",
      "Epoch 87/300\n",
      "1955/1955 [==============================] - 0s 179us/sample - loss: 0.8549 - accuracy: 0.6174 - val_loss: 1.0992 - val_accuracy: 0.4170\n",
      "Epoch 88/300\n",
      "1955/1955 [==============================] - 0s 187us/sample - loss: 0.8622 - accuracy: 0.6026 - val_loss: 1.0992 - val_accuracy: 0.4152\n",
      "Epoch 89/300\n",
      "1955/1955 [==============================] - 0s 180us/sample - loss: 0.8553 - accuracy: 0.6210 - val_loss: 1.0993 - val_accuracy: 0.4170\n",
      "Epoch 00089: early stopping\n",
      "247/247 [==============================] - 0s 88us/sample - loss: 1.4931 - accuracy: 0.1984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [11:21, 681.48s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:05<00:11,  5.83s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:11<00:05,  5.89s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:14<00:00,  4.92s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 633.7890625 steps, validate for 175.703125 steps\n",
      "Epoch 1/300\n",
      "634/633 [==============================] - 19s 30ms/step - loss: 1.1300 - accuracy: 0.4211 - val_loss: 1.1240 - val_accuracy: 0.4185\n",
      "Epoch 2/300\n",
      "634/633 [==============================] - 17s 27ms/step - loss: 1.0175 - accuracy: 0.4876 - val_loss: 1.1069 - val_accuracy: 0.4234\n",
      "Epoch 3/300\n",
      "634/633 [==============================] - 17s 27ms/step - loss: 0.9864 - accuracy: 0.5109 - val_loss: 1.0999 - val_accuracy: 0.4259\n",
      "Epoch 4/300\n",
      "634/633 [==============================] - 18s 28ms/step - loss: 0.9665 - accuracy: 0.5245 - val_loss: 1.0931 - val_accuracy: 0.4341\n",
      "Epoch 5/300\n",
      "634/633 [==============================] - 18s 28ms/step - loss: 0.9516 - accuracy: 0.5373 - val_loss: 1.0950 - val_accuracy: 0.4330\n",
      "Epoch 6/300\n",
      "634/633 [==============================] - 18s 28ms/step - loss: 0.9395 - accuracy: 0.5464 - val_loss: 1.0968 - val_accuracy: 0.4325\n",
      "Epoch 7/300\n",
      "634/633 [==============================] - 18s 28ms/step - loss: 0.9291 - accuracy: 0.5536 - val_loss: 1.0996 - val_accuracy: 0.4337\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 633.7890625 steps, validate for 175.703125 steps\n",
      "Epoch 1/300\n",
      "634/633 [==============================] - 42s 66ms/step - loss: 0.9113 - accuracy: 0.5660 - val_loss: 1.1355 - val_accuracy: 0.4213\n",
      "Epoch 2/300\n",
      "634/633 [==============================] - 40s 63ms/step - loss: 0.8827 - accuracy: 0.5868 - val_loss: 1.1188 - val_accuracy: 0.4462\n",
      "Epoch 3/300\n",
      "634/633 [==============================] - 40s 63ms/step - loss: 0.8603 - accuracy: 0.6037 - val_loss: 1.1078 - val_accuracy: 0.4301\n",
      "Epoch 4/300\n",
      "634/633 [==============================] - 40s 64ms/step - loss: 0.8401 - accuracy: 0.6173 - val_loss: 1.1220 - val_accuracy: 0.4353\n",
      "Epoch 5/300\n",
      "634/633 [==============================] - 40s 63ms/step - loss: 0.8215 - accuracy: 0.6290 - val_loss: 1.1169 - val_accuracy: 0.4483\n",
      "Epoch 6/300\n",
      "634/633 [==============================] - 40s 64ms/step - loss: 0.8038 - accuracy: 0.6412 - val_loss: 1.1385 - val_accuracy: 0.4215\n",
      "Epoch 00006: early stopping\n",
      "414/414 [==============================] - 0s 767us/sample - loss: 0.8643 - accuracy: 0.5507\n",
      "392/392 [==============================] - 0s 224us/sample - loss: 0.9163 - accuracy: 0.5459\n",
      "382/382 [==============================] - 0s 202us/sample - loss: 0.9218 - accuracy: 0.5262\n",
      "356/356 [==============================] - 0s 220us/sample - loss: 0.9325 - accuracy: 0.5281\n",
      "330/330 [==============================] - 0s 229us/sample - loss: 0.9666 - accuracy: 0.5091\n",
      "317/317 [==============================] - 0s 564us/sample - loss: 0.9793 - accuracy: 0.5079\n",
      "311/311 [==============================] - 0s 225us/sample - loss: 1.0165 - accuracy: 0.4952\n",
      "309/309 [==============================] - 0s 223us/sample - loss: 1.0104 - accuracy: 0.5016\n",
      "294/294 [==============================] - 0s 488us/sample - loss: 1.0446 - accuracy: 0.4320\n",
      "289/289 [==============================] - 0s 442us/sample - loss: 1.0698 - accuracy: 0.4118\n",
      "269/269 [==============================] - 0s 230us/sample - loss: 1.1003 - accuracy: 0.4387\n",
      "262/262 [==============================] - 0s 228us/sample - loss: 1.1027 - accuracy: 0.4389\n",
      "254/254 [==============================] - 0s 226us/sample - loss: 1.0540 - accuracy: 0.4528\n",
      "253/253 [==============================] - 0s 223us/sample - loss: 1.0977 - accuracy: 0.3794\n",
      "252/252 [==============================] - 0s 252us/sample - loss: 1.0950 - accuracy: 0.3849\n",
      "249/249 [==============================] - 0s 222us/sample - loss: 1.1124 - accuracy: 0.4137\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 1.0964 - accuracy: 0.4228\n",
      "244/244 [==============================] - 0s 241us/sample - loss: 1.1380 - accuracy: 0.4098\n",
      "243/243 [==============================] - 0s 229us/sample - loss: 1.1233 - accuracy: 0.3827\n",
      "241/241 [==============================] - 0s 215us/sample - loss: 1.1421 - accuracy: 0.3942\n",
      "239/239 [==============================] - 0s 580us/sample - loss: 1.1489 - accuracy: 0.3891\n",
      "236/236 [==============================] - 0s 260us/sample - loss: 1.1437 - accuracy: 0.3941\n",
      "235/235 [==============================] - 0s 226us/sample - loss: 1.1763 - accuracy: 0.3787\n",
      "229/229 [==============================] - 0s 263us/sample - loss: 1.0756 - accuracy: 0.4061\n",
      "229/229 [==============================] - 0s 296us/sample - loss: 1.1373 - accuracy: 0.3799\n",
      "225/225 [==============================] - 0s 238us/sample - loss: 1.1742 - accuracy: 0.3911\n",
      "223/223 [==============================] - 0s 237us/sample - loss: 1.1617 - accuracy: 0.4126\n",
      "222/222 [==============================] - 0s 293us/sample - loss: 1.1096 - accuracy: 0.4099\n",
      "222/222 [==============================] - 0s 243us/sample - loss: 1.1450 - accuracy: 0.3829\n",
      "223/223 [==============================] - 0s 235us/sample - loss: 1.1426 - accuracy: 0.3587\n",
      "222/222 [==============================] - 0s 243us/sample - loss: 1.1325 - accuracy: 0.3829\n",
      "216/216 [==============================] - 0s 250us/sample - loss: 1.1546 - accuracy: 0.3796\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 2022 samples, validate on 540 samples\n",
      "Epoch 1/300\n",
      "2022/2022 [==============================] - 2s 1ms/sample - loss: 1.1595 - accuracy: 0.3081 - val_loss: 1.1247 - val_accuracy: 0.3259\n",
      "Epoch 2/300\n",
      "2022/2022 [==============================] - 0s 184us/sample - loss: 1.1383 - accuracy: 0.3225 - val_loss: 1.1165 - val_accuracy: 0.3333\n",
      "Epoch 3/300\n",
      "2022/2022 [==============================] - 0s 205us/sample - loss: 1.1270 - accuracy: 0.3348 - val_loss: 1.1107 - val_accuracy: 0.3444\n",
      "Epoch 4/300\n",
      "2022/2022 [==============================] - 0s 184us/sample - loss: 1.1227 - accuracy: 0.3309 - val_loss: 1.1058 - val_accuracy: 0.3463\n",
      "Epoch 5/300\n",
      "2022/2022 [==============================] - 0s 176us/sample - loss: 1.1117 - accuracy: 0.3531 - val_loss: 1.1017 - val_accuracy: 0.3574\n",
      "Epoch 6/300\n",
      "2022/2022 [==============================] - 0s 178us/sample - loss: 1.1059 - accuracy: 0.3630 - val_loss: 1.0982 - val_accuracy: 0.3630\n",
      "Epoch 7/300\n",
      "2022/2022 [==============================] - 0s 195us/sample - loss: 1.0977 - accuracy: 0.3764 - val_loss: 1.0950 - val_accuracy: 0.3667\n",
      "Epoch 8/300\n",
      "2022/2022 [==============================] - 0s 192us/sample - loss: 1.1018 - accuracy: 0.3581 - val_loss: 1.0921 - val_accuracy: 0.3722\n",
      "Epoch 9/300\n",
      "2022/2022 [==============================] - 0s 183us/sample - loss: 1.0865 - accuracy: 0.3867 - val_loss: 1.0897 - val_accuracy: 0.3759\n",
      "Epoch 10/300\n",
      "2022/2022 [==============================] - 0s 188us/sample - loss: 1.0805 - accuracy: 0.3788 - val_loss: 1.0872 - val_accuracy: 0.3815\n",
      "Epoch 11/300\n",
      "2022/2022 [==============================] - 0s 195us/sample - loss: 1.0821 - accuracy: 0.3902 - val_loss: 1.0852 - val_accuracy: 0.3889\n",
      "Epoch 12/300\n",
      "2022/2022 [==============================] - 0s 192us/sample - loss: 1.0716 - accuracy: 0.3996 - val_loss: 1.0829 - val_accuracy: 0.3963\n",
      "Epoch 13/300\n",
      "2022/2022 [==============================] - 0s 192us/sample - loss: 1.0733 - accuracy: 0.4001 - val_loss: 1.0809 - val_accuracy: 0.4037\n",
      "Epoch 14/300\n",
      "2022/2022 [==============================] - 0s 174us/sample - loss: 1.0709 - accuracy: 0.4036 - val_loss: 1.0792 - val_accuracy: 0.4000\n",
      "Epoch 15/300\n",
      "2022/2022 [==============================] - 0s 177us/sample - loss: 1.0623 - accuracy: 0.4139 - val_loss: 1.0776 - val_accuracy: 0.4037\n",
      "Epoch 16/300\n",
      "2022/2022 [==============================] - 0s 191us/sample - loss: 1.0552 - accuracy: 0.4362 - val_loss: 1.0762 - val_accuracy: 0.4000\n",
      "Epoch 17/300\n",
      "2022/2022 [==============================] - 0s 181us/sample - loss: 1.0597 - accuracy: 0.4357 - val_loss: 1.0745 - val_accuracy: 0.4056\n",
      "Epoch 18/300\n",
      "2022/2022 [==============================] - 0s 173us/sample - loss: 1.0511 - accuracy: 0.4278 - val_loss: 1.0731 - val_accuracy: 0.4111\n",
      "Epoch 19/300\n",
      "2022/2022 [==============================] - 0s 179us/sample - loss: 1.0438 - accuracy: 0.4407 - val_loss: 1.0718 - val_accuracy: 0.4148\n",
      "Epoch 20/300\n",
      "2022/2022 [==============================] - 0s 172us/sample - loss: 1.0445 - accuracy: 0.4402 - val_loss: 1.0707 - val_accuracy: 0.4185\n",
      "Epoch 21/300\n",
      "2022/2022 [==============================] - 0s 185us/sample - loss: 1.0466 - accuracy: 0.4491 - val_loss: 1.0696 - val_accuracy: 0.4204\n",
      "Epoch 22/300\n",
      "2022/2022 [==============================] - 0s 180us/sample - loss: 1.0370 - accuracy: 0.4609 - val_loss: 1.0685 - val_accuracy: 0.4185\n",
      "Epoch 23/300\n",
      "2022/2022 [==============================] - 0s 185us/sample - loss: 1.0362 - accuracy: 0.4510 - val_loss: 1.0673 - val_accuracy: 0.4185\n",
      "Epoch 24/300\n",
      "2022/2022 [==============================] - 0s 186us/sample - loss: 1.0355 - accuracy: 0.4575 - val_loss: 1.0662 - val_accuracy: 0.4167\n",
      "Epoch 25/300\n",
      "2022/2022 [==============================] - 0s 183us/sample - loss: 1.0329 - accuracy: 0.4723 - val_loss: 1.0651 - val_accuracy: 0.4222\n",
      "Epoch 26/300\n",
      "2022/2022 [==============================] - 0s 190us/sample - loss: 1.0282 - accuracy: 0.4624 - val_loss: 1.0640 - val_accuracy: 0.4204\n",
      "Epoch 27/300\n",
      "2022/2022 [==============================] - 0s 184us/sample - loss: 1.0236 - accuracy: 0.4713 - val_loss: 1.0630 - val_accuracy: 0.4278\n",
      "Epoch 28/300\n",
      "2022/2022 [==============================] - 0s 197us/sample - loss: 1.0206 - accuracy: 0.4634 - val_loss: 1.0621 - val_accuracy: 0.4278\n",
      "Epoch 29/300\n",
      "2022/2022 [==============================] - 0s 186us/sample - loss: 1.0249 - accuracy: 0.4738 - val_loss: 1.0613 - val_accuracy: 0.4241\n",
      "Epoch 30/300\n",
      "2022/2022 [==============================] - 0s 184us/sample - loss: 1.0121 - accuracy: 0.4871 - val_loss: 1.0603 - val_accuracy: 0.4278\n",
      "Epoch 31/300\n",
      "2022/2022 [==============================] - 0s 191us/sample - loss: 1.0105 - accuracy: 0.4837 - val_loss: 1.0596 - val_accuracy: 0.4259\n",
      "Epoch 32/300\n",
      "2022/2022 [==============================] - 0s 193us/sample - loss: 1.0152 - accuracy: 0.4822 - val_loss: 1.0588 - val_accuracy: 0.4315\n",
      "Epoch 33/300\n",
      "2022/2022 [==============================] - 0s 188us/sample - loss: 1.0033 - accuracy: 0.4881 - val_loss: 1.0581 - val_accuracy: 0.4315\n",
      "Epoch 34/300\n",
      "2022/2022 [==============================] - 0s 200us/sample - loss: 1.0047 - accuracy: 0.4990 - val_loss: 1.0574 - val_accuracy: 0.4315\n",
      "Epoch 35/300\n",
      "2022/2022 [==============================] - 0s 183us/sample - loss: 1.0058 - accuracy: 0.4990 - val_loss: 1.0567 - val_accuracy: 0.4389\n",
      "Epoch 36/300\n",
      "2022/2022 [==============================] - 0s 182us/sample - loss: 0.9992 - accuracy: 0.5010 - val_loss: 1.0562 - val_accuracy: 0.4389\n",
      "Epoch 37/300\n",
      "2022/2022 [==============================] - 0s 187us/sample - loss: 0.9959 - accuracy: 0.5198 - val_loss: 1.0554 - val_accuracy: 0.4426\n",
      "Epoch 38/300\n",
      "2022/2022 [==============================] - 0s 195us/sample - loss: 0.9966 - accuracy: 0.5005 - val_loss: 1.0550 - val_accuracy: 0.4444\n",
      "Epoch 39/300\n",
      "2022/2022 [==============================] - 0s 199us/sample - loss: 0.9910 - accuracy: 0.5203 - val_loss: 1.0545 - val_accuracy: 0.4444\n",
      "Epoch 40/300\n",
      "2022/2022 [==============================] - 0s 187us/sample - loss: 0.9936 - accuracy: 0.4980 - val_loss: 1.0539 - val_accuracy: 0.4481\n",
      "Epoch 41/300\n",
      "2022/2022 [==============================] - 0s 197us/sample - loss: 0.9910 - accuracy: 0.5168 - val_loss: 1.0533 - val_accuracy: 0.4537\n",
      "Epoch 42/300\n",
      "2022/2022 [==============================] - 0s 212us/sample - loss: 0.9791 - accuracy: 0.5173 - val_loss: 1.0528 - val_accuracy: 0.4537\n",
      "Epoch 43/300\n",
      "2022/2022 [==============================] - 0s 197us/sample - loss: 0.9894 - accuracy: 0.5158 - val_loss: 1.0524 - val_accuracy: 0.4537\n",
      "Epoch 44/300\n",
      "2022/2022 [==============================] - 0s 205us/sample - loss: 0.9844 - accuracy: 0.5153 - val_loss: 1.0519 - val_accuracy: 0.4556\n",
      "Epoch 45/300\n",
      "2022/2022 [==============================] - 0s 200us/sample - loss: 0.9805 - accuracy: 0.5114 - val_loss: 1.0515 - val_accuracy: 0.4556\n",
      "Epoch 46/300\n",
      "2022/2022 [==============================] - 0s 174us/sample - loss: 0.9812 - accuracy: 0.5242 - val_loss: 1.0512 - val_accuracy: 0.4556\n",
      "Epoch 47/300\n",
      "2022/2022 [==============================] - 0s 182us/sample - loss: 0.9689 - accuracy: 0.5425 - val_loss: 1.0508 - val_accuracy: 0.4537\n",
      "Epoch 48/300\n",
      "2022/2022 [==============================] - 0s 188us/sample - loss: 0.9754 - accuracy: 0.5317 - val_loss: 1.0504 - val_accuracy: 0.4537\n",
      "Epoch 49/300\n",
      "2022/2022 [==============================] - 0s 178us/sample - loss: 0.9700 - accuracy: 0.5247 - val_loss: 1.0501 - val_accuracy: 0.4556\n",
      "Epoch 50/300\n",
      "2022/2022 [==============================] - 0s 179us/sample - loss: 0.9687 - accuracy: 0.5346 - val_loss: 1.0498 - val_accuracy: 0.4556\n",
      "Epoch 51/300\n",
      "2022/2022 [==============================] - 0s 194us/sample - loss: 0.9644 - accuracy: 0.5336 - val_loss: 1.0495 - val_accuracy: 0.4556\n",
      "Epoch 52/300\n",
      "2022/2022 [==============================] - 0s 180us/sample - loss: 0.9648 - accuracy: 0.5415 - val_loss: 1.0491 - val_accuracy: 0.4556\n",
      "Epoch 53/300\n",
      "2022/2022 [==============================] - 0s 177us/sample - loss: 0.9633 - accuracy: 0.5321 - val_loss: 1.0488 - val_accuracy: 0.4593\n",
      "Epoch 54/300\n",
      "2022/2022 [==============================] - 0s 177us/sample - loss: 0.9595 - accuracy: 0.5524 - val_loss: 1.0486 - val_accuracy: 0.4574\n",
      "Epoch 55/300\n",
      "2022/2022 [==============================] - 0s 176us/sample - loss: 0.9569 - accuracy: 0.5480 - val_loss: 1.0484 - val_accuracy: 0.4574\n",
      "Epoch 56/300\n",
      "2022/2022 [==============================] - 0s 186us/sample - loss: 0.9508 - accuracy: 0.5539 - val_loss: 1.0481 - val_accuracy: 0.4556\n",
      "Epoch 57/300\n",
      "2022/2022 [==============================] - 0s 183us/sample - loss: 0.9567 - accuracy: 0.5356 - val_loss: 1.0478 - val_accuracy: 0.4574\n",
      "Epoch 58/300\n",
      "2022/2022 [==============================] - 0s 190us/sample - loss: 0.9487 - accuracy: 0.5584 - val_loss: 1.0477 - val_accuracy: 0.4556\n",
      "Epoch 59/300\n",
      "2022/2022 [==============================] - 0s 180us/sample - loss: 0.9481 - accuracy: 0.5598 - val_loss: 1.0474 - val_accuracy: 0.4537\n",
      "Epoch 60/300\n",
      "2022/2022 [==============================] - 0s 179us/sample - loss: 0.9450 - accuracy: 0.5544 - val_loss: 1.0471 - val_accuracy: 0.4556\n",
      "Epoch 61/300\n",
      "2022/2022 [==============================] - 0s 183us/sample - loss: 0.9422 - accuracy: 0.5608 - val_loss: 1.0469 - val_accuracy: 0.4537\n",
      "Epoch 62/300\n",
      "2022/2022 [==============================] - 0s 203us/sample - loss: 0.9460 - accuracy: 0.5539 - val_loss: 1.0467 - val_accuracy: 0.4519\n",
      "Epoch 63/300\n",
      "2022/2022 [==============================] - 0s 207us/sample - loss: 0.9427 - accuracy: 0.5559 - val_loss: 1.0465 - val_accuracy: 0.4519\n",
      "Epoch 64/300\n",
      "2022/2022 [==============================] - 0s 206us/sample - loss: 0.9357 - accuracy: 0.5722 - val_loss: 1.0465 - val_accuracy: 0.4481\n",
      "Epoch 65/300\n",
      "2022/2022 [==============================] - 0s 188us/sample - loss: 0.9343 - accuracy: 0.5539 - val_loss: 1.0464 - val_accuracy: 0.4500\n",
      "Epoch 66/300\n",
      "2022/2022 [==============================] - 0s 185us/sample - loss: 0.9375 - accuracy: 0.5598 - val_loss: 1.0462 - val_accuracy: 0.4500\n",
      "Epoch 67/300\n",
      "2022/2022 [==============================] - 0s 174us/sample - loss: 0.9334 - accuracy: 0.5618 - val_loss: 1.0461 - val_accuracy: 0.4519\n",
      "Epoch 68/300\n",
      "2022/2022 [==============================] - 0s 198us/sample - loss: 0.9261 - accuracy: 0.5747 - val_loss: 1.0460 - val_accuracy: 0.4519\n",
      "Epoch 69/300\n",
      "2022/2022 [==============================] - 0s 192us/sample - loss: 0.9255 - accuracy: 0.5727 - val_loss: 1.0459 - val_accuracy: 0.4537\n",
      "Epoch 70/300\n",
      "2022/2022 [==============================] - 0s 185us/sample - loss: 0.9301 - accuracy: 0.5747 - val_loss: 1.0458 - val_accuracy: 0.4537\n",
      "Epoch 71/300\n",
      "2022/2022 [==============================] - 0s 168us/sample - loss: 0.9227 - accuracy: 0.5628 - val_loss: 1.0458 - val_accuracy: 0.4574\n",
      "Epoch 72/300\n",
      "2022/2022 [==============================] - 0s 181us/sample - loss: 0.9171 - accuracy: 0.5880 - val_loss: 1.0457 - val_accuracy: 0.4556\n",
      "Epoch 73/300\n",
      "2022/2022 [==============================] - 0s 179us/sample - loss: 0.9169 - accuracy: 0.5841 - val_loss: 1.0458 - val_accuracy: 0.4574\n",
      "Epoch 74/300\n",
      "2022/2022 [==============================] - 0s 180us/sample - loss: 0.9213 - accuracy: 0.5811 - val_loss: 1.0458 - val_accuracy: 0.4574\n",
      "Epoch 75/300\n",
      "2022/2022 [==============================] - 0s 187us/sample - loss: 0.9164 - accuracy: 0.5861 - val_loss: 1.0458 - val_accuracy: 0.4574\n",
      "Epoch 00075: early stopping\n",
      "194/194 [==============================] - 0s 143us/sample - loss: 1.0937 - accuracy: 0.3505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [24:09, 707.36s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:06<00:12,  6.13s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:12<00:06,  6.24s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.20s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 624.1328125 steps, validate for 168.0546875 steps\n",
      "Epoch 1/300\n",
      "625/624 [==============================] - 18s 29ms/step - loss: 1.1384 - accuracy: 0.4229 - val_loss: 1.1160 - val_accuracy: 0.4248\n",
      "Epoch 2/300\n",
      "625/624 [==============================] - 17s 28ms/step - loss: 1.0081 - accuracy: 0.4968 - val_loss: 1.1028 - val_accuracy: 0.4246\n",
      "Epoch 3/300\n",
      "625/624 [==============================] - 17s 28ms/step - loss: 0.9754 - accuracy: 0.5209 - val_loss: 1.0962 - val_accuracy: 0.4254\n",
      "Epoch 4/300\n",
      "625/624 [==============================] - 17s 27ms/step - loss: 0.9551 - accuracy: 0.5356 - val_loss: 1.1105 - val_accuracy: 0.4143\n",
      "Epoch 5/300\n",
      "625/624 [==============================] - 17s 28ms/step - loss: 0.9401 - accuracy: 0.5458 - val_loss: 1.1031 - val_accuracy: 0.42413 - accuracy: 0.54\n",
      "Epoch 6/300\n",
      "625/624 [==============================] - 17s 28ms/step - loss: 0.9276 - accuracy: 0.5565 - val_loss: 1.1041 - val_accuracy: 0.4281\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 624.1328125 steps, validate for 168.0546875 steps\n",
      "Epoch 1/300\n",
      "625/624 [==============================] - 41s 65ms/step - loss: 0.9047 - accuracy: 0.5726 - val_loss: 1.1131 - val_accuracy: 0.4154\n",
      "Epoch 2/300\n",
      "625/624 [==============================] - 40s 63ms/step - loss: 0.8729 - accuracy: 0.5952 - val_loss: 1.1274 - val_accuracy: 0.4187\n",
      "Epoch 3/300\n",
      "625/624 [==============================] - 40s 63ms/step - loss: 0.8479 - accuracy: 0.6130 - val_loss: 1.1124 - val_accuracy: 0.4264\n",
      "Epoch 4/300\n",
      "625/624 [==============================] - 40s 64ms/step - loss: 0.8267 - accuracy: 0.6266 - val_loss: 1.1423 - val_accuracy: 0.4295\n",
      "Epoch 5/300\n",
      "625/624 [==============================] - 40s 63ms/step - loss: 0.8067 - accuracy: 0.6404 - val_loss: 1.1622 - val_accuracy: 0.4078\n",
      "Epoch 6/300\n",
      "625/624 [==============================] - 40s 63ms/step - loss: 0.7881 - accuracy: 0.6513 - val_loss: 1.1163 - val_accuracy: 0.4406\n",
      "Epoch 00006: early stopping\n",
      "410/410 [==============================] - 0s 653us/sample - loss: 1.2410 - accuracy: 0.3268\n",
      "390/390 [==============================] - 0s 228us/sample - loss: 1.2592 - accuracy: 0.2718\n",
      "387/387 [==============================] - 0s 228us/sample - loss: 1.3328 - accuracy: 0.2145\n",
      "380/380 [==============================] - 0s 219us/sample - loss: 1.2999 - accuracy: 0.2684\n",
      "378/378 [==============================] - 0s 233us/sample - loss: 1.2956 - accuracy: 0.2302\n",
      "371/371 [==============================] - 0s 245us/sample - loss: 1.3267 - accuracy: 0.2318\n",
      "369/369 [==============================] - 0s 259us/sample - loss: 1.3618 - accuracy: 0.1951\n",
      "363/363 [==============================] - 0s 252us/sample - loss: 1.3654 - accuracy: 0.2369\n",
      "354/354 [==============================] - 0s 269us/sample - loss: 1.4321 - accuracy: 0.1921\n",
      "353/353 [==============================] - 0s 242us/sample - loss: 1.3504 - accuracy: 0.2096\n",
      "345/345 [==============================] - 0s 235us/sample - loss: 1.4322 - accuracy: 0.2029\n",
      "336/336 [==============================] - 0s 234us/sample - loss: 1.4174 - accuracy: 0.2143\n",
      "336/336 [==============================] - 0s 250us/sample - loss: 1.4329 - accuracy: 0.2232\n",
      "335/335 [==============================] - 0s 236us/sample - loss: 1.4215 - accuracy: 0.1910\n",
      "337/337 [==============================] - 0s 242us/sample - loss: 1.4618 - accuracy: 0.2107\n",
      "338/338 [==============================] - 0s 511us/sample - loss: 1.4481 - accuracy: 0.1864\n",
      "333/333 [==============================] - 0s 233us/sample - loss: 1.4926 - accuracy: 0.2162\n",
      "330/330 [==============================] - 0s 232us/sample - loss: 1.5445 - accuracy: 0.1697\n",
      "331/331 [==============================] - 0s 226us/sample - loss: 1.5248 - accuracy: 0.1662\n",
      "328/328 [==============================] - 0s 236us/sample - loss: 1.5573 - accuracy: 0.1555\n",
      "324/324 [==============================] - 0s 227us/sample - loss: 1.5011 - accuracy: 0.1728\n",
      "322/322 [==============================] - 0s 227us/sample - loss: 1.4682 - accuracy: 0.1739\n",
      "324/324 [==============================] - 0s 233us/sample - loss: 1.5288 - accuracy: 0.2130\n",
      "323/323 [==============================] - 0s 246us/sample - loss: 1.5208 - accuracy: 0.2012\n",
      "318/318 [==============================] - 0s 216us/sample - loss: 1.5091 - accuracy: 0.1761\n",
      "314/314 [==============================] - 0s 230us/sample - loss: 1.5468 - accuracy: 0.1879\n",
      "313/313 [==============================] - 0s 241us/sample - loss: 1.5593 - accuracy: 0.1406\n",
      "304/304 [==============================] - 0s 259us/sample - loss: 1.5894 - accuracy: 0.1513\n",
      "301/301 [==============================] - 0s 220us/sample - loss: 1.6388 - accuracy: 0.1229\n",
      "299/299 [==============================] - 0s 227us/sample - loss: 1.5430 - accuracy: 0.1371\n",
      "301/301 [==============================] - 0s 237us/sample - loss: 1.5347 - accuracy: 0.1960\n",
      "296/296 [==============================] - 0s 257us/sample - loss: 1.5645 - accuracy: 0.1453\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 1978 samples, validate on 513 samples\n",
      "Epoch 1/300\n",
      "1978/1978 [==============================] - 2s 1ms/sample - loss: 1.1924 - accuracy: 0.3160 - val_loss: 1.1512 - val_accuracy: 0.3099\n",
      "Epoch 2/300\n",
      "1978/1978 [==============================] - 0s 189us/sample - loss: 1.1588 - accuracy: 0.3367 - val_loss: 1.1411 - val_accuracy: 0.3275\n",
      "Epoch 3/300\n",
      "1978/1978 [==============================] - 0s 177us/sample - loss: 1.1552 - accuracy: 0.3493 - val_loss: 1.1343 - val_accuracy: 0.3353\n",
      "Epoch 4/300\n",
      "1978/1978 [==============================] - 0s 182us/sample - loss: 1.1362 - accuracy: 0.3706 - val_loss: 1.1286 - val_accuracy: 0.3470\n",
      "Epoch 5/300\n",
      "1978/1978 [==============================] - 0s 191us/sample - loss: 1.1182 - accuracy: 0.3989 - val_loss: 1.1239 - val_accuracy: 0.3509\n",
      "Epoch 6/300\n",
      "1978/1978 [==============================] - 0s 198us/sample - loss: 1.1105 - accuracy: 0.3908 - val_loss: 1.1198 - val_accuracy: 0.3587\n",
      "Epoch 7/300\n",
      "1978/1978 [==============================] - 0s 195us/sample - loss: 1.1064 - accuracy: 0.3923 - val_loss: 1.1160 - val_accuracy: 0.3587\n",
      "Epoch 8/300\n",
      "1978/1978 [==============================] - 0s 201us/sample - loss: 1.0987 - accuracy: 0.4105 - val_loss: 1.1128 - val_accuracy: 0.3587\n",
      "Epoch 9/300\n",
      "1978/1978 [==============================] - 0s 188us/sample - loss: 1.0881 - accuracy: 0.4125 - val_loss: 1.1099 - val_accuracy: 0.3587\n",
      "Epoch 10/300\n",
      "1978/1978 [==============================] - 0s 196us/sample - loss: 1.0870 - accuracy: 0.4191 - val_loss: 1.1069 - val_accuracy: 0.3665\n",
      "Epoch 11/300\n",
      "1978/1978 [==============================] - 0s 177us/sample - loss: 1.0744 - accuracy: 0.4383 - val_loss: 1.1043 - val_accuracy: 0.3821\n",
      "Epoch 12/300\n",
      "1978/1978 [==============================] - 0s 192us/sample - loss: 1.0780 - accuracy: 0.4277 - val_loss: 1.1019 - val_accuracy: 0.3899\n",
      "Epoch 13/300\n",
      "1978/1978 [==============================] - 0s 191us/sample - loss: 1.0636 - accuracy: 0.4358 - val_loss: 1.0996 - val_accuracy: 0.3996\n",
      "Epoch 14/300\n",
      "1978/1978 [==============================] - 0s 183us/sample - loss: 1.0605 - accuracy: 0.4459 - val_loss: 1.0973 - val_accuracy: 0.4035\n",
      "Epoch 15/300\n",
      "1978/1978 [==============================] - 0s 187us/sample - loss: 1.0543 - accuracy: 0.4474 - val_loss: 1.0955 - val_accuracy: 0.4035\n",
      "Epoch 16/300\n",
      "1978/1978 [==============================] - 0s 176us/sample - loss: 1.0532 - accuracy: 0.4540 - val_loss: 1.0937 - val_accuracy: 0.4074\n",
      "Epoch 17/300\n",
      "1978/1978 [==============================] - 0s 179us/sample - loss: 1.0470 - accuracy: 0.4641 - val_loss: 1.0919 - val_accuracy: 0.4113\n",
      "Epoch 18/300\n",
      "1978/1978 [==============================] - 0s 187us/sample - loss: 1.0448 - accuracy: 0.4727 - val_loss: 1.0902 - val_accuracy: 0.4113\n",
      "Epoch 19/300\n",
      "1978/1978 [==============================] - 0s 186us/sample - loss: 1.0366 - accuracy: 0.4722 - val_loss: 1.0885 - val_accuracy: 0.4113\n",
      "Epoch 20/300\n",
      "1978/1978 [==============================] - 0s 188us/sample - loss: 1.0300 - accuracy: 0.4838 - val_loss: 1.0869 - val_accuracy: 0.4113\n",
      "Epoch 21/300\n",
      "1978/1978 [==============================] - 0s 186us/sample - loss: 1.0352 - accuracy: 0.4671 - val_loss: 1.0853 - val_accuracy: 0.4133\n",
      "Epoch 22/300\n",
      "1978/1978 [==============================] - 0s 203us/sample - loss: 1.0209 - accuracy: 0.4899 - val_loss: 1.0837 - val_accuracy: 0.4172\n",
      "Epoch 23/300\n",
      "1978/1978 [==============================] - 0s 187us/sample - loss: 1.0256 - accuracy: 0.4848 - val_loss: 1.0822 - val_accuracy: 0.4191\n",
      "Epoch 24/300\n",
      "1978/1978 [==============================] - 0s 200us/sample - loss: 1.0177 - accuracy: 0.4843 - val_loss: 1.0810 - val_accuracy: 0.4172\n",
      "Epoch 25/300\n",
      "1978/1978 [==============================] - 0s 200us/sample - loss: 1.0167 - accuracy: 0.4909 - val_loss: 1.0796 - val_accuracy: 0.4172\n",
      "Epoch 26/300\n",
      "1978/1978 [==============================] - 0s 181us/sample - loss: 1.0040 - accuracy: 0.5025 - val_loss: 1.0783 - val_accuracy: 0.4191\n",
      "Epoch 27/300\n",
      "1978/1978 [==============================] - 0s 193us/sample - loss: 1.0031 - accuracy: 0.5030 - val_loss: 1.0769 - val_accuracy: 0.4230\n",
      "Epoch 28/300\n",
      "1978/1978 [==============================] - 0s 179us/sample - loss: 1.0064 - accuracy: 0.5005 - val_loss: 1.0758 - val_accuracy: 0.4288\n",
      "Epoch 29/300\n",
      "1978/1978 [==============================] - 0s 194us/sample - loss: 1.0005 - accuracy: 0.5030 - val_loss: 1.0748 - val_accuracy: 0.4308\n",
      "Epoch 30/300\n",
      "1978/1978 [==============================] - 0s 194us/sample - loss: 1.0006 - accuracy: 0.5222 - val_loss: 1.0736 - val_accuracy: 0.4327\n",
      "Epoch 31/300\n",
      "1978/1978 [==============================] - 0s 191us/sample - loss: 0.9944 - accuracy: 0.5222 - val_loss: 1.0725 - val_accuracy: 0.4366\n",
      "Epoch 32/300\n",
      "1978/1978 [==============================] - 0s 185us/sample - loss: 0.9889 - accuracy: 0.5182 - val_loss: 1.0715 - val_accuracy: 0.4288\n",
      "Epoch 33/300\n",
      "1978/1978 [==============================] - 0s 188us/sample - loss: 0.9840 - accuracy: 0.5167 - val_loss: 1.0703 - val_accuracy: 0.4327\n",
      "Epoch 34/300\n",
      "1978/1978 [==============================] - 0s 179us/sample - loss: 0.9799 - accuracy: 0.5334 - val_loss: 1.0695 - val_accuracy: 0.4327\n",
      "Epoch 35/300\n",
      "1978/1978 [==============================] - 0s 175us/sample - loss: 0.9806 - accuracy: 0.5308 - val_loss: 1.0684 - val_accuracy: 0.4327\n",
      "Epoch 36/300\n",
      "1978/1978 [==============================] - 0s 192us/sample - loss: 0.9697 - accuracy: 0.5384 - val_loss: 1.0674 - val_accuracy: 0.4288\n",
      "Epoch 37/300\n",
      "1978/1978 [==============================] - 0s 186us/sample - loss: 0.9761 - accuracy: 0.5344 - val_loss: 1.0664 - val_accuracy: 0.4269\n",
      "Epoch 38/300\n",
      "1978/1978 [==============================] - 0s 182us/sample - loss: 0.9709 - accuracy: 0.5430 - val_loss: 1.0657 - val_accuracy: 0.4308\n",
      "Epoch 39/300\n",
      "1978/1978 [==============================] - 0s 178us/sample - loss: 0.9683 - accuracy: 0.5425 - val_loss: 1.0649 - val_accuracy: 0.4250\n",
      "Epoch 40/300\n",
      "1978/1978 [==============================] - 0s 189us/sample - loss: 0.9606 - accuracy: 0.5501 - val_loss: 1.0642 - val_accuracy: 0.4250\n",
      "Epoch 41/300\n",
      "1978/1978 [==============================] - 0s 181us/sample - loss: 0.9623 - accuracy: 0.5465 - val_loss: 1.0635 - val_accuracy: 0.4308\n",
      "Epoch 42/300\n",
      "1978/1978 [==============================] - 0s 189us/sample - loss: 0.9601 - accuracy: 0.5465 - val_loss: 1.0629 - val_accuracy: 0.4269\n",
      "Epoch 43/300\n",
      "1978/1978 [==============================] - 0s 204us/sample - loss: 0.9475 - accuracy: 0.5571 - val_loss: 1.0621 - val_accuracy: 0.4288\n",
      "Epoch 44/300\n",
      "1978/1978 [==============================] - 0s 191us/sample - loss: 0.9528 - accuracy: 0.5541 - val_loss: 1.0616 - val_accuracy: 0.4308\n",
      "Epoch 45/300\n",
      "1978/1978 [==============================] - 0s 207us/sample - loss: 0.9440 - accuracy: 0.5541 - val_loss: 1.0609 - val_accuracy: 0.4327\n",
      "Epoch 46/300\n",
      "1978/1978 [==============================] - 0s 189us/sample - loss: 0.9452 - accuracy: 0.5627 - val_loss: 1.0603 - val_accuracy: 0.4347\n",
      "Epoch 47/300\n",
      "1978/1978 [==============================] - 0s 198us/sample - loss: 0.9366 - accuracy: 0.5698 - val_loss: 1.0596 - val_accuracy: 0.4347\n",
      "Epoch 48/300\n",
      "1978/1978 [==============================] - 0s 195us/sample - loss: 0.9452 - accuracy: 0.5637 - val_loss: 1.0591 - val_accuracy: 0.4347\n",
      "Epoch 49/300\n",
      "1978/1978 [==============================] - 0s 189us/sample - loss: 0.9368 - accuracy: 0.5763 - val_loss: 1.0584 - val_accuracy: 0.4327\n",
      "Epoch 50/300\n",
      "1978/1978 [==============================] - 0s 188us/sample - loss: 0.9315 - accuracy: 0.5637 - val_loss: 1.0579 - val_accuracy: 0.4308\n",
      "Epoch 51/300\n",
      "1978/1978 [==============================] - 0s 190us/sample - loss: 0.9282 - accuracy: 0.5723 - val_loss: 1.0574 - val_accuracy: 0.4366\n",
      "Epoch 52/300\n",
      "1978/1978 [==============================] - 0s 177us/sample - loss: 0.9232 - accuracy: 0.5738 - val_loss: 1.0569 - val_accuracy: 0.4405\n",
      "Epoch 53/300\n",
      "1978/1978 [==============================] - 0s 183us/sample - loss: 0.9296 - accuracy: 0.5688 - val_loss: 1.0566 - val_accuracy: 0.4425\n",
      "Epoch 54/300\n",
      "1978/1978 [==============================] - 0s 187us/sample - loss: 0.9151 - accuracy: 0.5844 - val_loss: 1.0560 - val_accuracy: 0.4483\n",
      "Epoch 55/300\n",
      "1978/1978 [==============================] - 0s 173us/sample - loss: 0.9160 - accuracy: 0.5799 - val_loss: 1.0556 - val_accuracy: 0.4483\n",
      "Epoch 56/300\n",
      "1978/1978 [==============================] - 0s 189us/sample - loss: 0.9160 - accuracy: 0.5839 - val_loss: 1.0550 - val_accuracy: 0.4464\n",
      "Epoch 57/300\n",
      "1978/1978 [==============================] - 0s 183us/sample - loss: 0.9132 - accuracy: 0.5900 - val_loss: 1.0547 - val_accuracy: 0.4522\n",
      "Epoch 58/300\n",
      "1978/1978 [==============================] - 0s 183us/sample - loss: 0.9100 - accuracy: 0.5824 - val_loss: 1.0543 - val_accuracy: 0.4503\n",
      "Epoch 59/300\n",
      "1978/1978 [==============================] - 0s 188us/sample - loss: 0.9115 - accuracy: 0.5895 - val_loss: 1.0540 - val_accuracy: 0.4503\n",
      "Epoch 60/300\n",
      "1978/1978 [==============================] - 0s 191us/sample - loss: 0.9082 - accuracy: 0.5859 - val_loss: 1.0536 - val_accuracy: 0.4483\n",
      "Epoch 61/300\n",
      "1978/1978 [==============================] - 0s 179us/sample - loss: 0.9000 - accuracy: 0.6057 - val_loss: 1.0533 - val_accuracy: 0.4483\n",
      "Epoch 62/300\n",
      "1978/1978 [==============================] - 0s 192us/sample - loss: 0.9044 - accuracy: 0.5996 - val_loss: 1.0530 - val_accuracy: 0.4503\n",
      "Epoch 63/300\n",
      "1978/1978 [==============================] - 0s 192us/sample - loss: 0.8974 - accuracy: 0.6036 - val_loss: 1.0526 - val_accuracy: 0.4522\n",
      "Epoch 64/300\n",
      "1978/1978 [==============================] - 0s 189us/sample - loss: 0.8961 - accuracy: 0.5961 - val_loss: 1.0525 - val_accuracy: 0.4522\n",
      "Epoch 65/300\n",
      "1978/1978 [==============================] - 0s 192us/sample - loss: 0.8943 - accuracy: 0.5930 - val_loss: 1.0522 - val_accuracy: 0.4561\n",
      "Epoch 66/300\n",
      "1978/1978 [==============================] - 0s 190us/sample - loss: 0.8928 - accuracy: 0.5910 - val_loss: 1.0519 - val_accuracy: 0.4581\n",
      "Epoch 67/300\n",
      "1978/1978 [==============================] - 0s 200us/sample - loss: 0.8908 - accuracy: 0.6001 - val_loss: 1.0517 - val_accuracy: 0.4581\n",
      "Epoch 68/300\n",
      "1978/1978 [==============================] - 0s 193us/sample - loss: 0.8831 - accuracy: 0.6183 - val_loss: 1.0515 - val_accuracy: 0.4581\n",
      "Epoch 69/300\n",
      "1978/1978 [==============================] - 0s 198us/sample - loss: 0.8871 - accuracy: 0.6011 - val_loss: 1.0514 - val_accuracy: 0.4600\n",
      "Epoch 70/300\n",
      "1978/1978 [==============================] - 0s 193us/sample - loss: 0.8922 - accuracy: 0.5981 - val_loss: 1.0512 - val_accuracy: 0.4620\n",
      "Epoch 71/300\n",
      "1978/1978 [==============================] - 0s 190us/sample - loss: 0.8842 - accuracy: 0.6016 - val_loss: 1.0508 - val_accuracy: 0.4620\n",
      "Epoch 72/300\n",
      "1978/1978 [==============================] - 0s 203us/sample - loss: 0.8775 - accuracy: 0.6102 - val_loss: 1.0507 - val_accuracy: 0.4620\n",
      "Epoch 73/300\n",
      "1978/1978 [==============================] - 0s 198us/sample - loss: 0.8770 - accuracy: 0.6077 - val_loss: 1.0506 - val_accuracy: 0.4620\n",
      "Epoch 74/300\n",
      "1978/1978 [==============================] - 0s 190us/sample - loss: 0.8788 - accuracy: 0.6112 - val_loss: 1.0504 - val_accuracy: 0.4620\n",
      "Epoch 75/300\n",
      "1978/1978 [==============================] - 0s 189us/sample - loss: 0.8703 - accuracy: 0.6153 - val_loss: 1.0503 - val_accuracy: 0.4600\n",
      "Epoch 76/300\n",
      "1978/1978 [==============================] - 0s 185us/sample - loss: 0.8661 - accuracy: 0.6143 - val_loss: 1.0503 - val_accuracy: 0.4581\n",
      "Epoch 77/300\n",
      "1978/1978 [==============================] - 0s 174us/sample - loss: 0.8578 - accuracy: 0.6229 - val_loss: 1.0504 - val_accuracy: 0.4581\n",
      "Epoch 78/300\n",
      "1978/1978 [==============================] - 0s 181us/sample - loss: 0.8648 - accuracy: 0.6259 - val_loss: 1.0503 - val_accuracy: 0.4561\n",
      "Epoch 79/300\n",
      "1978/1978 [==============================] - 0s 185us/sample - loss: 0.8621 - accuracy: 0.6052 - val_loss: 1.0504 - val_accuracy: 0.4561\n",
      "Epoch 00079: early stopping\n",
      "265/265 [==============================] - 0s 101us/sample - loss: 1.3303 - accuracy: 0.2415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [36:37, 719.66s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:05<00:11,  5.84s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:11<00:05,  5.91s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:14<00:00,  4.96s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 631.3203125 steps, validate for 176.953125 steps\n",
      "Epoch 1/300\n",
      "632/631 [==============================] - 19s 30ms/step - loss: 1.1512 - accuracy: 0.4105 - val_loss: 1.1146 - val_accuracy: 0.4123\n",
      "Epoch 2/300\n",
      "632/631 [==============================] - 18s 28ms/step - loss: 1.0167 - accuracy: 0.4830 - val_loss: 1.1003 - val_accuracy: 0.4004\n",
      "Epoch 3/300\n",
      "632/631 [==============================] - 18s 28ms/step - loss: 0.9877 - accuracy: 0.5045 - val_loss: 1.0913 - val_accuracy: 0.4033\n",
      "Epoch 4/300\n",
      "632/631 [==============================] - 18s 28ms/step - loss: 0.9694 - accuracy: 0.5208 - val_loss: 1.0910 - val_accuracy: 0.4042\n",
      "Epoch 5/300\n",
      "632/631 [==============================] - 18s 28ms/step - loss: 0.9548 - accuracy: 0.5320 - val_loss: 1.0973 - val_accuracy: 0.4025\n",
      "Epoch 6/300\n",
      "632/631 [==============================] - 18s 28ms/step - loss: 0.9427 - accuracy: 0.5426 - val_loss: 1.0999 - val_accuracy: 0.4016\n",
      "Epoch 7/300\n",
      "632/631 [==============================] - 18s 28ms/step - loss: 0.9320 - accuracy: 0.5520 - val_loss: 1.1014 - val_accuracy: 0.4013\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 631.3203125 steps, validate for 176.953125 steps\n",
      "Epoch 1/300\n",
      "632/631 [==============================] - 42s 66ms/step - loss: 0.9127 - accuracy: 0.5644 - val_loss: 1.1207 - val_accuracy: 0.4073\n",
      "Epoch 2/300\n",
      "632/631 [==============================] - 40s 64ms/step - loss: 0.8821 - accuracy: 0.5863 - val_loss: 1.1471 - val_accuracy: 0.3943\n",
      "Epoch 3/300\n",
      "632/631 [==============================] - 40s 64ms/step - loss: 0.8584 - accuracy: 0.6015 - val_loss: 1.1232 - val_accuracy: 0.4069\n",
      "Epoch 4/300\n",
      "632/631 [==============================] - 41s 64ms/step - loss: 0.8374 - accuracy: 0.6175 - val_loss: 1.1316 - val_accuracy: 0.4118\n",
      "Epoch 00004: early stopping\n",
      "325/325 [==============================] - 0s 939us/sample - loss: 1.1974 - accuracy: 0.3354\n",
      "312/312 [==============================] - 0s 269us/sample - loss: 1.1419 - accuracy: 0.3910\n",
      "308/308 [==============================] - 0s 232us/sample - loss: 1.1345 - accuracy: 0.3506\n",
      "295/295 [==============================] - 0s 257us/sample - loss: 1.1436 - accuracy: 0.4102\n",
      "292/292 [==============================] - 0s 246us/sample - loss: 1.1080 - accuracy: 0.4281\n",
      "287/287 [==============================] - 0s 213us/sample - loss: 1.0979 - accuracy: 0.4495\n",
      "285/285 [==============================] - 0s 250us/sample - loss: 1.0096 - accuracy: 0.5053\n",
      "282/282 [==============================] - 0s 255us/sample - loss: 1.1102 - accuracy: 0.4397\n",
      "282/282 [==============================] - 0s 258us/sample - loss: 1.0942 - accuracy: 0.4326\n",
      "280/280 [==============================] - 0s 281us/sample - loss: 1.1061 - accuracy: 0.4500\n",
      "279/279 [==============================] - 0s 232us/sample - loss: 1.1272 - accuracy: 0.3907\n",
      "278/278 [==============================] - 0s 292us/sample - loss: 1.1180 - accuracy: 0.4604\n",
      "278/278 [==============================] - 0s 296us/sample - loss: 1.0809 - accuracy: 0.4388\n",
      "270/270 [==============================] - 0s 577us/sample - loss: 1.1363 - accuracy: 0.3519\n",
      "269/269 [==============================] - 0s 273us/sample - loss: 1.1508 - accuracy: 0.3755\n",
      "265/265 [==============================] - 0s 509us/sample - loss: 1.1361 - accuracy: 0.4000\n",
      "267/267 [==============================] - 0s 206us/sample - loss: 1.2017 - accuracy: 0.3258\n",
      "269/269 [==============================] - 0s 288us/sample - loss: 1.1545 - accuracy: 0.3457\n",
      "267/267 [==============================] - 0s 255us/sample - loss: 1.1843 - accuracy: 0.3745\n",
      "268/268 [==============================] - 0s 252us/sample - loss: 1.1287 - accuracy: 0.3843\n",
      "265/265 [==============================] - 0s 231us/sample - loss: 1.1924 - accuracy: 0.3396\n",
      "263/263 [==============================] - 0s 248us/sample - loss: 1.2041 - accuracy: 0.3612\n",
      "261/261 [==============================] - 0s 233us/sample - loss: 1.2214 - accuracy: 0.3716\n",
      "264/264 [==============================] - 0s 227us/sample - loss: 1.1832 - accuracy: 0.3409\n",
      "263/263 [==============================] - 0s 264us/sample - loss: 1.1351 - accuracy: 0.4183\n",
      "265/265 [==============================] - 0s 220us/sample - loss: 1.1579 - accuracy: 0.3660\n",
      "262/262 [==============================] - 0s 231us/sample - loss: 1.1743 - accuracy: 0.3779\n",
      "260/260 [==============================] - 0s 212us/sample - loss: 1.1436 - accuracy: 0.3846\n",
      "259/259 [==============================] - 0s 217us/sample - loss: 1.1768 - accuracy: 0.3320\n",
      "255/255 [==============================] - 0s 222us/sample - loss: 1.1636 - accuracy: 0.4235\n",
      "256/256 [==============================] - 0s 967us/sample - loss: 1.1404 - accuracy: 0.3984\n",
      "253/253 [==============================] - 0s 225us/sample - loss: 1.1702 - accuracy: 0.3992\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 1980 samples, validate on 543 samples\n",
      "Epoch 1/300\n",
      "1980/1980 [==============================] - 2s 1ms/sample - loss: 1.1868 - accuracy: 0.3000 - val_loss: 1.1775 - val_accuracy: 0.2818\n",
      "Epoch 2/300\n",
      "1980/1980 [==============================] - 0s 181us/sample - loss: 1.1621 - accuracy: 0.3394 - val_loss: 1.1659 - val_accuracy: 0.2891\n",
      "Epoch 3/300\n",
      "1980/1980 [==============================] - 0s 188us/sample - loss: 1.1545 - accuracy: 0.3323 - val_loss: 1.1575 - val_accuracy: 0.2910\n",
      "Epoch 4/300\n",
      "1980/1980 [==============================] - 0s 195us/sample - loss: 1.1443 - accuracy: 0.3399 - val_loss: 1.1509 - val_accuracy: 0.2947\n",
      "Epoch 5/300\n",
      "1980/1980 [==============================] - 0s 179us/sample - loss: 1.1323 - accuracy: 0.3566 - val_loss: 1.1452 - val_accuracy: 0.2910\n",
      "Epoch 6/300\n",
      "1980/1980 [==============================] - 0s 188us/sample - loss: 1.1211 - accuracy: 0.3571 - val_loss: 1.1403 - val_accuracy: 0.2910\n",
      "Epoch 7/300\n",
      "1980/1980 [==============================] - 0s 200us/sample - loss: 1.1222 - accuracy: 0.3692 - val_loss: 1.1360 - val_accuracy: 0.3002\n",
      "Epoch 8/300\n",
      "1980/1980 [==============================] - 0s 189us/sample - loss: 1.1082 - accuracy: 0.3833 - val_loss: 1.1325 - val_accuracy: 0.3112\n",
      "Epoch 9/300\n",
      "1980/1980 [==============================] - 0s 207us/sample - loss: 1.1097 - accuracy: 0.3904 - val_loss: 1.1292 - val_accuracy: 0.3241\n",
      "Epoch 10/300\n",
      "1980/1980 [==============================] - 0s 192us/sample - loss: 1.1025 - accuracy: 0.3838 - val_loss: 1.1258 - val_accuracy: 0.3297\n",
      "Epoch 11/300\n",
      "1980/1980 [==============================] - 0s 195us/sample - loss: 1.0999 - accuracy: 0.3838 - val_loss: 1.1229 - val_accuracy: 0.3315\n",
      "Epoch 12/300\n",
      "1980/1980 [==============================] - 0s 187us/sample - loss: 1.0888 - accuracy: 0.3909 - val_loss: 1.1202 - val_accuracy: 0.3370\n",
      "Epoch 13/300\n",
      "1980/1980 [==============================] - 0s 185us/sample - loss: 1.0826 - accuracy: 0.4091 - val_loss: 1.1179 - val_accuracy: 0.3389\n",
      "Epoch 14/300\n",
      "1980/1980 [==============================] - 0s 180us/sample - loss: 1.0849 - accuracy: 0.4136 - val_loss: 1.1153 - val_accuracy: 0.3462\n",
      "Epoch 15/300\n",
      "1980/1980 [==============================] - 0s 179us/sample - loss: 1.0838 - accuracy: 0.4030 - val_loss: 1.1131 - val_accuracy: 0.3462\n",
      "Epoch 16/300\n",
      "1980/1980 [==============================] - 0s 187us/sample - loss: 1.0731 - accuracy: 0.4121 - val_loss: 1.1110 - val_accuracy: 0.3462\n",
      "Epoch 17/300\n",
      "1980/1980 [==============================] - 0s 187us/sample - loss: 1.0769 - accuracy: 0.4121 - val_loss: 1.1089 - val_accuracy: 0.3554\n",
      "Epoch 18/300\n",
      "1980/1980 [==============================] - 0s 194us/sample - loss: 1.0668 - accuracy: 0.4182 - val_loss: 1.1070 - val_accuracy: 0.3665\n",
      "Epoch 19/300\n",
      "1980/1980 [==============================] - 0s 190us/sample - loss: 1.0735 - accuracy: 0.4202 - val_loss: 1.1053 - val_accuracy: 0.3646\n",
      "Epoch 20/300\n",
      "1980/1980 [==============================] - 0s 183us/sample - loss: 1.0659 - accuracy: 0.4207 - val_loss: 1.1035 - val_accuracy: 0.3702\n",
      "Epoch 21/300\n",
      "1980/1980 [==============================] - 0s 189us/sample - loss: 1.0634 - accuracy: 0.4258 - val_loss: 1.1019 - val_accuracy: 0.3738\n",
      "Epoch 22/300\n",
      "1980/1980 [==============================] - 0s 186us/sample - loss: 1.0598 - accuracy: 0.4258 - val_loss: 1.1003 - val_accuracy: 0.3738\n",
      "Epoch 23/300\n",
      "1980/1980 [==============================] - 0s 193us/sample - loss: 1.0543 - accuracy: 0.4419 - val_loss: 1.0989 - val_accuracy: 0.3775\n",
      "Epoch 24/300\n",
      "1980/1980 [==============================] - 0s 183us/sample - loss: 1.0488 - accuracy: 0.4389 - val_loss: 1.0975 - val_accuracy: 0.3831\n",
      "Epoch 25/300\n",
      "1980/1980 [==============================] - 0s 182us/sample - loss: 1.0464 - accuracy: 0.4641 - val_loss: 1.0960 - val_accuracy: 0.3849\n",
      "Epoch 26/300\n",
      "1980/1980 [==============================] - 0s 213us/sample - loss: 1.0427 - accuracy: 0.4455 - val_loss: 1.0947 - val_accuracy: 0.3886\n",
      "Epoch 27/300\n",
      "1980/1980 [==============================] - 0s 193us/sample - loss: 1.0455 - accuracy: 0.4399 - val_loss: 1.0935 - val_accuracy: 0.3904\n",
      "Epoch 28/300\n",
      "1980/1980 [==============================] - 0s 191us/sample - loss: 1.0342 - accuracy: 0.4606 - val_loss: 1.0923 - val_accuracy: 0.3867\n",
      "Epoch 29/300\n",
      "1980/1980 [==============================] - 0s 199us/sample - loss: 1.0349 - accuracy: 0.4697 - val_loss: 1.0911 - val_accuracy: 0.3831\n",
      "Epoch 30/300\n",
      "1980/1980 [==============================] - 0s 176us/sample - loss: 1.0370 - accuracy: 0.4732 - val_loss: 1.0900 - val_accuracy: 0.3831\n",
      "Epoch 31/300\n",
      "1980/1980 [==============================] - 0s 183us/sample - loss: 1.0311 - accuracy: 0.4621 - val_loss: 1.0888 - val_accuracy: 0.3831\n",
      "Epoch 32/300\n",
      "1980/1980 [==============================] - 0s 182us/sample - loss: 1.0254 - accuracy: 0.4859 - val_loss: 1.0878 - val_accuracy: 0.3867\n",
      "Epoch 33/300\n",
      "1980/1980 [==============================] - 0s 192us/sample - loss: 1.0212 - accuracy: 0.4788 - val_loss: 1.0868 - val_accuracy: 0.3831\n",
      "Epoch 34/300\n",
      "1980/1980 [==============================] - 0s 196us/sample - loss: 1.0255 - accuracy: 0.4621 - val_loss: 1.0858 - val_accuracy: 0.3831\n",
      "Epoch 35/300\n",
      "1980/1980 [==============================] - 0s 182us/sample - loss: 1.0282 - accuracy: 0.4601 - val_loss: 1.0849 - val_accuracy: 0.3886\n",
      "Epoch 36/300\n",
      "1980/1980 [==============================] - 0s 182us/sample - loss: 1.0191 - accuracy: 0.4722 - val_loss: 1.0839 - val_accuracy: 0.3904\n",
      "Epoch 37/300\n",
      "1980/1980 [==============================] - 0s 182us/sample - loss: 1.0295 - accuracy: 0.4788 - val_loss: 1.0830 - val_accuracy: 0.3904\n",
      "Epoch 38/300\n",
      "1980/1980 [==============================] - 0s 188us/sample - loss: 1.0143 - accuracy: 0.4874 - val_loss: 1.0822 - val_accuracy: 0.3941\n",
      "Epoch 39/300\n",
      "1980/1980 [==============================] - 0s 184us/sample - loss: 1.0083 - accuracy: 0.4929 - val_loss: 1.0814 - val_accuracy: 0.3941\n",
      "Epoch 40/300\n",
      "1980/1980 [==============================] - 0s 193us/sample - loss: 1.0112 - accuracy: 0.4874 - val_loss: 1.0807 - val_accuracy: 0.3923\n",
      "Epoch 41/300\n",
      "1980/1980 [==============================] - 0s 194us/sample - loss: 1.0102 - accuracy: 0.4848 - val_loss: 1.0800 - val_accuracy: 0.3923\n",
      "Epoch 42/300\n",
      "1980/1980 [==============================] - 0s 204us/sample - loss: 1.0074 - accuracy: 0.4975 - val_loss: 1.0792 - val_accuracy: 0.3886\n",
      "Epoch 43/300\n",
      "1980/1980 [==============================] - 0s 190us/sample - loss: 1.0082 - accuracy: 0.4894 - val_loss: 1.0786 - val_accuracy: 0.3886\n",
      "Epoch 44/300\n",
      "1980/1980 [==============================] - 0s 190us/sample - loss: 1.0026 - accuracy: 0.4939 - val_loss: 1.0780 - val_accuracy: 0.3941\n",
      "Epoch 45/300\n",
      "1980/1980 [==============================] - 0s 182us/sample - loss: 0.9991 - accuracy: 0.4843 - val_loss: 1.0774 - val_accuracy: 0.3941\n",
      "Epoch 46/300\n",
      "1980/1980 [==============================] - 0s 183us/sample - loss: 1.0010 - accuracy: 0.5040 - val_loss: 1.0767 - val_accuracy: 0.3978\n",
      "Epoch 47/300\n",
      "1980/1980 [==============================] - 0s 173us/sample - loss: 0.9994 - accuracy: 0.4960 - val_loss: 1.0762 - val_accuracy: 0.3978\n",
      "Epoch 48/300\n",
      "1980/1980 [==============================] - 0s 191us/sample - loss: 0.9896 - accuracy: 0.5071 - val_loss: 1.0757 - val_accuracy: 0.3959\n",
      "Epoch 49/300\n",
      "1980/1980 [==============================] - 0s 185us/sample - loss: 0.9923 - accuracy: 0.4924 - val_loss: 1.0751 - val_accuracy: 0.3959\n",
      "Epoch 50/300\n",
      "1980/1980 [==============================] - 0s 175us/sample - loss: 0.9928 - accuracy: 0.5045 - val_loss: 1.0747 - val_accuracy: 0.3959\n",
      "Epoch 51/300\n",
      "1980/1980 [==============================] - 0s 198us/sample - loss: 0.9926 - accuracy: 0.5010 - val_loss: 1.0741 - val_accuracy: 0.3941\n",
      "Epoch 52/300\n",
      "1980/1980 [==============================] - 0s 186us/sample - loss: 0.9835 - accuracy: 0.5106 - val_loss: 1.0736 - val_accuracy: 0.3978\n",
      "Epoch 53/300\n",
      "1980/1980 [==============================] - 0s 198us/sample - loss: 0.9828 - accuracy: 0.5217 - val_loss: 1.0732 - val_accuracy: 0.3959\n",
      "Epoch 54/300\n",
      "1980/1980 [==============================] - 0s 186us/sample - loss: 0.9797 - accuracy: 0.5222 - val_loss: 1.0729 - val_accuracy: 0.3978\n",
      "Epoch 55/300\n",
      "1980/1980 [==============================] - 0s 183us/sample - loss: 0.9802 - accuracy: 0.5263 - val_loss: 1.0725 - val_accuracy: 0.3978\n",
      "Epoch 56/300\n",
      "1980/1980 [==============================] - 0s 189us/sample - loss: 0.9838 - accuracy: 0.5162 - val_loss: 1.0721 - val_accuracy: 0.3978\n",
      "Epoch 57/300\n",
      "1980/1980 [==============================] - 0s 189us/sample - loss: 0.9796 - accuracy: 0.5182 - val_loss: 1.0717 - val_accuracy: 0.3978\n",
      "Epoch 58/300\n",
      "1980/1980 [==============================] - 0s 189us/sample - loss: 0.9748 - accuracy: 0.5152 - val_loss: 1.0713 - val_accuracy: 0.3978\n",
      "Epoch 59/300\n",
      "1980/1980 [==============================] - 0s 189us/sample - loss: 0.9743 - accuracy: 0.5374 - val_loss: 1.0710 - val_accuracy: 0.4015\n",
      "Epoch 60/300\n",
      "1980/1980 [==============================] - 0s 190us/sample - loss: 0.9757 - accuracy: 0.5141 - val_loss: 1.0705 - val_accuracy: 0.4015\n",
      "Epoch 61/300\n",
      "1980/1980 [==============================] - 0s 199us/sample - loss: 0.9663 - accuracy: 0.5232 - val_loss: 1.0703 - val_accuracy: 0.4088\n",
      "Epoch 62/300\n",
      "1980/1980 [==============================] - 0s 192us/sample - loss: 0.9637 - accuracy: 0.5318 - val_loss: 1.0700 - val_accuracy: 0.4125\n",
      "Epoch 63/300\n",
      "1980/1980 [==============================] - 0s 204us/sample - loss: 0.9639 - accuracy: 0.5303 - val_loss: 1.0698 - val_accuracy: 0.4070\n",
      "Epoch 64/300\n",
      "1980/1980 [==============================] - 0s 192us/sample - loss: 0.9750 - accuracy: 0.5222 - val_loss: 1.0696 - val_accuracy: 0.4070\n",
      "Epoch 65/300\n",
      "1980/1980 [==============================] - 0s 195us/sample - loss: 0.9612 - accuracy: 0.5379 - val_loss: 1.0694 - val_accuracy: 0.4088\n",
      "Epoch 66/300\n",
      "1980/1980 [==============================] - 0s 181us/sample - loss: 0.9598 - accuracy: 0.5308 - val_loss: 1.0691 - val_accuracy: 0.4088\n",
      "Epoch 67/300\n",
      "1980/1980 [==============================] - 0s 179us/sample - loss: 0.9633 - accuracy: 0.5374 - val_loss: 1.0690 - val_accuracy: 0.4088\n",
      "Epoch 68/300\n",
      "1980/1980 [==============================] - 0s 190us/sample - loss: 0.9533 - accuracy: 0.5404 - val_loss: 1.0687 - val_accuracy: 0.4107\n",
      "Epoch 69/300\n",
      "1980/1980 [==============================] - 0s 188us/sample - loss: 0.9569 - accuracy: 0.5343 - val_loss: 1.0686 - val_accuracy: 0.4107\n",
      "Epoch 70/300\n",
      "1980/1980 [==============================] - 0s 189us/sample - loss: 0.9500 - accuracy: 0.5439 - val_loss: 1.0684 - val_accuracy: 0.4070\n",
      "Epoch 71/300\n",
      "1980/1980 [==============================] - 0s 183us/sample - loss: 0.9522 - accuracy: 0.5384 - val_loss: 1.0683 - val_accuracy: 0.4088\n",
      "Epoch 72/300\n",
      "1980/1980 [==============================] - 0s 182us/sample - loss: 0.9461 - accuracy: 0.5455 - val_loss: 1.0681 - val_accuracy: 0.4107\n",
      "Epoch 73/300\n",
      "1980/1980 [==============================] - 0s 188us/sample - loss: 0.9547 - accuracy: 0.5460 - val_loss: 1.0681 - val_accuracy: 0.4107\n",
      "Epoch 74/300\n",
      "1980/1980 [==============================] - 0s 196us/sample - loss: 0.9482 - accuracy: 0.5566 - val_loss: 1.0679 - val_accuracy: 0.4107\n",
      "Epoch 75/300\n",
      "1980/1980 [==============================] - 0s 198us/sample - loss: 0.9443 - accuracy: 0.5485 - val_loss: 1.0678 - val_accuracy: 0.4125\n",
      "Epoch 76/300\n",
      "1980/1980 [==============================] - 0s 182us/sample - loss: 0.9479 - accuracy: 0.5465 - val_loss: 1.0677 - val_accuracy: 0.4125\n",
      "Epoch 77/300\n",
      "1980/1980 [==============================] - 0s 189us/sample - loss: 0.9482 - accuracy: 0.5419 - val_loss: 1.0677 - val_accuracy: 0.4180\n",
      "Epoch 78/300\n",
      "1980/1980 [==============================] - 0s 185us/sample - loss: 0.9356 - accuracy: 0.5480 - val_loss: 1.0676 - val_accuracy: 0.4180\n",
      "Epoch 79/300\n",
      "1980/1980 [==============================] - 0s 184us/sample - loss: 0.9315 - accuracy: 0.5707 - val_loss: 1.0675 - val_accuracy: 0.4199\n",
      "Epoch 80/300\n",
      "1980/1980 [==============================] - 0s 184us/sample - loss: 0.9349 - accuracy: 0.5591 - val_loss: 1.0675 - val_accuracy: 0.4199\n",
      "Epoch 81/300\n",
      "1980/1980 [==============================] - 0s 199us/sample - loss: 0.9351 - accuracy: 0.5540 - val_loss: 1.0674 - val_accuracy: 0.4217\n",
      "Epoch 82/300\n",
      "1980/1980 [==============================] - 0s 204us/sample - loss: 0.9379 - accuracy: 0.5551 - val_loss: 1.0674 - val_accuracy: 0.4217\n",
      "Epoch 83/300\n",
      "1980/1980 [==============================] - 0s 191us/sample - loss: 0.9345 - accuracy: 0.5621 - val_loss: 1.0675 - val_accuracy: 0.4217\n",
      "Epoch 84/300\n",
      "1980/1980 [==============================] - 0s 196us/sample - loss: 0.9298 - accuracy: 0.5586 - val_loss: 1.0675 - val_accuracy: 0.4236\n",
      "Epoch 00084: early stopping\n",
      "233/233 [==============================] - 0s 117us/sample - loss: 1.0389 - accuracy: 0.4764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [48:08, 710.89s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:06<00:12,  6.27s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:12<00:06,  6.22s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.09s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 628.515625 steps, validate for 183.390625 steps\n",
      "Epoch 1/300\n",
      "629/628 [==============================] - 19s 30ms/step - loss: 1.0967 - accuracy: 0.4314 - val_loss: 1.1254 - val_accuracy: 0.3852\n",
      "Epoch 2/300\n",
      "629/628 [==============================] - 17s 28ms/step - loss: 1.0149 - accuracy: 0.4844 - val_loss: 1.1103 - val_accuracy: 0.3915\n",
      "Epoch 3/300\n",
      "629/628 [==============================] - 18s 28ms/step - loss: 0.9875 - accuracy: 0.5049 - val_loss: 1.1050 - val_accuracy: 0.3950\n",
      "Epoch 4/300\n",
      "629/628 [==============================] - 18s 28ms/step - loss: 0.9698 - accuracy: 0.5193 - val_loss: 1.1030 - val_accuracy: 0.3995\n",
      "Epoch 5/300\n",
      "629/628 [==============================] - 17s 28ms/step - loss: 0.9558 - accuracy: 0.5324 - val_loss: 1.1032 - val_accuracy: 0.4000\n",
      "Epoch 6/300\n",
      "629/628 [==============================] - 17s 28ms/step - loss: 0.9442 - accuracy: 0.5422 - val_loss: 1.1026 - val_accuracy: 0.4032\n",
      "Epoch 7/300\n",
      "629/628 [==============================] - 18s 28ms/step - loss: 0.9340 - accuracy: 0.5495 - val_loss: 1.0948 - val_accuracy: 0.4116\n",
      "Epoch 8/300\n",
      "629/628 [==============================] - 18s 28ms/step - loss: 0.9251 - accuracy: 0.5567 - val_loss: 1.1021 - val_accuracy: 0.4113\n",
      "Epoch 9/300\n",
      "629/628 [==============================] - 18s 28ms/step - loss: 0.9168 - accuracy: 0.5621 - val_loss: 1.1106 - val_accuracy: 0.4058\n",
      "Epoch 10/300\n",
      "629/628 [==============================] - 18s 28ms/step - loss: 0.9099 - accuracy: 0.5665 - val_loss: 1.1146 - val_accuracy: 0.4076\n",
      "Epoch 00010: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 628.515625 steps, validate for 183.390625 steps\n",
      "Epoch 1/300\n",
      "629/628 [==============================] - 42s 66ms/step - loss: 0.8954 - accuracy: 0.5763 - val_loss: 1.0894 - val_accuracy: 0.4273\n",
      "Epoch 2/300\n",
      "629/628 [==============================] - 40s 64ms/step - loss: 0.8683 - accuracy: 0.5957 - val_loss: 1.1903 - val_accuracy: 0.3810\n",
      "Epoch 3/300\n",
      "629/628 [==============================] - 41s 65ms/step - loss: 0.8477 - accuracy: 0.6097 - val_loss: 1.1399 - val_accuracy: 0.4145\n",
      "Epoch 4/300\n",
      "629/628 [==============================] - 41s 64ms/step - loss: 0.8289 - accuracy: 0.6239 - val_loss: 1.1243 - val_accuracy: 0.4289\n",
      "Epoch 00004: early stopping\n",
      "318/318 [==============================] - 0s 1ms/sample - loss: 1.3413 - accuracy: 0.2453\n",
      "306/306 [==============================] - 0s 203us/sample - loss: 1.3001 - accuracy: 0.2582\n",
      "301/301 [==============================] - 0s 238us/sample - loss: 1.3178 - accuracy: 0.2193\n",
      "301/301 [==============================] - 0s 266us/sample - loss: 1.3587 - accuracy: 0.2591\n",
      "295/295 [==============================] - 0s 229us/sample - loss: 1.3344 - accuracy: 0.2576\n",
      "295/295 [==============================] - 0s 243us/sample - loss: 1.3296 - accuracy: 0.2814\n",
      "286/286 [==============================] - 0s 226us/sample - loss: 1.3692 - accuracy: 0.2657\n",
      "281/281 [==============================] - 0s 221us/sample - loss: 1.3425 - accuracy: 0.2491\n",
      "278/278 [==============================] - 0s 236us/sample - loss: 1.3519 - accuracy: 0.2626\n",
      "276/276 [==============================] - 0s 223us/sample - loss: 1.3299 - accuracy: 0.2754\n",
      "276/276 [==============================] - 0s 232us/sample - loss: 1.3550 - accuracy: 0.2101\n",
      "270/270 [==============================] - 0s 229us/sample - loss: 1.3563 - accuracy: 0.2556\n",
      "268/268 [==============================] - 0s 233us/sample - loss: 1.2995 - accuracy: 0.2500\n",
      "260/260 [==============================] - 0s 260us/sample - loss: 1.3832 - accuracy: 0.2192\n",
      "257/257 [==============================] - 0s 269us/sample - loss: 1.3559 - accuracy: 0.2568\n",
      "253/253 [==============================] - 0s 276us/sample - loss: 1.3129 - accuracy: 0.3083\n",
      "252/252 [==============================] - 0s 231us/sample - loss: 1.3099 - accuracy: 0.2500\n",
      "253/253 [==============================] - 0s 220us/sample - loss: 1.3116 - accuracy: 0.2885\n",
      "252/252 [==============================] - 0s 233us/sample - loss: 1.3130 - accuracy: 0.2937\n",
      "248/248 [==============================] - 0s 225us/sample - loss: 1.2737 - accuracy: 0.2984\n",
      "245/245 [==============================] - 0s 261us/sample - loss: 1.4023 - accuracy: 0.2286\n",
      "243/243 [==============================] - 0s 250us/sample - loss: 1.2820 - accuracy: 0.2675\n",
      "244/244 [==============================] - 0s 264us/sample - loss: 1.3461 - accuracy: 0.2500\n",
      "242/242 [==============================] - 0s 271us/sample - loss: 1.2795 - accuracy: 0.2727\n",
      "237/237 [==============================] - 0s 232us/sample - loss: 1.2899 - accuracy: 0.2574\n",
      "237/237 [==============================] - 0s 262us/sample - loss: 1.2618 - accuracy: 0.2996\n",
      "233/233 [==============================] - 0s 264us/sample - loss: 1.2924 - accuracy: 0.2704\n",
      "228/228 [==============================] - 0s 284us/sample - loss: 1.2726 - accuracy: 0.2851\n",
      "227/227 [==============================] - 0s 256us/sample - loss: 1.2827 - accuracy: 0.2996\n",
      "223/223 [==============================] - 0s 229us/sample - loss: 1.1506 - accuracy: 0.4126\n",
      "220/220 [==============================] - 0s 258us/sample - loss: 1.1877 - accuracy: 0.3409\n",
      "214/214 [==============================] - 0s 232us/sample - loss: 1.1818 - accuracy: 0.3505\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 1998 samples, validate on 561 samples\n",
      "Epoch 1/300\n",
      "1998/1998 [==============================] - 2s 1ms/sample - loss: 1.1540 - accuracy: 0.3278 - val_loss: 1.1733 - val_accuracy: 0.3244\n",
      "Epoch 2/300\n",
      "1998/1998 [==============================] - 0s 182us/sample - loss: 1.1327 - accuracy: 0.3433 - val_loss: 1.1634 - val_accuracy: 0.3422\n",
      "Epoch 3/300\n",
      "1998/1998 [==============================] - 0s 183us/sample - loss: 1.1184 - accuracy: 0.3694 - val_loss: 1.1569 - val_accuracy: 0.3512\n",
      "Epoch 4/300\n",
      "1998/1998 [==============================] - 0s 195us/sample - loss: 1.1118 - accuracy: 0.3774 - val_loss: 1.1523 - val_accuracy: 0.3547\n",
      "Epoch 5/300\n",
      "1998/1998 [==============================] - 0s 188us/sample - loss: 1.1031 - accuracy: 0.3824 - val_loss: 1.1485 - val_accuracy: 0.3583\n",
      "Epoch 6/300\n",
      "1998/1998 [==============================] - 0s 189us/sample - loss: 1.0927 - accuracy: 0.3924 - val_loss: 1.1453 - val_accuracy: 0.3529\n",
      "Epoch 7/300\n",
      "1998/1998 [==============================] - 0s 192us/sample - loss: 1.0840 - accuracy: 0.4114 - val_loss: 1.1424 - val_accuracy: 0.3547\n",
      "Epoch 8/300\n",
      "1998/1998 [==============================] - 0s 188us/sample - loss: 1.0827 - accuracy: 0.4069 - val_loss: 1.1400 - val_accuracy: 0.3583\n",
      "Epoch 9/300\n",
      "1998/1998 [==============================] - 0s 187us/sample - loss: 1.0756 - accuracy: 0.4059 - val_loss: 1.1380 - val_accuracy: 0.3636\n",
      "Epoch 10/300\n",
      "1998/1998 [==============================] - 0s 178us/sample - loss: 1.0651 - accuracy: 0.4314 - val_loss: 1.1360 - val_accuracy: 0.3672\n",
      "Epoch 11/300\n",
      "1998/1998 [==============================] - 0s 192us/sample - loss: 1.0659 - accuracy: 0.4274 - val_loss: 1.1343 - val_accuracy: 0.3690\n",
      "Epoch 12/300\n",
      "1998/1998 [==============================] - 0s 197us/sample - loss: 1.0638 - accuracy: 0.4224 - val_loss: 1.1327 - val_accuracy: 0.3743\n",
      "Epoch 13/300\n",
      "1998/1998 [==============================] - 0s 184us/sample - loss: 1.0588 - accuracy: 0.4289 - val_loss: 1.1311 - val_accuracy: 0.3708\n",
      "Epoch 14/300\n",
      "1998/1998 [==============================] - 0s 197us/sample - loss: 1.0592 - accuracy: 0.4354 - val_loss: 1.1298 - val_accuracy: 0.3672\n",
      "Epoch 15/300\n",
      "1998/1998 [==============================] - 0s 182us/sample - loss: 1.0557 - accuracy: 0.4419 - val_loss: 1.1284 - val_accuracy: 0.3708\n",
      "Epoch 16/300\n",
      "1998/1998 [==============================] - 0s 173us/sample - loss: 1.0455 - accuracy: 0.4384 - val_loss: 1.1273 - val_accuracy: 0.3743\n",
      "Epoch 17/300\n",
      "1998/1998 [==============================] - 0s 187us/sample - loss: 1.0440 - accuracy: 0.4354 - val_loss: 1.1262 - val_accuracy: 0.3743\n",
      "Epoch 18/300\n",
      "1998/1998 [==============================] - 0s 192us/sample - loss: 1.0456 - accuracy: 0.4585 - val_loss: 1.1251 - val_accuracy: 0.3725\n",
      "Epoch 19/300\n",
      "1998/1998 [==============================] - 0s 197us/sample - loss: 1.0414 - accuracy: 0.4595 - val_loss: 1.1242 - val_accuracy: 0.3743\n",
      "Epoch 20/300\n",
      "1998/1998 [==============================] - 0s 196us/sample - loss: 1.0325 - accuracy: 0.4570 - val_loss: 1.1232 - val_accuracy: 0.3743\n",
      "Epoch 21/300\n",
      "1998/1998 [==============================] - 0s 200us/sample - loss: 1.0403 - accuracy: 0.4434 - val_loss: 1.1223 - val_accuracy: 0.3708\n",
      "Epoch 22/300\n",
      "1998/1998 [==============================] - 0s 195us/sample - loss: 1.0402 - accuracy: 0.4374 - val_loss: 1.1216 - val_accuracy: 0.3690\n",
      "Epoch 23/300\n",
      "1998/1998 [==============================] - 0s 183us/sample - loss: 1.0288 - accuracy: 0.4620 - val_loss: 1.1209 - val_accuracy: 0.3708\n",
      "Epoch 24/300\n",
      "1998/1998 [==============================] - 0s 185us/sample - loss: 1.0300 - accuracy: 0.4545 - val_loss: 1.1202 - val_accuracy: 0.3690\n",
      "Epoch 25/300\n",
      "1998/1998 [==============================] - 0s 183us/sample - loss: 1.0247 - accuracy: 0.4810 - val_loss: 1.1194 - val_accuracy: 0.3690\n",
      "Epoch 26/300\n",
      "1998/1998 [==============================] - 0s 186us/sample - loss: 1.0144 - accuracy: 0.4740 - val_loss: 1.1188 - val_accuracy: 0.3708\n",
      "Epoch 27/300\n",
      "1998/1998 [==============================] - 0s 176us/sample - loss: 1.0213 - accuracy: 0.4635 - val_loss: 1.1182 - val_accuracy: 0.3743\n",
      "Epoch 28/300\n",
      "1998/1998 [==============================] - 0s 186us/sample - loss: 1.0123 - accuracy: 0.4680 - val_loss: 1.1175 - val_accuracy: 0.3725\n",
      "Epoch 29/300\n",
      "1998/1998 [==============================] - 0s 193us/sample - loss: 1.0163 - accuracy: 0.4740 - val_loss: 1.1168 - val_accuracy: 0.3743\n",
      "Epoch 30/300\n",
      "1998/1998 [==============================] - 0s 193us/sample - loss: 1.0123 - accuracy: 0.4795 - val_loss: 1.1162 - val_accuracy: 0.3743\n",
      "Epoch 31/300\n",
      "1998/1998 [==============================] - 0s 205us/sample - loss: 1.0083 - accuracy: 0.4680 - val_loss: 1.1156 - val_accuracy: 0.3761\n",
      "Epoch 32/300\n",
      "1998/1998 [==============================] - 0s 190us/sample - loss: 1.0123 - accuracy: 0.4830 - val_loss: 1.1151 - val_accuracy: 0.3779\n",
      "Epoch 33/300\n",
      "1998/1998 [==============================] - 0s 179us/sample - loss: 1.0030 - accuracy: 0.4900 - val_loss: 1.1146 - val_accuracy: 0.3779\n",
      "Epoch 34/300\n",
      "1998/1998 [==============================] - 0s 193us/sample - loss: 1.0027 - accuracy: 0.4945 - val_loss: 1.1141 - val_accuracy: 0.3797\n",
      "Epoch 35/300\n",
      "1998/1998 [==============================] - 0s 197us/sample - loss: 1.0040 - accuracy: 0.4810 - val_loss: 1.1137 - val_accuracy: 0.3815\n",
      "Epoch 36/300\n",
      "1998/1998 [==============================] - 0s 183us/sample - loss: 0.9934 - accuracy: 0.5015 - val_loss: 1.1131 - val_accuracy: 0.3815\n",
      "Epoch 37/300\n",
      "1998/1998 [==============================] - 0s 182us/sample - loss: 0.9959 - accuracy: 0.4995 - val_loss: 1.1127 - val_accuracy: 0.3832\n",
      "Epoch 38/300\n",
      "1998/1998 [==============================] - 0s 199us/sample - loss: 0.9957 - accuracy: 0.5015 - val_loss: 1.1124 - val_accuracy: 0.3850\n",
      "Epoch 39/300\n",
      "1998/1998 [==============================] - 0s 200us/sample - loss: 0.9914 - accuracy: 0.4925 - val_loss: 1.1118 - val_accuracy: 0.3850\n",
      "Epoch 40/300\n",
      "1998/1998 [==============================] - 0s 191us/sample - loss: 0.9857 - accuracy: 0.4975 - val_loss: 1.1114 - val_accuracy: 0.3850\n",
      "Epoch 41/300\n",
      "1998/1998 [==============================] - 0s 188us/sample - loss: 0.9891 - accuracy: 0.4985 - val_loss: 1.1110 - val_accuracy: 0.3850\n",
      "Epoch 42/300\n",
      "1998/1998 [==============================] - 0s 209us/sample - loss: 0.9859 - accuracy: 0.5010 - val_loss: 1.1106 - val_accuracy: 0.3850\n",
      "Epoch 43/300\n",
      "1998/1998 [==============================] - 0s 187us/sample - loss: 0.9909 - accuracy: 0.4995 - val_loss: 1.1101 - val_accuracy: 0.3868\n",
      "Epoch 44/300\n",
      "1998/1998 [==============================] - 0s 185us/sample - loss: 0.9793 - accuracy: 0.4965 - val_loss: 1.1098 - val_accuracy: 0.3868\n",
      "Epoch 45/300\n",
      "1998/1998 [==============================] - 0s 181us/sample - loss: 0.9869 - accuracy: 0.4970 - val_loss: 1.1094 - val_accuracy: 0.3886\n",
      "Epoch 46/300\n",
      "1998/1998 [==============================] - 0s 186us/sample - loss: 0.9781 - accuracy: 0.4975 - val_loss: 1.1091 - val_accuracy: 0.3868\n",
      "Epoch 47/300\n",
      "1998/1998 [==============================] - 0s 181us/sample - loss: 0.9796 - accuracy: 0.5030 - val_loss: 1.1088 - val_accuracy: 0.3868\n",
      "Epoch 48/300\n",
      "1998/1998 [==============================] - 0s 191us/sample - loss: 0.9746 - accuracy: 0.5165 - val_loss: 1.1085 - val_accuracy: 0.3886\n",
      "Epoch 49/300\n",
      "1998/1998 [==============================] - 0s 181us/sample - loss: 0.9721 - accuracy: 0.5135 - val_loss: 1.1081 - val_accuracy: 0.3868\n",
      "Epoch 50/300\n",
      "1998/1998 [==============================] - 0s 180us/sample - loss: 0.9727 - accuracy: 0.5105 - val_loss: 1.1078 - val_accuracy: 0.3868\n",
      "Epoch 51/300\n",
      "1998/1998 [==============================] - 0s 204us/sample - loss: 0.9707 - accuracy: 0.5040 - val_loss: 1.1076 - val_accuracy: 0.3868\n",
      "Epoch 52/300\n",
      "1998/1998 [==============================] - 0s 212us/sample - loss: 0.9657 - accuracy: 0.5225 - val_loss: 1.1073 - val_accuracy: 0.3868\n",
      "Epoch 53/300\n",
      "1998/1998 [==============================] - 0s 197us/sample - loss: 0.9661 - accuracy: 0.5240 - val_loss: 1.1071 - val_accuracy: 0.3868\n",
      "Epoch 54/300\n",
      "1998/1998 [==============================] - 0s 209us/sample - loss: 0.9602 - accuracy: 0.5310 - val_loss: 1.1069 - val_accuracy: 0.3850\n",
      "Epoch 55/300\n",
      "1998/1998 [==============================] - 0s 199us/sample - loss: 0.9661 - accuracy: 0.5160 - val_loss: 1.1067 - val_accuracy: 0.3850\n",
      "Epoch 56/300\n",
      "1998/1998 [==============================] - 0s 203us/sample - loss: 0.9621 - accuracy: 0.5215 - val_loss: 1.1066 - val_accuracy: 0.3832\n",
      "Epoch 57/300\n",
      "1998/1998 [==============================] - 0s 187us/sample - loss: 0.9494 - accuracy: 0.5430 - val_loss: 1.1064 - val_accuracy: 0.3797\n",
      "Epoch 58/300\n",
      "1998/1998 [==============================] - 0s 169us/sample - loss: 0.9533 - accuracy: 0.5325 - val_loss: 1.1063 - val_accuracy: 0.3815\n",
      "Epoch 59/300\n",
      "1998/1998 [==============================] - 0s 172us/sample - loss: 0.9510 - accuracy: 0.5260 - val_loss: 1.1060 - val_accuracy: 0.3815\n",
      "Epoch 60/300\n",
      "1998/1998 [==============================] - 0s 185us/sample - loss: 0.9509 - accuracy: 0.5350 - val_loss: 1.1060 - val_accuracy: 0.3779\n",
      "Epoch 61/300\n",
      "1998/1998 [==============================] - 0s 190us/sample - loss: 0.9494 - accuracy: 0.5320 - val_loss: 1.1057 - val_accuracy: 0.3779\n",
      "Epoch 62/300\n",
      "1998/1998 [==============================] - 0s 191us/sample - loss: 0.9531 - accuracy: 0.5205 - val_loss: 1.1055 - val_accuracy: 0.3761\n",
      "Epoch 63/300\n",
      "1998/1998 [==============================] - 0s 204us/sample - loss: 0.9446 - accuracy: 0.5340 - val_loss: 1.1055 - val_accuracy: 0.3761\n",
      "Epoch 64/300\n",
      "1998/1998 [==============================] - 0s 195us/sample - loss: 0.9458 - accuracy: 0.5325 - val_loss: 1.1054 - val_accuracy: 0.3743\n",
      "Epoch 65/300\n",
      "1998/1998 [==============================] - 0s 201us/sample - loss: 0.9435 - accuracy: 0.5395 - val_loss: 1.1052 - val_accuracy: 0.3743\n",
      "Epoch 66/300\n",
      "1998/1998 [==============================] - 0s 187us/sample - loss: 0.9487 - accuracy: 0.5200 - val_loss: 1.1052 - val_accuracy: 0.3761\n",
      "Epoch 67/300\n",
      "1998/1998 [==============================] - 0s 194us/sample - loss: 0.9410 - accuracy: 0.5460 - val_loss: 1.1051 - val_accuracy: 0.3761\n",
      "Epoch 68/300\n",
      "1998/1998 [==============================] - 0s 190us/sample - loss: 0.9385 - accuracy: 0.5315 - val_loss: 1.1049 - val_accuracy: 0.3797\n",
      "Epoch 69/300\n",
      "1998/1998 [==============================] - 0s 177us/sample - loss: 0.9368 - accuracy: 0.5385 - val_loss: 1.1048 - val_accuracy: 0.3797\n",
      "Epoch 70/300\n",
      "1998/1998 [==============================] - 0s 189us/sample - loss: 0.9379 - accuracy: 0.5430 - val_loss: 1.1047 - val_accuracy: 0.3797\n",
      "Epoch 71/300\n",
      "1998/1998 [==============================] - 0s 177us/sample - loss: 0.9358 - accuracy: 0.5405 - val_loss: 1.1046 - val_accuracy: 0.3797\n",
      "Epoch 72/300\n",
      "1998/1998 [==============================] - 0s 185us/sample - loss: 0.9294 - accuracy: 0.5440 - val_loss: 1.1046 - val_accuracy: 0.3797\n",
      "Epoch 73/300\n",
      "1998/1998 [==============================] - 0s 183us/sample - loss: 0.9288 - accuracy: 0.5485 - val_loss: 1.1046 - val_accuracy: 0.3779\n",
      "Epoch 74/300\n",
      "1998/1998 [==============================] - 0s 185us/sample - loss: 0.9197 - accuracy: 0.5661 - val_loss: 1.1047 - val_accuracy: 0.3779\n",
      "Epoch 75/300\n",
      "1998/1998 [==============================] - 0s 183us/sample - loss: 0.9221 - accuracy: 0.5521 - val_loss: 1.1046 - val_accuracy: 0.3797\n",
      "Epoch 00075: early stopping\n",
      "197/197 [==============================] - 0s 134us/sample - loss: 1.0334 - accuracy: 0.4264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [1:00:36, 727.35s/it]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:05<00:11,  5.87s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:12<00:06,  6.09s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.08s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 586.0078125 steps, validate for 212.5390625 steps\n",
      "Epoch 1/300\n",
      "587/586 [==============================] - 18s 31ms/step - loss: 1.1121 - accuracy: 0.4408 - val_loss: 1.0924 - val_accuracy: 0.4417\n",
      "Epoch 2/300\n",
      "587/586 [==============================] - 17s 29ms/step - loss: 1.0044 - accuracy: 0.5004 - val_loss: 1.0745 - val_accuracy: 0.4593\n",
      "Epoch 3/300\n",
      "587/586 [==============================] - 17s 29ms/step - loss: 0.9742 - accuracy: 0.5246 - val_loss: 1.0607 - val_accuracy: 0.4677\n",
      "Epoch 4/300\n",
      "587/586 [==============================] - 17s 29ms/step - loss: 0.9555 - accuracy: 0.5363 - val_loss: 1.0599 - val_accuracy: 0.4658\n",
      "Epoch 5/300\n",
      "587/586 [==============================] - 17s 29ms/step - loss: 0.9409 - accuracy: 0.5488 - val_loss: 1.0559 - val_accuracy: 0.4730\n",
      "Epoch 6/300\n",
      "587/586 [==============================] - 17s 29ms/step - loss: 0.9287 - accuracy: 0.5572 - val_loss: 1.0642 - val_accuracy: 0.4709\n",
      "Epoch 7/300\n",
      "587/586 [==============================] - 17s 30ms/step - loss: 0.9180 - accuracy: 0.5636 - val_loss: 1.0651 - val_accuracy: 0.4661\n",
      "Epoch 8/300\n",
      "587/586 [==============================] - 17s 29ms/step - loss: 0.9085 - accuracy: 0.5721 - val_loss: 1.0697 - val_accuracy: 0.4700\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 586.0078125 steps, validate for 212.5390625 steps\n",
      "Epoch 1/300\n",
      "587/586 [==============================] - 39s 67ms/step - loss: 0.8905 - accuracy: 0.5855 - val_loss: 1.1493 - val_accuracy: 0.4445\n",
      "Epoch 2/300\n",
      "587/586 [==============================] - 38s 65ms/step - loss: 0.8611 - accuracy: 0.6047 - val_loss: 1.0654 - val_accuracy: 0.4702\n",
      "Epoch 3/300\n",
      "587/586 [==============================] - 38s 65ms/step - loss: 0.8372 - accuracy: 0.6227 - val_loss: 1.0808 - val_accuracy: 0.4707\n",
      "Epoch 4/300\n",
      "587/586 [==============================] - 39s 66ms/step - loss: 0.8165 - accuracy: 0.6363 - val_loss: 1.0720 - val_accuracy: 0.4793\n",
      "Epoch 5/300\n",
      "587/586 [==============================] - 38s 65ms/step - loss: 0.7974 - accuracy: 0.6464 - val_loss: 1.1079 - val_accuracy: 0.4895\n",
      "Epoch 00005: early stopping\n",
      "380/380 [==============================] - 0s 800us/sample - loss: 1.5984 - accuracy: 0.1605\n",
      "369/369 [==============================] - 0s 233us/sample - loss: 1.6519 - accuracy: 0.1463\n",
      "364/364 [==============================] - 0s 205us/sample - loss: 1.6909 - accuracy: 0.1401\n",
      "360/360 [==============================] - 0s 223us/sample - loss: 1.6295 - accuracy: 0.1472\n",
      "355/355 [==============================] - 0s 242us/sample - loss: 1.6629 - accuracy: 0.1718\n",
      "346/346 [==============================] - 0s 243us/sample - loss: 1.6732 - accuracy: 0.1850\n",
      "343/343 [==============================] - 0s 249us/sample - loss: 1.6401 - accuracy: 0.1749\n",
      "336/336 [==============================] - 0s 276us/sample - loss: 1.6587 - accuracy: 0.1607\n",
      "332/332 [==============================] - 0s 248us/sample - loss: 1.7022 - accuracy: 0.1777\n",
      "328/328 [==============================] - 0s 260us/sample - loss: 1.7461 - accuracy: 0.1646\n",
      "323/323 [==============================] - 0s 232us/sample - loss: 1.6658 - accuracy: 0.1858\n",
      "322/322 [==============================] - 0s 223us/sample - loss: 1.7648 - accuracy: 0.1801\n",
      "313/313 [==============================] - 0s 231us/sample - loss: 1.6295 - accuracy: 0.2396\n",
      "312/312 [==============================] - 0s 228us/sample - loss: 1.6452 - accuracy: 0.2051\n",
      "310/310 [==============================] - 0s 245us/sample - loss: 1.6842 - accuracy: 0.2387\n",
      "308/308 [==============================] - 0s 233us/sample - loss: 1.7016 - accuracy: 0.2273\n",
      "305/305 [==============================] - 0s 253us/sample - loss: 1.6827 - accuracy: 0.2033\n",
      "301/301 [==============================] - 0s 263us/sample - loss: 1.6939 - accuracy: 0.2392\n",
      "299/299 [==============================] - 0s 259us/sample - loss: 1.7127 - accuracy: 0.2174\n",
      "298/298 [==============================] - 0s 262us/sample - loss: 1.7350 - accuracy: 0.2047\n",
      "293/293 [==============================] - 0s 257us/sample - loss: 1.6259 - accuracy: 0.2423\n",
      "291/291 [==============================] - 0s 249us/sample - loss: 1.6825 - accuracy: 0.2096\n",
      "291/291 [==============================] - 0s 242us/sample - loss: 1.6613 - accuracy: 0.2165\n",
      "291/291 [==============================] - 0s 233us/sample - loss: 1.6200 - accuracy: 0.1890\n",
      "292/292 [==============================] - 0s 271us/sample - loss: 1.6607 - accuracy: 0.2295\n",
      "287/287 [==============================] - 0s 218us/sample - loss: 1.7194 - accuracy: 0.1533\n",
      "286/286 [==============================] - 0s 226us/sample - loss: 1.7094 - accuracy: 0.1958\n",
      "286/286 [==============================] - 0s 222us/sample - loss: 1.6680 - accuracy: 0.1993\n",
      "279/279 [==============================] - 0s 262us/sample - loss: 1.5834 - accuracy: 0.2401\n",
      "277/277 [==============================] - 0s 253us/sample - loss: 1.7158 - accuracy: 0.1697\n",
      "277/277 [==============================] - 0s 223us/sample - loss: 1.6600 - accuracy: 0.1805\n",
      "275/275 [==============================] - 0s 240us/sample - loss: 1.7631 - accuracy: 0.1818\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 1821 samples, validate on 688 samples\n",
      "Epoch 1/300\n",
      "1821/1821 [==============================] - 3s 1ms/sample - loss: 1.1243 - accuracy: 0.3806 - val_loss: 1.1217 - val_accuracy: 0.3590\n",
      "Epoch 2/300\n",
      "1821/1821 [==============================] - 0s 200us/sample - loss: 1.0967 - accuracy: 0.4064 - val_loss: 1.1129 - val_accuracy: 0.3706\n",
      "Epoch 3/300\n",
      "1821/1821 [==============================] - 0s 200us/sample - loss: 1.0866 - accuracy: 0.4267 - val_loss: 1.1066 - val_accuracy: 0.3765\n",
      "Epoch 4/300\n",
      "1821/1821 [==============================] - 0s 192us/sample - loss: 1.0784 - accuracy: 0.4300 - val_loss: 1.1019 - val_accuracy: 0.3852\n",
      "Epoch 5/300\n",
      "1821/1821 [==============================] - 0s 182us/sample - loss: 1.0747 - accuracy: 0.4388 - val_loss: 1.0980 - val_accuracy: 0.3895\n",
      "Epoch 6/300\n",
      "1821/1821 [==============================] - 0s 190us/sample - loss: 1.0753 - accuracy: 0.4322 - val_loss: 1.0945 - val_accuracy: 0.3895\n",
      "Epoch 7/300\n",
      "1821/1821 [==============================] - 0s 194us/sample - loss: 1.0652 - accuracy: 0.4481 - val_loss: 1.0914 - val_accuracy: 0.3968\n",
      "Epoch 8/300\n",
      "1821/1821 [==============================] - 0s 202us/sample - loss: 1.0574 - accuracy: 0.4596 - val_loss: 1.0887 - val_accuracy: 0.3997\n",
      "Epoch 9/300\n",
      "1821/1821 [==============================] - 0s 205us/sample - loss: 1.0578 - accuracy: 0.4684 - val_loss: 1.0863 - val_accuracy: 0.4041\n",
      "Epoch 10/300\n",
      "1821/1821 [==============================] - 0s 201us/sample - loss: 1.0461 - accuracy: 0.4756 - val_loss: 1.0840 - val_accuracy: 0.4084\n",
      "Epoch 11/300\n",
      "1821/1821 [==============================] - 0s 196us/sample - loss: 1.0430 - accuracy: 0.4717 - val_loss: 1.0820 - val_accuracy: 0.4172\n",
      "Epoch 12/300\n",
      "1821/1821 [==============================] - 0s 203us/sample - loss: 1.0449 - accuracy: 0.4673 - val_loss: 1.0801 - val_accuracy: 0.4230\n",
      "Epoch 13/300\n",
      "1821/1821 [==============================] - 0s 204us/sample - loss: 1.0376 - accuracy: 0.4811 - val_loss: 1.0784 - val_accuracy: 0.4230\n",
      "Epoch 14/300\n",
      "1821/1821 [==============================] - 0s 236us/sample - loss: 1.0329 - accuracy: 0.4772 - val_loss: 1.0766 - val_accuracy: 0.4244\n",
      "Epoch 15/300\n",
      "1821/1821 [==============================] - 0s 213us/sample - loss: 1.0257 - accuracy: 0.4789 - val_loss: 1.0750 - val_accuracy: 0.4273\n",
      "Epoch 16/300\n",
      "1821/1821 [==============================] - 0s 209us/sample - loss: 1.0276 - accuracy: 0.4953 - val_loss: 1.0735 - val_accuracy: 0.4244\n",
      "Epoch 17/300\n",
      "1821/1821 [==============================] - 0s 205us/sample - loss: 1.0194 - accuracy: 0.4937 - val_loss: 1.0722 - val_accuracy: 0.4273\n",
      "Epoch 18/300\n",
      "1821/1821 [==============================] - 0s 198us/sample - loss: 1.0199 - accuracy: 0.5036 - val_loss: 1.0708 - val_accuracy: 0.4331\n",
      "Epoch 19/300\n",
      "1821/1821 [==============================] - 0s 202us/sample - loss: 1.0242 - accuracy: 0.4915 - val_loss: 1.0695 - val_accuracy: 0.4375\n",
      "Epoch 20/300\n",
      "1821/1821 [==============================] - 0s 194us/sample - loss: 1.0135 - accuracy: 0.4970 - val_loss: 1.0683 - val_accuracy: 0.4360\n",
      "Epoch 21/300\n",
      "1821/1821 [==============================] - 0s 200us/sample - loss: 1.0129 - accuracy: 0.4970 - val_loss: 1.0671 - val_accuracy: 0.4360\n",
      "Epoch 22/300\n",
      "1821/1821 [==============================] - 0s 175us/sample - loss: 1.0102 - accuracy: 0.4904 - val_loss: 1.0659 - val_accuracy: 0.4360\n",
      "Epoch 23/300\n",
      "1821/1821 [==============================] - 0s 190us/sample - loss: 1.0132 - accuracy: 0.5030 - val_loss: 1.0649 - val_accuracy: 0.4375\n",
      "Epoch 24/300\n",
      "1821/1821 [==============================] - 0s 189us/sample - loss: 1.0012 - accuracy: 0.5025 - val_loss: 1.0639 - val_accuracy: 0.4390\n",
      "Epoch 25/300\n",
      "1821/1821 [==============================] - 0s 201us/sample - loss: 1.0003 - accuracy: 0.5063 - val_loss: 1.0629 - val_accuracy: 0.4390\n",
      "Epoch 26/300\n",
      "1821/1821 [==============================] - 0s 182us/sample - loss: 1.0106 - accuracy: 0.5074 - val_loss: 1.0620 - val_accuracy: 0.4419\n",
      "Epoch 27/300\n",
      "1821/1821 [==============================] - 0s 187us/sample - loss: 0.9992 - accuracy: 0.5118 - val_loss: 1.0610 - val_accuracy: 0.4433\n",
      "Epoch 28/300\n",
      "1821/1821 [==============================] - 0s 204us/sample - loss: 0.9972 - accuracy: 0.5151 - val_loss: 1.0601 - val_accuracy: 0.4433\n",
      "Epoch 29/300\n",
      "1821/1821 [==============================] - 0s 201us/sample - loss: 0.9928 - accuracy: 0.5063 - val_loss: 1.0592 - val_accuracy: 0.4448\n",
      "Epoch 30/300\n",
      "1821/1821 [==============================] - 0s 191us/sample - loss: 0.9902 - accuracy: 0.5250 - val_loss: 1.0584 - val_accuracy: 0.4462\n",
      "Epoch 31/300\n",
      "1821/1821 [==============================] - 0s 183us/sample - loss: 0.9889 - accuracy: 0.5162 - val_loss: 1.0576 - val_accuracy: 0.4477\n",
      "Epoch 32/300\n",
      "1821/1821 [==============================] - 0s 191us/sample - loss: 0.9883 - accuracy: 0.5233 - val_loss: 1.0568 - val_accuracy: 0.4491\n",
      "Epoch 33/300\n",
      "1821/1821 [==============================] - 0s 185us/sample - loss: 0.9861 - accuracy: 0.5283 - val_loss: 1.0561 - val_accuracy: 0.4491\n",
      "Epoch 34/300\n",
      "1821/1821 [==============================] - 0s 184us/sample - loss: 0.9800 - accuracy: 0.5244 - val_loss: 1.0554 - val_accuracy: 0.4520\n",
      "Epoch 35/300\n",
      "1821/1821 [==============================] - 0s 181us/sample - loss: 0.9782 - accuracy: 0.5294 - val_loss: 1.0547 - val_accuracy: 0.4491\n",
      "Epoch 36/300\n",
      "1821/1821 [==============================] - 0s 213us/sample - loss: 0.9767 - accuracy: 0.5272 - val_loss: 1.0540 - val_accuracy: 0.4506\n",
      "Epoch 37/300\n",
      "1821/1821 [==============================] - 0s 196us/sample - loss: 0.9810 - accuracy: 0.5299 - val_loss: 1.0533 - val_accuracy: 0.4491\n",
      "Epoch 38/300\n",
      "1821/1821 [==============================] - 0s 190us/sample - loss: 0.9738 - accuracy: 0.5470 - val_loss: 1.0528 - val_accuracy: 0.4491\n",
      "Epoch 39/300\n",
      "1821/1821 [==============================] - 0s 209us/sample - loss: 0.9733 - accuracy: 0.5233 - val_loss: 1.0522 - val_accuracy: 0.4491\n",
      "Epoch 40/300\n",
      "1821/1821 [==============================] - 0s 194us/sample - loss: 0.9735 - accuracy: 0.5420 - val_loss: 1.0516 - val_accuracy: 0.4477\n",
      "Epoch 41/300\n",
      "1821/1821 [==============================] - 0s 195us/sample - loss: 0.9698 - accuracy: 0.5382 - val_loss: 1.0510 - val_accuracy: 0.4477\n",
      "Epoch 42/300\n",
      "1821/1821 [==============================] - 0s 205us/sample - loss: 0.9709 - accuracy: 0.5211 - val_loss: 1.0505 - val_accuracy: 0.4477\n",
      "Epoch 43/300\n",
      "1821/1821 [==============================] - 0s 209us/sample - loss: 0.9730 - accuracy: 0.5327 - val_loss: 1.0499 - val_accuracy: 0.4477\n",
      "Epoch 44/300\n",
      "1821/1821 [==============================] - 0s 186us/sample - loss: 0.9660 - accuracy: 0.5316 - val_loss: 1.0494 - val_accuracy: 0.4491\n",
      "Epoch 45/300\n",
      "1821/1821 [==============================] - 0s 179us/sample - loss: 0.9637 - accuracy: 0.5437 - val_loss: 1.0487 - val_accuracy: 0.4506\n",
      "Epoch 46/300\n",
      "1821/1821 [==============================] - 0s 189us/sample - loss: 0.9564 - accuracy: 0.5486 - val_loss: 1.0482 - val_accuracy: 0.4506\n",
      "Epoch 47/300\n",
      "1821/1821 [==============================] - 0s 194us/sample - loss: 0.9525 - accuracy: 0.5343 - val_loss: 1.0477 - val_accuracy: 0.4520\n",
      "Epoch 48/300\n",
      "1821/1821 [==============================] - 0s 206us/sample - loss: 0.9538 - accuracy: 0.5470 - val_loss: 1.0472 - val_accuracy: 0.4535\n",
      "Epoch 49/300\n",
      "1821/1821 [==============================] - 0s 184us/sample - loss: 0.9570 - accuracy: 0.5470 - val_loss: 1.0467 - val_accuracy: 0.4535\n",
      "Epoch 50/300\n",
      "1821/1821 [==============================] - 0s 199us/sample - loss: 0.9506 - accuracy: 0.5574 - val_loss: 1.0462 - val_accuracy: 0.4535\n",
      "Epoch 51/300\n",
      "1821/1821 [==============================] - 0s 208us/sample - loss: 0.9440 - accuracy: 0.5508 - val_loss: 1.0458 - val_accuracy: 0.4535\n",
      "Epoch 52/300\n",
      "1821/1821 [==============================] - 0s 208us/sample - loss: 0.9414 - accuracy: 0.5695 - val_loss: 1.0453 - val_accuracy: 0.4506\n",
      "Epoch 53/300\n",
      "1821/1821 [==============================] - 0s 185us/sample - loss: 0.9422 - accuracy: 0.5502 - val_loss: 1.0449 - val_accuracy: 0.4506\n",
      "Epoch 54/300\n",
      "1821/1821 [==============================] - 0s 202us/sample - loss: 0.9516 - accuracy: 0.5497 - val_loss: 1.0445 - val_accuracy: 0.4520\n",
      "Epoch 55/300\n",
      "1821/1821 [==============================] - 0s 198us/sample - loss: 0.9412 - accuracy: 0.5629 - val_loss: 1.0440 - val_accuracy: 0.4520\n",
      "Epoch 56/300\n",
      "1821/1821 [==============================] - 0s 188us/sample - loss: 0.9468 - accuracy: 0.5513 - val_loss: 1.0436 - val_accuracy: 0.4549\n",
      "Epoch 57/300\n",
      "1821/1821 [==============================] - 0s 201us/sample - loss: 0.9428 - accuracy: 0.5491 - val_loss: 1.0432 - val_accuracy: 0.4549\n",
      "Epoch 58/300\n",
      "1821/1821 [==============================] - 0s 178us/sample - loss: 0.9375 - accuracy: 0.5557 - val_loss: 1.0428 - val_accuracy: 0.4549\n",
      "Epoch 59/300\n",
      "1821/1821 [==============================] - 0s 195us/sample - loss: 0.9317 - accuracy: 0.5750 - val_loss: 1.0423 - val_accuracy: 0.4564\n",
      "Epoch 60/300\n",
      "1821/1821 [==============================] - 0s 190us/sample - loss: 0.9369 - accuracy: 0.5629 - val_loss: 1.0420 - val_accuracy: 0.4564\n",
      "Epoch 61/300\n",
      "1821/1821 [==============================] - 0s 192us/sample - loss: 0.9293 - accuracy: 0.5574 - val_loss: 1.0416 - val_accuracy: 0.4564\n",
      "Epoch 62/300\n",
      "1821/1821 [==============================] - 0s 201us/sample - loss: 0.9337 - accuracy: 0.5744 - val_loss: 1.0414 - val_accuracy: 0.4564\n",
      "Epoch 63/300\n",
      "1821/1821 [==============================] - 0s 204us/sample - loss: 0.9300 - accuracy: 0.5739 - val_loss: 1.0410 - val_accuracy: 0.4564\n",
      "Epoch 64/300\n",
      "1821/1821 [==============================] - 0s 207us/sample - loss: 0.9307 - accuracy: 0.5563 - val_loss: 1.0408 - val_accuracy: 0.4564\n",
      "Epoch 65/300\n",
      "1821/1821 [==============================] - 0s 202us/sample - loss: 0.9312 - accuracy: 0.5596 - val_loss: 1.0404 - val_accuracy: 0.4578\n",
      "Epoch 66/300\n",
      "1821/1821 [==============================] - 0s 207us/sample - loss: 0.9229 - accuracy: 0.5722 - val_loss: 1.0401 - val_accuracy: 0.4564\n",
      "Epoch 67/300\n",
      "1821/1821 [==============================] - 0s 197us/sample - loss: 0.9242 - accuracy: 0.5662 - val_loss: 1.0397 - val_accuracy: 0.4564\n",
      "Epoch 68/300\n",
      "1821/1821 [==============================] - 0s 195us/sample - loss: 0.9164 - accuracy: 0.5837 - val_loss: 1.0394 - val_accuracy: 0.4564\n",
      "Epoch 69/300\n",
      "1821/1821 [==============================] - 0s 206us/sample - loss: 0.9139 - accuracy: 0.5788 - val_loss: 1.0392 - val_accuracy: 0.4608\n",
      "Epoch 70/300\n",
      "1821/1821 [==============================] - 0s 205us/sample - loss: 0.9089 - accuracy: 0.5892 - val_loss: 1.0389 - val_accuracy: 0.4608\n",
      "Epoch 71/300\n",
      "1821/1821 [==============================] - 0s 184us/sample - loss: 0.9116 - accuracy: 0.5761 - val_loss: 1.0386 - val_accuracy: 0.4608\n",
      "Epoch 72/300\n",
      "1821/1821 [==============================] - 0s 202us/sample - loss: 0.9141 - accuracy: 0.5744 - val_loss: 1.0384 - val_accuracy: 0.4608\n",
      "Epoch 73/300\n",
      "1821/1821 [==============================] - 0s 184us/sample - loss: 0.9180 - accuracy: 0.5711 - val_loss: 1.0380 - val_accuracy: 0.4578\n",
      "Epoch 74/300\n",
      "1821/1821 [==============================] - 0s 186us/sample - loss: 0.9119 - accuracy: 0.5848 - val_loss: 1.0378 - val_accuracy: 0.4593\n",
      "Epoch 75/300\n",
      "1821/1821 [==============================] - 0s 199us/sample - loss: 0.9118 - accuracy: 0.5777 - val_loss: 1.0377 - val_accuracy: 0.4593\n",
      "Epoch 76/300\n",
      "1821/1821 [==============================] - 0s 202us/sample - loss: 0.9044 - accuracy: 0.5953 - val_loss: 1.0373 - val_accuracy: 0.4622\n",
      "Epoch 77/300\n",
      "1821/1821 [==============================] - 0s 206us/sample - loss: 0.9064 - accuracy: 0.5914 - val_loss: 1.0371 - val_accuracy: 0.4637\n",
      "Epoch 78/300\n",
      "1821/1821 [==============================] - 0s 198us/sample - loss: 0.9098 - accuracy: 0.5881 - val_loss: 1.0369 - val_accuracy: 0.4622\n",
      "Epoch 79/300\n",
      "1821/1821 [==============================] - 0s 187us/sample - loss: 0.9037 - accuracy: 0.5848 - val_loss: 1.0366 - val_accuracy: 0.4622\n",
      "Epoch 80/300\n",
      "1821/1821 [==============================] - 0s 185us/sample - loss: 0.8997 - accuracy: 0.5881 - val_loss: 1.0365 - val_accuracy: 0.4637\n",
      "Epoch 81/300\n",
      "1821/1821 [==============================] - 0s 193us/sample - loss: 0.8920 - accuracy: 0.5969 - val_loss: 1.0363 - val_accuracy: 0.4608\n",
      "Epoch 82/300\n",
      "1821/1821 [==============================] - 0s 201us/sample - loss: 0.9002 - accuracy: 0.5898 - val_loss: 1.0361 - val_accuracy: 0.4593\n",
      "Epoch 83/300\n",
      "1821/1821 [==============================] - 0s 209us/sample - loss: 0.8936 - accuracy: 0.5931 - val_loss: 1.0360 - val_accuracy: 0.4593\n",
      "Epoch 84/300\n",
      "1821/1821 [==============================] - 0s 203us/sample - loss: 0.8916 - accuracy: 0.5925 - val_loss: 1.0358 - val_accuracy: 0.4578\n",
      "Epoch 85/300\n",
      "1821/1821 [==============================] - 0s 204us/sample - loss: 0.8884 - accuracy: 0.5947 - val_loss: 1.0356 - val_accuracy: 0.4578\n",
      "Epoch 86/300\n",
      "1821/1821 [==============================] - 0s 208us/sample - loss: 0.8889 - accuracy: 0.5843 - val_loss: 1.0354 - val_accuracy: 0.4578\n",
      "Epoch 87/300\n",
      "1821/1821 [==============================] - 0s 209us/sample - loss: 0.8937 - accuracy: 0.5986 - val_loss: 1.0352 - val_accuracy: 0.4564\n",
      "Epoch 88/300\n",
      "1821/1821 [==============================] - 0s 196us/sample - loss: 0.8893 - accuracy: 0.5991 - val_loss: 1.0350 - val_accuracy: 0.4622\n",
      "Epoch 89/300\n",
      "1821/1821 [==============================] - 0s 193us/sample - loss: 0.8833 - accuracy: 0.5986 - val_loss: 1.0348 - val_accuracy: 0.4622\n",
      "Epoch 90/300\n",
      "1821/1821 [==============================] - 0s 198us/sample - loss: 0.8911 - accuracy: 0.5953 - val_loss: 1.0346 - val_accuracy: 0.4593\n",
      "Epoch 91/300\n",
      "1821/1821 [==============================] - 0s 191us/sample - loss: 0.8823 - accuracy: 0.6074 - val_loss: 1.0344 - val_accuracy: 0.4593\n",
      "Epoch 92/300\n",
      "1821/1821 [==============================] - 0s 202us/sample - loss: 0.8837 - accuracy: 0.6008 - val_loss: 1.0344 - val_accuracy: 0.4608\n",
      "Epoch 93/300\n",
      "1821/1821 [==============================] - 0s 207us/sample - loss: 0.8734 - accuracy: 0.6041 - val_loss: 1.0342 - val_accuracy: 0.4622\n",
      "Epoch 94/300\n",
      "1821/1821 [==============================] - 0s 211us/sample - loss: 0.8796 - accuracy: 0.5947 - val_loss: 1.0340 - val_accuracy: 0.4637\n",
      "Epoch 95/300\n",
      "1821/1821 [==============================] - 0s 195us/sample - loss: 0.8778 - accuracy: 0.6035 - val_loss: 1.0339 - val_accuracy: 0.4651\n",
      "Epoch 96/300\n",
      "1821/1821 [==============================] - 0s 196us/sample - loss: 0.8758 - accuracy: 0.6030 - val_loss: 1.0338 - val_accuracy: 0.4651\n",
      "Epoch 97/300\n",
      "1821/1821 [==============================] - 0s 194us/sample - loss: 0.8782 - accuracy: 0.6090 - val_loss: 1.0336 - val_accuracy: 0.4666\n",
      "Epoch 98/300\n",
      "1821/1821 [==============================] - 0s 195us/sample - loss: 0.8702 - accuracy: 0.6074 - val_loss: 1.0336 - val_accuracy: 0.4680\n",
      "Epoch 99/300\n",
      "1821/1821 [==============================] - 0s 201us/sample - loss: 0.8715 - accuracy: 0.6101 - val_loss: 1.0335 - val_accuracy: 0.4666\n",
      "Epoch 100/300\n",
      "1821/1821 [==============================] - 0s 195us/sample - loss: 0.8695 - accuracy: 0.6101 - val_loss: 1.0335 - val_accuracy: 0.4651\n",
      "Epoch 101/300\n",
      "1821/1821 [==============================] - 0s 212us/sample - loss: 0.8685 - accuracy: 0.6205 - val_loss: 1.0333 - val_accuracy: 0.4651\n",
      "Epoch 102/300\n",
      "1821/1821 [==============================] - 0s 195us/sample - loss: 0.8630 - accuracy: 0.6216 - val_loss: 1.0332 - val_accuracy: 0.4651\n",
      "Epoch 103/300\n",
      "1821/1821 [==============================] - 0s 200us/sample - loss: 0.8572 - accuracy: 0.6255 - val_loss: 1.0331 - val_accuracy: 0.4666\n",
      "Epoch 104/300\n",
      "1821/1821 [==============================] - 0s 210us/sample - loss: 0.8648 - accuracy: 0.6129 - val_loss: 1.0330 - val_accuracy: 0.4651\n",
      "Epoch 105/300\n",
      "1821/1821 [==============================] - 0s 197us/sample - loss: 0.8625 - accuracy: 0.6161 - val_loss: 1.0329 - val_accuracy: 0.4666\n",
      "Epoch 106/300\n",
      "1821/1821 [==============================] - 0s 196us/sample - loss: 0.8541 - accuracy: 0.6282 - val_loss: 1.0329 - val_accuracy: 0.4666\n",
      "Epoch 107/300\n",
      "1821/1821 [==============================] - 0s 195us/sample - loss: 0.8548 - accuracy: 0.6244 - val_loss: 1.0329 - val_accuracy: 0.4680\n",
      "Epoch 108/300\n",
      "1821/1821 [==============================] - 0s 189us/sample - loss: 0.8586 - accuracy: 0.6216 - val_loss: 1.0327 - val_accuracy: 0.4651\n",
      "Epoch 109/300\n",
      "1821/1821 [==============================] - 0s 205us/sample - loss: 0.8518 - accuracy: 0.6090 - val_loss: 1.0327 - val_accuracy: 0.4651\n",
      "Epoch 110/300\n",
      "1821/1821 [==============================] - 0s 193us/sample - loss: 0.8612 - accuracy: 0.6139 - val_loss: 1.0326 - val_accuracy: 0.4637\n",
      "Epoch 111/300\n",
      "1821/1821 [==============================] - 0s 196us/sample - loss: 0.8438 - accuracy: 0.6359 - val_loss: 1.0324 - val_accuracy: 0.4637\n",
      "Epoch 112/300\n",
      "1821/1821 [==============================] - 0s 202us/sample - loss: 0.8524 - accuracy: 0.6249 - val_loss: 1.0324 - val_accuracy: 0.4637\n",
      "Epoch 113/300\n",
      "1821/1821 [==============================] - 0s 206us/sample - loss: 0.8481 - accuracy: 0.6233 - val_loss: 1.0324 - val_accuracy: 0.4666\n",
      "Epoch 114/300\n",
      "1821/1821 [==============================] - 0s 217us/sample - loss: 0.8373 - accuracy: 0.6282 - val_loss: 1.0324 - val_accuracy: 0.4680\n",
      "Epoch 115/300\n",
      "1821/1821 [==============================] - 0s 203us/sample - loss: 0.8413 - accuracy: 0.6244 - val_loss: 1.0324 - val_accuracy: 0.4680\n",
      "Epoch 116/300\n",
      "1821/1821 [==============================] - 0s 192us/sample - loss: 0.8422 - accuracy: 0.6260 - val_loss: 1.0323 - val_accuracy: 0.4680\n",
      "Epoch 117/300\n",
      "1821/1821 [==============================] - 0s 196us/sample - loss: 0.8351 - accuracy: 0.6293 - val_loss: 1.0322 - val_accuracy: 0.4637\n",
      "Epoch 118/300\n",
      "1821/1821 [==============================] - 0s 197us/sample - loss: 0.8540 - accuracy: 0.6178 - val_loss: 1.0322 - val_accuracy: 0.4651\n",
      "Epoch 119/300\n",
      "1821/1821 [==============================] - 0s 199us/sample - loss: 0.8395 - accuracy: 0.6216 - val_loss: 1.0322 - val_accuracy: 0.4709\n",
      "Epoch 120/300\n",
      "1821/1821 [==============================] - 0s 200us/sample - loss: 0.8404 - accuracy: 0.6321 - val_loss: 1.0321 - val_accuracy: 0.4709\n",
      "Epoch 121/300\n",
      "1821/1821 [==============================] - 0s 200us/sample - loss: 0.8348 - accuracy: 0.6299 - val_loss: 1.0320 - val_accuracy: 0.4724\n",
      "Epoch 122/300\n",
      "1821/1821 [==============================] - 0s 209us/sample - loss: 0.8437 - accuracy: 0.6222 - val_loss: 1.0320 - val_accuracy: 0.4724\n",
      "Epoch 123/300\n",
      "1821/1821 [==============================] - 0s 178us/sample - loss: 0.8392 - accuracy: 0.6354 - val_loss: 1.0321 - val_accuracy: 0.4724\n",
      "Epoch 124/300\n",
      "1821/1821 [==============================] - 0s 208us/sample - loss: 0.8292 - accuracy: 0.6458 - val_loss: 1.0319 - val_accuracy: 0.4724\n",
      "Epoch 125/300\n",
      "1821/1821 [==============================] - 0s 226us/sample - loss: 0.8273 - accuracy: 0.6376 - val_loss: 1.0320 - val_accuracy: 0.4724\n",
      "Epoch 126/300\n",
      "1821/1821 [==============================] - 0s 195us/sample - loss: 0.8214 - accuracy: 0.6370 - val_loss: 1.0321 - val_accuracy: 0.4738\n",
      "Epoch 127/300\n",
      "1821/1821 [==============================] - 0s 188us/sample - loss: 0.8280 - accuracy: 0.6381 - val_loss: 1.0321 - val_accuracy: 0.4753\n",
      "Epoch 00127: early stopping\n",
      "247/247 [==============================] - ETA: 0s - loss: 1.5646 - accuracy: 0.12 - 0s 104us/sample - loss: 1.4089 - accuracy: 0.2227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [12:30, 750.57s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:06<00:12,  6.07s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:12<00:06,  6.17s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.16s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 586.390625 steps, validate for 223.1015625 steps\n",
      "Epoch 1/300\n",
      "587/586 [==============================] - 18s 31ms/step - loss: 1.0923 - accuracy: 0.4449 - val_loss: 1.1314 - val_accuracy: 0.4159\n",
      "Epoch 2/300\n",
      "587/586 [==============================] - 17s 30ms/step - loss: 1.0046 - accuracy: 0.4975 - val_loss: 1.1038 - val_accuracy: 0.4288\n",
      "Epoch 3/300\n",
      "587/586 [==============================] - 17s 30ms/step - loss: 0.9790 - accuracy: 0.5171 - val_loss: 1.1032 - val_accuracy: 0.4346\n",
      "Epoch 4/300\n",
      "587/586 [==============================] - 18s 30ms/step - loss: 0.9622 - accuracy: 0.5301 - val_loss: 1.0990 - val_accuracy: 0.4408\n",
      "Epoch 5/300\n",
      "587/586 [==============================] - 17s 29ms/step - loss: 0.9487 - accuracy: 0.5417 - val_loss: 1.0934 - val_accuracy: 0.4363\n",
      "Epoch 6/300\n",
      "587/586 [==============================] - 17s 30ms/step - loss: 0.9373 - accuracy: 0.5506 - val_loss: 1.0948 - val_accuracy: 0.4396\n",
      "Epoch 7/300\n",
      "587/586 [==============================] - 17s 30ms/step - loss: 0.9274 - accuracy: 0.5561 - val_loss: 1.0938 - val_accuracy: 0.4377\n",
      "Epoch 8/300\n",
      "587/586 [==============================] - 17s 30ms/step - loss: 0.9182 - accuracy: 0.5632 - val_loss: 1.0979 - val_accuracy: 0.4399\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 586.390625 steps, validate for 223.1015625 steps\n",
      "Epoch 1/300\n",
      "587/586 [==============================] - 40s 68ms/step - loss: 0.9005 - accuracy: 0.5745 - val_loss: 1.1306 - val_accuracy: 0.4589\n",
      "Epoch 2/300\n",
      "587/586 [==============================] - 38s 65ms/step - loss: 0.8715 - accuracy: 0.5953 - val_loss: 1.0957 - val_accuracy: 0.4609\n",
      "Epoch 3/300\n",
      "587/586 [==============================] - 39s 66ms/step - loss: 0.8484 - accuracy: 0.6107 - val_loss: 1.1172 - val_accuracy: 0.4368\n",
      "Epoch 4/300\n",
      "587/586 [==============================] - 38s 66ms/step - loss: 0.8278 - accuracy: 0.6240 - val_loss: 1.1381 - val_accuracy: 0.4266\n",
      "Epoch 5/300\n",
      "587/586 [==============================] - 39s 66ms/step - loss: 0.8083 - accuracy: 0.6369 - val_loss: 1.1564 - val_accuracy: 0.4192\n",
      "Epoch 00005: early stopping\n",
      "414/414 [==============================] - 0s 734us/sample - loss: 1.1137 - accuracy: 0.3841\n",
      "392/392 [==============================] - 0s 211us/sample - loss: 1.1962 - accuracy: 0.3469\n",
      "382/382 [==============================] - 0s 211us/sample - loss: 1.1911 - accuracy: 0.3298\n",
      "356/356 [==============================] - 0s 235us/sample - loss: 1.1942 - accuracy: 0.3427\n",
      "330/330 [==============================] - 0s 216us/sample - loss: 1.1689 - accuracy: 0.3697\n",
      "317/317 [==============================] - 0s 260us/sample - loss: 1.1543 - accuracy: 0.3785\n",
      "311/311 [==============================] - 0s 246us/sample - loss: 1.1458 - accuracy: 0.3569\n",
      "309/309 [==============================] - 0s 299us/sample - loss: 1.1109 - accuracy: 0.3786\n",
      "294/294 [==============================] - 0s 242us/sample - loss: 1.1606 - accuracy: 0.3810\n",
      "289/289 [==============================] - 0s 245us/sample - loss: 1.1488 - accuracy: 0.3633\n",
      "269/269 [==============================] - 0s 236us/sample - loss: 1.1764 - accuracy: 0.3903\n",
      "262/262 [==============================] - 0s 252us/sample - loss: 1.1628 - accuracy: 0.4084\n",
      "254/254 [==============================] - 0s 265us/sample - loss: 1.0995 - accuracy: 0.4173\n",
      "253/253 [==============================] - 0s 263us/sample - loss: 1.1691 - accuracy: 0.3834\n",
      "252/252 [==============================] - 0s 257us/sample - loss: 1.1889 - accuracy: 0.3373\n",
      "249/249 [==============================] - 0s 303us/sample - loss: 1.1485 - accuracy: 0.3655\n",
      "246/246 [==============================] - 0s 222us/sample - loss: 1.1978 - accuracy: 0.3577\n",
      "244/244 [==============================] - 0s 244us/sample - loss: 1.1790 - accuracy: 0.3730\n",
      "243/243 [==============================] - 0s 239us/sample - loss: 1.2086 - accuracy: 0.3827\n",
      "241/241 [==============================] - 0s 261us/sample - loss: 1.1857 - accuracy: 0.3444\n",
      "239/239 [==============================] - 0s 226us/sample - loss: 1.1303 - accuracy: 0.4100\n",
      "236/236 [==============================] - 0s 247us/sample - loss: 1.1812 - accuracy: 0.3602\n",
      "235/235 [==============================] - 0s 239us/sample - loss: 1.1941 - accuracy: 0.3617\n",
      "229/229 [==============================] - 0s 230us/sample - loss: 1.2002 - accuracy: 0.3319\n",
      "229/229 [==============================] - 0s 227us/sample - loss: 1.1988 - accuracy: 0.3362\n",
      "225/225 [==============================] - 0s 228us/sample - loss: 1.1983 - accuracy: 0.3600\n",
      "223/223 [==============================] - 0s 251us/sample - loss: 1.2171 - accuracy: 0.3722\n",
      "222/222 [==============================] - 0s 252us/sample - loss: 1.1939 - accuracy: 0.3333\n",
      "222/222 [==============================] - 0s 227us/sample - loss: 1.1977 - accuracy: 0.3288\n",
      "223/223 [==============================] - 0s 241us/sample - loss: 1.1792 - accuracy: 0.3587\n",
      "222/222 [==============================] - 0s 269us/sample - loss: 1.1853 - accuracy: 0.3604\n",
      "216/216 [==============================] - 0s 247us/sample - loss: 1.2259 - accuracy: 0.3333\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 1842 samples, validate on 720 samples\n",
      "Epoch 1/300\n",
      "1842/1842 [==============================] - 3s 1ms/sample - loss: 1.1860 - accuracy: 0.3236 - val_loss: 1.1522 - val_accuracy: 0.3444\n",
      "Epoch 2/300\n",
      "1842/1842 [==============================] - 0s 225us/sample - loss: 1.1552 - accuracy: 0.3534 - val_loss: 1.1370 - val_accuracy: 0.3653\n",
      "Epoch 3/300\n",
      "1842/1842 [==============================] - 0s 205us/sample - loss: 1.1340 - accuracy: 0.3643 - val_loss: 1.1274 - val_accuracy: 0.3792\n",
      "Epoch 4/300\n",
      "1842/1842 [==============================] - 0s 203us/sample - loss: 1.1250 - accuracy: 0.3931 - val_loss: 1.1199 - val_accuracy: 0.3833\n",
      "Epoch 5/300\n",
      "1842/1842 [==============================] - 0s 185us/sample - loss: 1.1080 - accuracy: 0.4050 - val_loss: 1.1137 - val_accuracy: 0.3847\n",
      "Epoch 6/300\n",
      "1842/1842 [==============================] - 0s 200us/sample - loss: 1.0994 - accuracy: 0.4121 - val_loss: 1.1090 - val_accuracy: 0.3944\n",
      "Epoch 7/300\n",
      "1842/1842 [==============================] - 0s 223us/sample - loss: 1.0963 - accuracy: 0.4186 - val_loss: 1.1044 - val_accuracy: 0.4028\n",
      "Epoch 8/300\n",
      "1842/1842 [==============================] - 0s 201us/sample - loss: 1.0911 - accuracy: 0.4245 - val_loss: 1.1006 - val_accuracy: 0.4125\n",
      "Epoch 9/300\n",
      "1842/1842 [==============================] - 0s 197us/sample - loss: 1.0760 - accuracy: 0.4457 - val_loss: 1.0974 - val_accuracy: 0.4111\n",
      "Epoch 10/300\n",
      "1842/1842 [==============================] - 0s 214us/sample - loss: 1.0724 - accuracy: 0.4403 - val_loss: 1.0943 - val_accuracy: 0.4181\n",
      "Epoch 11/300\n",
      "1842/1842 [==============================] - 0s 207us/sample - loss: 1.0587 - accuracy: 0.4419 - val_loss: 1.0916 - val_accuracy: 0.4278\n",
      "Epoch 12/300\n",
      "1842/1842 [==============================] - 0s 191us/sample - loss: 1.0637 - accuracy: 0.4446 - val_loss: 1.0891 - val_accuracy: 0.4306\n",
      "Epoch 13/300\n",
      "1842/1842 [==============================] - 0s 187us/sample - loss: 1.0511 - accuracy: 0.4560 - val_loss: 1.0868 - val_accuracy: 0.4278\n",
      "Epoch 14/300\n",
      "1842/1842 [==============================] - 0s 193us/sample - loss: 1.0627 - accuracy: 0.4501 - val_loss: 1.0847 - val_accuracy: 0.4333\n",
      "Epoch 15/300\n",
      "1842/1842 [==============================] - 0s 198us/sample - loss: 1.0510 - accuracy: 0.4598 - val_loss: 1.0826 - val_accuracy: 0.4306\n",
      "Epoch 16/300\n",
      "1842/1842 [==============================] - 0s 202us/sample - loss: 1.0501 - accuracy: 0.4457 - val_loss: 1.0807 - val_accuracy: 0.4347\n",
      "Epoch 17/300\n",
      "1842/1842 [==============================] - 0s 212us/sample - loss: 1.0400 - accuracy: 0.4794 - val_loss: 1.0790 - val_accuracy: 0.4319\n",
      "Epoch 18/300\n",
      "1842/1842 [==============================] - 0s 197us/sample - loss: 1.0430 - accuracy: 0.4701 - val_loss: 1.0775 - val_accuracy: 0.4319\n",
      "Epoch 19/300\n",
      "1842/1842 [==============================] - 0s 189us/sample - loss: 1.0354 - accuracy: 0.4739 - val_loss: 1.0759 - val_accuracy: 0.4361\n",
      "Epoch 20/300\n",
      "1842/1842 [==============================] - 0s 204us/sample - loss: 1.0331 - accuracy: 0.4864 - val_loss: 1.0745 - val_accuracy: 0.4361\n",
      "Epoch 21/300\n",
      "1842/1842 [==============================] - 0s 185us/sample - loss: 1.0379 - accuracy: 0.4881 - val_loss: 1.0731 - val_accuracy: 0.4375\n",
      "Epoch 22/300\n",
      "1842/1842 [==============================] - 0s 221us/sample - loss: 1.0104 - accuracy: 0.4913 - val_loss: 1.0718 - val_accuracy: 0.4431\n",
      "Epoch 23/300\n",
      "1842/1842 [==============================] - 0s 198us/sample - loss: 1.0284 - accuracy: 0.4756 - val_loss: 1.0706 - val_accuracy: 0.4458\n",
      "Epoch 24/300\n",
      "1842/1842 [==============================] - 0s 200us/sample - loss: 1.0178 - accuracy: 0.5022 - val_loss: 1.0694 - val_accuracy: 0.4500\n",
      "Epoch 25/300\n",
      "1842/1842 [==============================] - 0s 196us/sample - loss: 1.0111 - accuracy: 0.4946 - val_loss: 1.0681 - val_accuracy: 0.4528\n",
      "Epoch 26/300\n",
      "1842/1842 [==============================] - 0s 203us/sample - loss: 1.0217 - accuracy: 0.4794 - val_loss: 1.0669 - val_accuracy: 0.4528\n",
      "Epoch 27/300\n",
      "1842/1842 [==============================] - 0s 214us/sample - loss: 1.0041 - accuracy: 0.4940 - val_loss: 1.0658 - val_accuracy: 0.4569\n",
      "Epoch 28/300\n",
      "1842/1842 [==============================] - 0s 205us/sample - loss: 1.0011 - accuracy: 0.5000 - val_loss: 1.0648 - val_accuracy: 0.4583\n",
      "Epoch 29/300\n",
      "1842/1842 [==============================] - 0s 189us/sample - loss: 1.0022 - accuracy: 0.5043 - val_loss: 1.0639 - val_accuracy: 0.4556\n",
      "Epoch 30/300\n",
      "1842/1842 [==============================] - 0s 226us/sample - loss: 1.0035 - accuracy: 0.4886 - val_loss: 1.0629 - val_accuracy: 0.4556\n",
      "Epoch 31/300\n",
      "1842/1842 [==============================] - 0s 204us/sample - loss: 1.0011 - accuracy: 0.5016 - val_loss: 1.0621 - val_accuracy: 0.4556\n",
      "Epoch 32/300\n",
      "1842/1842 [==============================] - 0s 200us/sample - loss: 1.0023 - accuracy: 0.5016 - val_loss: 1.0612 - val_accuracy: 0.4569\n",
      "Epoch 33/300\n",
      "1842/1842 [==============================] - 0s 208us/sample - loss: 0.9971 - accuracy: 0.5071 - val_loss: 1.0604 - val_accuracy: 0.4556\n",
      "Epoch 34/300\n",
      "1842/1842 [==============================] - 0s 204us/sample - loss: 0.9906 - accuracy: 0.5016 - val_loss: 1.0596 - val_accuracy: 0.4542\n",
      "Epoch 35/300\n",
      "1842/1842 [==============================] - 0s 186us/sample - loss: 0.9918 - accuracy: 0.5103 - val_loss: 1.0589 - val_accuracy: 0.4556\n",
      "Epoch 36/300\n",
      "1842/1842 [==============================] - 0s 191us/sample - loss: 0.9878 - accuracy: 0.5157 - val_loss: 1.0582 - val_accuracy: 0.4542\n",
      "Epoch 37/300\n",
      "1842/1842 [==============================] - 0s 198us/sample - loss: 0.9941 - accuracy: 0.5092 - val_loss: 1.0574 - val_accuracy: 0.4569\n",
      "Epoch 38/300\n",
      "1842/1842 [==============================] - 0s 209us/sample - loss: 0.9926 - accuracy: 0.5071 - val_loss: 1.0567 - val_accuracy: 0.4569\n",
      "Epoch 39/300\n",
      "1842/1842 [==============================] - 0s 202us/sample - loss: 0.9777 - accuracy: 0.5271 - val_loss: 1.0561 - val_accuracy: 0.4569\n",
      "Epoch 40/300\n",
      "1842/1842 [==============================] - 0s 235us/sample - loss: 0.9806 - accuracy: 0.5255 - val_loss: 1.0555 - val_accuracy: 0.4597\n",
      "Epoch 41/300\n",
      "1842/1842 [==============================] - 0s 206us/sample - loss: 0.9738 - accuracy: 0.5337 - val_loss: 1.0549 - val_accuracy: 0.4611\n",
      "Epoch 42/300\n",
      "1842/1842 [==============================] - 0s 214us/sample - loss: 0.9774 - accuracy: 0.5190 - val_loss: 1.0543 - val_accuracy: 0.4611\n",
      "Epoch 43/300\n",
      "1842/1842 [==============================] - 0s 220us/sample - loss: 0.9686 - accuracy: 0.5195 - val_loss: 1.0538 - val_accuracy: 0.4625\n",
      "Epoch 44/300\n",
      "1842/1842 [==============================] - 0s 206us/sample - loss: 0.9669 - accuracy: 0.5364 - val_loss: 1.0532 - val_accuracy: 0.4611\n",
      "Epoch 45/300\n",
      "1842/1842 [==============================] - 0s 209us/sample - loss: 0.9722 - accuracy: 0.5266 - val_loss: 1.0528 - val_accuracy: 0.4611\n",
      "Epoch 46/300\n",
      "1842/1842 [==============================] - 0s 214us/sample - loss: 0.9630 - accuracy: 0.5396 - val_loss: 1.0523 - val_accuracy: 0.4611\n",
      "Epoch 47/300\n",
      "1842/1842 [==============================] - 0s 218us/sample - loss: 0.9634 - accuracy: 0.5423 - val_loss: 1.0518 - val_accuracy: 0.4611\n",
      "Epoch 48/300\n",
      "1842/1842 [==============================] - 0s 209us/sample - loss: 0.9552 - accuracy: 0.5418 - val_loss: 1.0512 - val_accuracy: 0.4625\n",
      "Epoch 49/300\n",
      "1842/1842 [==============================] - 0s 216us/sample - loss: 0.9628 - accuracy: 0.5228 - val_loss: 1.0508 - val_accuracy: 0.4625\n",
      "Epoch 50/300\n",
      "1842/1842 [==============================] - 0s 202us/sample - loss: 0.9521 - accuracy: 0.5472 - val_loss: 1.0503 - val_accuracy: 0.4597\n",
      "Epoch 51/300\n",
      "1842/1842 [==============================] - 0s 203us/sample - loss: 0.9558 - accuracy: 0.5429 - val_loss: 1.0499 - val_accuracy: 0.4597\n",
      "Epoch 52/300\n",
      "1842/1842 [==============================] - 0s 210us/sample - loss: 0.9565 - accuracy: 0.5407 - val_loss: 1.0496 - val_accuracy: 0.4597\n",
      "Epoch 53/300\n",
      "1842/1842 [==============================] - 0s 214us/sample - loss: 0.9520 - accuracy: 0.5369 - val_loss: 1.0491 - val_accuracy: 0.4597\n",
      "Epoch 54/300\n",
      "1842/1842 [==============================] - 0s 220us/sample - loss: 0.9563 - accuracy: 0.5396 - val_loss: 1.0488 - val_accuracy: 0.4611\n",
      "Epoch 55/300\n",
      "1842/1842 [==============================] - 0s 205us/sample - loss: 0.9493 - accuracy: 0.5407 - val_loss: 1.0485 - val_accuracy: 0.4611\n",
      "Epoch 56/300\n",
      "1842/1842 [==============================] - 0s 208us/sample - loss: 0.9374 - accuracy: 0.5527 - val_loss: 1.0482 - val_accuracy: 0.4611\n",
      "Epoch 57/300\n",
      "1842/1842 [==============================] - 0s 210us/sample - loss: 0.9504 - accuracy: 0.5554 - val_loss: 1.0480 - val_accuracy: 0.4597\n",
      "Epoch 58/300\n",
      "1842/1842 [==============================] - 0s 206us/sample - loss: 0.9388 - accuracy: 0.5456 - val_loss: 1.0475 - val_accuracy: 0.4597\n",
      "Epoch 59/300\n",
      "1842/1842 [==============================] - 0s 200us/sample - loss: 0.9463 - accuracy: 0.5353 - val_loss: 1.0473 - val_accuracy: 0.4597\n",
      "Epoch 60/300\n",
      "1842/1842 [==============================] - 0s 203us/sample - loss: 0.9401 - accuracy: 0.5499 - val_loss: 1.0470 - val_accuracy: 0.4597\n",
      "Epoch 61/300\n",
      "1842/1842 [==============================] - 0s 204us/sample - loss: 0.9417 - accuracy: 0.5532 - val_loss: 1.0468 - val_accuracy: 0.4583\n",
      "Epoch 62/300\n",
      "1842/1842 [==============================] - 0s 200us/sample - loss: 0.9377 - accuracy: 0.5565 - val_loss: 1.0464 - val_accuracy: 0.4569\n",
      "Epoch 63/300\n",
      "1842/1842 [==============================] - 0s 210us/sample - loss: 0.9345 - accuracy: 0.5603 - val_loss: 1.0462 - val_accuracy: 0.4569\n",
      "Epoch 64/300\n",
      "1842/1842 [==============================] - 0s 210us/sample - loss: 0.9372 - accuracy: 0.5586 - val_loss: 1.0459 - val_accuracy: 0.4611\n",
      "Epoch 65/300\n",
      "1842/1842 [==============================] - 0s 201us/sample - loss: 0.9306 - accuracy: 0.5684 - val_loss: 1.0458 - val_accuracy: 0.4597\n",
      "Epoch 66/300\n",
      "1842/1842 [==============================] - 0s 195us/sample - loss: 0.9299 - accuracy: 0.5483 - val_loss: 1.0455 - val_accuracy: 0.4569\n",
      "Epoch 67/300\n",
      "1842/1842 [==============================] - 0s 202us/sample - loss: 0.9230 - accuracy: 0.5700 - val_loss: 1.0452 - val_accuracy: 0.4597\n",
      "Epoch 68/300\n",
      "1842/1842 [==============================] - 0s 203us/sample - loss: 0.9183 - accuracy: 0.5727 - val_loss: 1.0451 - val_accuracy: 0.4597\n",
      "Epoch 69/300\n",
      "1842/1842 [==============================] - 0s 204us/sample - loss: 0.9243 - accuracy: 0.5559 - val_loss: 1.0450 - val_accuracy: 0.4597\n",
      "Epoch 70/300\n",
      "1842/1842 [==============================] - 0s 199us/sample - loss: 0.9135 - accuracy: 0.5771 - val_loss: 1.0450 - val_accuracy: 0.4639\n",
      "Epoch 71/300\n",
      "1842/1842 [==============================] - 0s 211us/sample - loss: 0.9182 - accuracy: 0.5695 - val_loss: 1.0447 - val_accuracy: 0.4639\n",
      "Epoch 72/300\n",
      "1842/1842 [==============================] - 0s 201us/sample - loss: 0.9219 - accuracy: 0.5624 - val_loss: 1.0447 - val_accuracy: 0.4667\n",
      "Epoch 73/300\n",
      "1842/1842 [==============================] - 0s 199us/sample - loss: 0.9114 - accuracy: 0.5760 - val_loss: 1.0445 - val_accuracy: 0.4653\n",
      "Epoch 74/300\n",
      "1842/1842 [==============================] - 0s 204us/sample - loss: 0.9134 - accuracy: 0.5771 - val_loss: 1.0444 - val_accuracy: 0.4694\n",
      "Epoch 75/300\n",
      "1842/1842 [==============================] - 0s 214us/sample - loss: 0.9083 - accuracy: 0.5858 - val_loss: 1.0444 - val_accuracy: 0.4694\n",
      "Epoch 76/300\n",
      "1842/1842 [==============================] - 0s 218us/sample - loss: 0.9131 - accuracy: 0.5820 - val_loss: 1.0443 - val_accuracy: 0.4694\n",
      "Epoch 77/300\n",
      "1842/1842 [==============================] - 0s 211us/sample - loss: 0.9030 - accuracy: 0.5825 - val_loss: 1.0441 - val_accuracy: 0.4722\n",
      "Epoch 78/300\n",
      "1842/1842 [==============================] - 0s 210us/sample - loss: 0.9143 - accuracy: 0.5613 - val_loss: 1.0440 - val_accuracy: 0.4736\n",
      "Epoch 79/300\n",
      "1842/1842 [==============================] - 0s 219us/sample - loss: 0.9087 - accuracy: 0.5825 - val_loss: 1.0439 - val_accuracy: 0.4750\n",
      "Epoch 80/300\n",
      "1842/1842 [==============================] - 0s 202us/sample - loss: 0.9017 - accuracy: 0.5863 - val_loss: 1.0439 - val_accuracy: 0.4764\n",
      "Epoch 81/300\n",
      "1842/1842 [==============================] - 0s 216us/sample - loss: 0.9007 - accuracy: 0.5733 - val_loss: 1.0438 - val_accuracy: 0.4764\n",
      "Epoch 82/300\n",
      "1842/1842 [==============================] - 0s 200us/sample - loss: 0.8971 - accuracy: 0.5863 - val_loss: 1.0438 - val_accuracy: 0.4750\n",
      "Epoch 83/300\n",
      "1842/1842 [==============================] - 0s 207us/sample - loss: 0.8969 - accuracy: 0.5950 - val_loss: 1.0437 - val_accuracy: 0.4736\n",
      "Epoch 84/300\n",
      "1842/1842 [==============================] - 0s 201us/sample - loss: 0.9029 - accuracy: 0.5809 - val_loss: 1.0436 - val_accuracy: 0.4708\n",
      "Epoch 85/300\n",
      "1842/1842 [==============================] - 0s 213us/sample - loss: 0.8965 - accuracy: 0.5869 - val_loss: 1.0437 - val_accuracy: 0.4722\n",
      "Epoch 86/300\n",
      "1842/1842 [==============================] - 0s 192us/sample - loss: 0.8999 - accuracy: 0.5744 - val_loss: 1.0438 - val_accuracy: 0.4694\n",
      "Epoch 87/300\n",
      "1842/1842 [==============================] - 0s 215us/sample - loss: 0.8877 - accuracy: 0.5912 - val_loss: 1.0437 - val_accuracy: 0.4694\n",
      "Epoch 00087: early stopping\n",
      "194/194 [==============================] - 0s 108us/sample - loss: 1.1885 - accuracy: 0.2320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [24:48, 746.74s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:05<00:11,  5.74s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:11<00:05,  5.73s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:14<00:00,  4.90s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 573.8046875 steps, validate for 218.3828125 steps\n",
      "Epoch 1/300\n",
      "574/573 [==============================] - 18s 32ms/step - loss: 1.1200 - accuracy: 0.4224 - val_loss: 1.1001 - val_accuracy: 0.4338\n",
      "Epoch 2/300\n",
      "574/573 [==============================] - 17s 30ms/step - loss: 1.0156 - accuracy: 0.4818 - val_loss: 1.0751 - val_accuracy: 0.4502\n",
      "Epoch 3/300\n",
      "574/573 [==============================] - 17s 30ms/step - loss: 0.9826 - accuracy: 0.5067 - val_loss: 1.0708 - val_accuracy: 0.4547\n",
      "Epoch 4/300\n",
      "574/573 [==============================] - 17s 30ms/step - loss: 0.9613 - accuracy: 0.5251 - val_loss: 1.0732 - val_accuracy: 0.4506\n",
      "Epoch 5/300\n",
      "574/573 [==============================] - 17s 30ms/step - loss: 0.9450 - accuracy: 0.5382 - val_loss: 1.0731 - val_accuracy: 0.4529\n",
      "Epoch 6/300\n",
      "574/573 [==============================] - 17s 30ms/step - loss: 0.9311 - accuracy: 0.5502 - val_loss: 1.0739 - val_accuracy: 0.4537\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 573.8046875 steps, validate for 218.3828125 steps\n",
      "Epoch 1/300\n",
      "574/573 [==============================] - 39s 68ms/step - loss: 0.9075 - accuracy: 0.5674 - val_loss: 1.0899 - val_accuracy: 0.4616\n",
      "Epoch 2/300\n",
      "574/573 [==============================] - 38s 66ms/step - loss: 0.8724 - accuracy: 0.5927 - val_loss: 1.1192 - val_accuracy: 0.4408\n",
      "Epoch 3/300\n",
      "574/573 [==============================] - 38s 66ms/step - loss: 0.8460 - accuracy: 0.6126 - val_loss: 1.0873 - val_accuracy: 0.4584\n",
      "Epoch 4/300\n",
      "574/573 [==============================] - 38s 66ms/step - loss: 0.8225 - accuracy: 0.6295 - val_loss: 1.1256 - val_accuracy: 0.4357\n",
      "Epoch 5/300\n",
      "574/573 [==============================] - 38s 66ms/step - loss: 0.8015 - accuracy: 0.6414 - val_loss: 1.1248 - val_accuracy: 0.4563\n",
      "Epoch 6/300\n",
      "574/573 [==============================] - 38s 67ms/step - loss: 0.7815 - accuracy: 0.6552 - val_loss: 1.0928 - val_accuracy: 0.4696\n",
      "Epoch 00006: early stopping\n",
      "410/410 [==============================] - 0s 763us/sample - loss: 1.1868 - accuracy: 0.2902\n",
      "390/390 [==============================] - 0s 243us/sample - loss: 1.2760 - accuracy: 0.2436\n",
      "387/387 [==============================] - 0s 230us/sample - loss: 1.2781 - accuracy: 0.2481\n",
      "380/380 [==============================] - 0s 228us/sample - loss: 1.2426 - accuracy: 0.2921\n",
      "378/378 [==============================] - 0s 259us/sample - loss: 1.2764 - accuracy: 0.2989\n",
      "371/371 [==============================] - 0s 248us/sample - loss: 1.2478 - accuracy: 0.3073\n",
      "369/369 [==============================] - 0s 237us/sample - loss: 1.3452 - accuracy: 0.2873\n",
      "363/363 [==============================] - 0s 237us/sample - loss: 1.3745 - accuracy: 0.2176\n",
      "354/354 [==============================] - 0s 268us/sample - loss: 1.4156 - accuracy: 0.1949\n",
      "353/353 [==============================] - 0s 269us/sample - loss: 1.3975 - accuracy: 0.2380\n",
      "345/345 [==============================] - 0s 261us/sample - loss: 1.4674 - accuracy: 0.2290\n",
      "336/336 [==============================] - 0s 258us/sample - loss: 1.4297 - accuracy: 0.2411\n",
      "336/336 [==============================] - 0s 229us/sample - loss: 1.4679 - accuracy: 0.2292\n",
      "335/335 [==============================] - 0s 242us/sample - loss: 1.4929 - accuracy: 0.1791\n",
      "337/337 [==============================] - 0s 263us/sample - loss: 1.4907 - accuracy: 0.2374\n",
      "338/338 [==============================] - 0s 247us/sample - loss: 1.4603 - accuracy: 0.2189\n",
      "333/333 [==============================] - 0s 250us/sample - loss: 1.5408 - accuracy: 0.2132\n",
      "330/330 [==============================] - 0s 252us/sample - loss: 1.5588 - accuracy: 0.1970\n",
      "331/331 [==============================] - 0s 276us/sample - loss: 1.4829 - accuracy: 0.2356\n",
      "328/328 [==============================] - 0s 245us/sample - loss: 1.5008 - accuracy: 0.2500\n",
      "324/324 [==============================] - 0s 239us/sample - loss: 1.5147 - accuracy: 0.2130\n",
      "322/322 [==============================] - 0s 262us/sample - loss: 1.5175 - accuracy: 0.2236\n",
      "324/324 [==============================] - 0s 282us/sample - loss: 1.5722 - accuracy: 0.1975\n",
      "323/323 [==============================] - 0s 229us/sample - loss: 1.5115 - accuracy: 0.1950\n",
      "318/318 [==============================] - 0s 260us/sample - loss: 1.5403 - accuracy: 0.2201\n",
      "314/314 [==============================] - 0s 262us/sample - loss: 1.5100 - accuracy: 0.2070\n",
      "313/313 [==============================] - 0s 252us/sample - loss: 1.5238 - accuracy: 0.1661\n",
      "304/304 [==============================] - 0s 254us/sample - loss: 1.5885 - accuracy: 0.1678\n",
      "301/301 [==============================] - 0s 255us/sample - loss: 1.6048 - accuracy: 0.1728\n",
      "299/299 [==============================] - 0s 242us/sample - loss: 1.5228 - accuracy: 0.1973\n",
      "301/301 [==============================] - 0s 245us/sample - loss: 1.5434 - accuracy: 0.2027\n",
      "296/296 [==============================] - 0s 234us/sample - loss: 1.5594 - accuracy: 0.2162\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 1785 samples, validate on 706 samples\n",
      "Epoch 1/300\n",
      "1785/1785 [==============================] - 2s 1ms/sample - loss: 1.1610 - accuracy: 0.3507 - val_loss: 1.1330 - val_accuracy: 0.3669\n",
      "Epoch 2/300\n",
      "1785/1785 [==============================] - 0s 228us/sample - loss: 1.1354 - accuracy: 0.3697 - val_loss: 1.1219 - val_accuracy: 0.3739\n",
      "Epoch 3/300\n",
      "1785/1785 [==============================] - 0s 202us/sample - loss: 1.1320 - accuracy: 0.3703 - val_loss: 1.1135 - val_accuracy: 0.3824\n",
      "Epoch 4/300\n",
      "1785/1785 [==============================] - 0s 213us/sample - loss: 1.1051 - accuracy: 0.4006 - val_loss: 1.1072 - val_accuracy: 0.3853\n",
      "Epoch 5/300\n",
      "1785/1785 [==============================] - 0s 213us/sample - loss: 1.0907 - accuracy: 0.4095 - val_loss: 1.1018 - val_accuracy: 0.3853\n",
      "Epoch 6/300\n",
      "1785/1785 [==============================] - 0s 201us/sample - loss: 1.0865 - accuracy: 0.4174 - val_loss: 1.0971 - val_accuracy: 0.3938\n",
      "Epoch 7/300\n",
      "1785/1785 [==============================] - 0s 205us/sample - loss: 1.0909 - accuracy: 0.4022 - val_loss: 1.0931 - val_accuracy: 0.4037\n",
      "Epoch 8/300\n",
      "1785/1785 [==============================] - 0s 195us/sample - loss: 1.0623 - accuracy: 0.4263 - val_loss: 1.0895 - val_accuracy: 0.4093\n",
      "Epoch 9/300\n",
      "1785/1785 [==============================] - 0s 197us/sample - loss: 1.0648 - accuracy: 0.4213 - val_loss: 1.0861 - val_accuracy: 0.4136\n",
      "Epoch 10/300\n",
      "1785/1785 [==============================] - 0s 192us/sample - loss: 1.0612 - accuracy: 0.4359 - val_loss: 1.0832 - val_accuracy: 0.4193\n",
      "Epoch 11/300\n",
      "1785/1785 [==============================] - 0s 207us/sample - loss: 1.0536 - accuracy: 0.4370 - val_loss: 1.0803 - val_accuracy: 0.4207\n",
      "Epoch 12/300\n",
      "1785/1785 [==============================] - 0s 200us/sample - loss: 1.0543 - accuracy: 0.4403 - val_loss: 1.0776 - val_accuracy: 0.4320\n",
      "Epoch 13/300\n",
      "1785/1785 [==============================] - 0s 212us/sample - loss: 1.0513 - accuracy: 0.4398 - val_loss: 1.0752 - val_accuracy: 0.4320\n",
      "Epoch 14/300\n",
      "1785/1785 [==============================] - 0s 209us/sample - loss: 1.0378 - accuracy: 0.4723 - val_loss: 1.0729 - val_accuracy: 0.4306\n",
      "Epoch 15/300\n",
      "1785/1785 [==============================] - 0s 204us/sample - loss: 1.0379 - accuracy: 0.4594 - val_loss: 1.0709 - val_accuracy: 0.4306\n",
      "Epoch 16/300\n",
      "1785/1785 [==============================] - 0s 193us/sample - loss: 1.0418 - accuracy: 0.4566 - val_loss: 1.0688 - val_accuracy: 0.4292\n",
      "Epoch 17/300\n",
      "1785/1785 [==============================] - 0s 204us/sample - loss: 1.0345 - accuracy: 0.4605 - val_loss: 1.0669 - val_accuracy: 0.4348\n",
      "Epoch 18/300\n",
      "1785/1785 [==============================] - 0s 200us/sample - loss: 1.0281 - accuracy: 0.4801 - val_loss: 1.0650 - val_accuracy: 0.4363\n",
      "Epoch 19/300\n",
      "1785/1785 [==============================] - 0s 200us/sample - loss: 1.0327 - accuracy: 0.4661 - val_loss: 1.0633 - val_accuracy: 0.4419\n",
      "Epoch 20/300\n",
      "1785/1785 [==============================] - 0s 206us/sample - loss: 1.0209 - accuracy: 0.4812 - val_loss: 1.0616 - val_accuracy: 0.4391\n",
      "Epoch 21/300\n",
      "1785/1785 [==============================] - 0s 216us/sample - loss: 1.0197 - accuracy: 0.4734 - val_loss: 1.0601 - val_accuracy: 0.4391\n",
      "Epoch 22/300\n",
      "1785/1785 [==============================] - 0s 218us/sample - loss: 1.0154 - accuracy: 0.4818 - val_loss: 1.0585 - val_accuracy: 0.4405\n",
      "Epoch 23/300\n",
      "1785/1785 [==============================] - 0s 203us/sample - loss: 1.0132 - accuracy: 0.4880 - val_loss: 1.0571 - val_accuracy: 0.4433\n",
      "Epoch 24/300\n",
      "1785/1785 [==============================] - 0s 218us/sample - loss: 1.0124 - accuracy: 0.4975 - val_loss: 1.0558 - val_accuracy: 0.4448\n",
      "Epoch 25/300\n",
      "1785/1785 [==============================] - 0s 205us/sample - loss: 1.0079 - accuracy: 0.4952 - val_loss: 1.0544 - val_accuracy: 0.4433\n",
      "Epoch 26/300\n",
      "1785/1785 [==============================] - 0s 195us/sample - loss: 0.9963 - accuracy: 0.5165 - val_loss: 1.0530 - val_accuracy: 0.4462\n",
      "Epoch 27/300\n",
      "1785/1785 [==============================] - 0s 214us/sample - loss: 0.9952 - accuracy: 0.4913 - val_loss: 1.0518 - val_accuracy: 0.4504\n",
      "Epoch 28/300\n",
      "1785/1785 [==============================] - 0s 200us/sample - loss: 0.9920 - accuracy: 0.5137 - val_loss: 1.0507 - val_accuracy: 0.4547\n",
      "Epoch 29/300\n",
      "1785/1785 [==============================] - 0s 199us/sample - loss: 0.9946 - accuracy: 0.5188 - val_loss: 1.0495 - val_accuracy: 0.4575\n",
      "Epoch 30/300\n",
      "1785/1785 [==============================] - 0s 194us/sample - loss: 0.9914 - accuracy: 0.5216 - val_loss: 1.0484 - val_accuracy: 0.4589\n",
      "Epoch 31/300\n",
      "1785/1785 [==============================] - 0s 188us/sample - loss: 0.9820 - accuracy: 0.5132 - val_loss: 1.0473 - val_accuracy: 0.4547\n",
      "Epoch 32/300\n",
      "1785/1785 [==============================] - 0s 196us/sample - loss: 0.9778 - accuracy: 0.5261 - val_loss: 1.0463 - val_accuracy: 0.4589\n",
      "Epoch 33/300\n",
      "1785/1785 [==============================] - 0s 198us/sample - loss: 0.9817 - accuracy: 0.5266 - val_loss: 1.0452 - val_accuracy: 0.4618\n",
      "Epoch 34/300\n",
      "1785/1785 [==============================] - 0s 192us/sample - loss: 0.9715 - accuracy: 0.5289 - val_loss: 1.0443 - val_accuracy: 0.4632\n",
      "Epoch 35/300\n",
      "1785/1785 [==============================] - 0s 201us/sample - loss: 0.9658 - accuracy: 0.5333 - val_loss: 1.0434 - val_accuracy: 0.4646\n",
      "Epoch 36/300\n",
      "1785/1785 [==============================] - 0s 214us/sample - loss: 0.9636 - accuracy: 0.5339 - val_loss: 1.0425 - val_accuracy: 0.4660\n",
      "Epoch 37/300\n",
      "1785/1785 [==============================] - 0s 192us/sample - loss: 0.9681 - accuracy: 0.5266 - val_loss: 1.0415 - val_accuracy: 0.4703\n",
      "Epoch 38/300\n",
      "1785/1785 [==============================] - 0s 198us/sample - loss: 0.9674 - accuracy: 0.5261 - val_loss: 1.0407 - val_accuracy: 0.4717\n",
      "Epoch 39/300\n",
      "1785/1785 [==============================] - 0s 189us/sample - loss: 0.9678 - accuracy: 0.5423 - val_loss: 1.0399 - val_accuracy: 0.4759\n",
      "Epoch 40/300\n",
      "1785/1785 [==============================] - 0s 199us/sample - loss: 0.9603 - accuracy: 0.5317 - val_loss: 1.0391 - val_accuracy: 0.4731\n",
      "Epoch 41/300\n",
      "1785/1785 [==============================] - 0s 203us/sample - loss: 0.9586 - accuracy: 0.5597 - val_loss: 1.0383 - val_accuracy: 0.4703\n",
      "Epoch 42/300\n",
      "1785/1785 [==============================] - 0s 201us/sample - loss: 0.9571 - accuracy: 0.5529 - val_loss: 1.0375 - val_accuracy: 0.4745\n",
      "Epoch 43/300\n",
      "1785/1785 [==============================] - 0s 214us/sample - loss: 0.9519 - accuracy: 0.5597 - val_loss: 1.0367 - val_accuracy: 0.4703\n",
      "Epoch 44/300\n",
      "1785/1785 [==============================] - 0s 204us/sample - loss: 0.9497 - accuracy: 0.5541 - val_loss: 1.0359 - val_accuracy: 0.4674\n",
      "Epoch 45/300\n",
      "1785/1785 [==============================] - 0s 207us/sample - loss: 0.9440 - accuracy: 0.5664 - val_loss: 1.0352 - val_accuracy: 0.4703\n",
      "Epoch 46/300\n",
      "1785/1785 [==============================] - 0s 206us/sample - loss: 0.9475 - accuracy: 0.5412 - val_loss: 1.0344 - val_accuracy: 0.4717\n",
      "Epoch 47/300\n",
      "1785/1785 [==============================] - 0s 192us/sample - loss: 0.9404 - accuracy: 0.5541 - val_loss: 1.0337 - val_accuracy: 0.4717\n",
      "Epoch 48/300\n",
      "1785/1785 [==============================] - 0s 201us/sample - loss: 0.9457 - accuracy: 0.5658 - val_loss: 1.0330 - val_accuracy: 0.4688\n",
      "Epoch 49/300\n",
      "1785/1785 [==============================] - 0s 218us/sample - loss: 0.9381 - accuracy: 0.5602 - val_loss: 1.0323 - val_accuracy: 0.4674\n",
      "Epoch 50/300\n",
      "1785/1785 [==============================] - 0s 200us/sample - loss: 0.9275 - accuracy: 0.5714 - val_loss: 1.0317 - val_accuracy: 0.4674\n",
      "Epoch 51/300\n",
      "1785/1785 [==============================] - 0s 201us/sample - loss: 0.9234 - accuracy: 0.5776 - val_loss: 1.0311 - val_accuracy: 0.4660\n",
      "Epoch 52/300\n",
      "1785/1785 [==============================] - 0s 209us/sample - loss: 0.9287 - accuracy: 0.5669 - val_loss: 1.0305 - val_accuracy: 0.4646\n",
      "Epoch 53/300\n",
      "1785/1785 [==============================] - 0s 212us/sample - loss: 0.9295 - accuracy: 0.5765 - val_loss: 1.0299 - val_accuracy: 0.4632\n",
      "Epoch 54/300\n",
      "1785/1785 [==============================] - 0s 193us/sample - loss: 0.9239 - accuracy: 0.5854 - val_loss: 1.0294 - val_accuracy: 0.4632\n",
      "Epoch 55/300\n",
      "1785/1785 [==============================] - 0s 193us/sample - loss: 0.9280 - accuracy: 0.5681 - val_loss: 1.0287 - val_accuracy: 0.4618\n",
      "Epoch 56/300\n",
      "1785/1785 [==============================] - 0s 193us/sample - loss: 0.9338 - accuracy: 0.5681 - val_loss: 1.0282 - val_accuracy: 0.4603\n",
      "Epoch 57/300\n",
      "1785/1785 [==============================] - 0s 204us/sample - loss: 0.9171 - accuracy: 0.5894 - val_loss: 1.0276 - val_accuracy: 0.4618\n",
      "Epoch 58/300\n",
      "1785/1785 [==============================] - 0s 184us/sample - loss: 0.9221 - accuracy: 0.5697 - val_loss: 1.0270 - val_accuracy: 0.4618\n",
      "Epoch 59/300\n",
      "1785/1785 [==============================] - 0s 194us/sample - loss: 0.9256 - accuracy: 0.5810 - val_loss: 1.0264 - val_accuracy: 0.4660\n",
      "Epoch 60/300\n",
      "1785/1785 [==============================] - 0s 202us/sample - loss: 0.9161 - accuracy: 0.5877 - val_loss: 1.0259 - val_accuracy: 0.4674\n",
      "Epoch 61/300\n",
      "1785/1785 [==============================] - 0s 191us/sample - loss: 0.9135 - accuracy: 0.5894 - val_loss: 1.0255 - val_accuracy: 0.4660\n",
      "Epoch 62/300\n",
      "1785/1785 [==============================] - 0s 196us/sample - loss: 0.9083 - accuracy: 0.5866 - val_loss: 1.0250 - val_accuracy: 0.4646\n",
      "Epoch 63/300\n",
      "1785/1785 [==============================] - 0s 220us/sample - loss: 0.9066 - accuracy: 0.5882 - val_loss: 1.0245 - val_accuracy: 0.4660\n",
      "Epoch 64/300\n",
      "1785/1785 [==============================] - 0s 197us/sample - loss: 0.9027 - accuracy: 0.5933 - val_loss: 1.0240 - val_accuracy: 0.4646\n",
      "Epoch 65/300\n",
      "1785/1785 [==============================] - 0s 196us/sample - loss: 0.8976 - accuracy: 0.5922 - val_loss: 1.0235 - val_accuracy: 0.4660\n",
      "Epoch 66/300\n",
      "1785/1785 [==============================] - 0s 190us/sample - loss: 0.9040 - accuracy: 0.6067 - val_loss: 1.0231 - val_accuracy: 0.4632\n",
      "Epoch 67/300\n",
      "1785/1785 [==============================] - 0s 200us/sample - loss: 0.9005 - accuracy: 0.6000 - val_loss: 1.0226 - val_accuracy: 0.4660\n",
      "Epoch 68/300\n",
      "1785/1785 [==============================] - 0s 201us/sample - loss: 0.9044 - accuracy: 0.5933 - val_loss: 1.0221 - val_accuracy: 0.4660\n",
      "Epoch 69/300\n",
      "1785/1785 [==============================] - 0s 195us/sample - loss: 0.8941 - accuracy: 0.6011 - val_loss: 1.0217 - val_accuracy: 0.4688\n",
      "Epoch 70/300\n",
      "1785/1785 [==============================] - 0s 196us/sample - loss: 0.8898 - accuracy: 0.6028 - val_loss: 1.0214 - val_accuracy: 0.4703\n",
      "Epoch 71/300\n",
      "1785/1785 [==============================] - 0s 194us/sample - loss: 0.8872 - accuracy: 0.6118 - val_loss: 1.0209 - val_accuracy: 0.4717\n",
      "Epoch 72/300\n",
      "1785/1785 [==============================] - 0s 215us/sample - loss: 0.8862 - accuracy: 0.6106 - val_loss: 1.0204 - val_accuracy: 0.4731\n",
      "Epoch 73/300\n",
      "1785/1785 [==============================] - 0s 200us/sample - loss: 0.8873 - accuracy: 0.6123 - val_loss: 1.0201 - val_accuracy: 0.4717\n",
      "Epoch 74/300\n",
      "1785/1785 [==============================] - 0s 200us/sample - loss: 0.8777 - accuracy: 0.6140 - val_loss: 1.0197 - val_accuracy: 0.4717\n",
      "Epoch 75/300\n",
      "1785/1785 [==============================] - 0s 206us/sample - loss: 0.8811 - accuracy: 0.6179 - val_loss: 1.0194 - val_accuracy: 0.4717\n",
      "Epoch 76/300\n",
      "1785/1785 [==============================] - 0s 216us/sample - loss: 0.8792 - accuracy: 0.6230 - val_loss: 1.0190 - val_accuracy: 0.4745\n",
      "Epoch 77/300\n",
      "1785/1785 [==============================] - 0s 223us/sample - loss: 0.8763 - accuracy: 0.6168 - val_loss: 1.0187 - val_accuracy: 0.4745\n",
      "Epoch 78/300\n",
      "1785/1785 [==============================] - 0s 197us/sample - loss: 0.8711 - accuracy: 0.6303 - val_loss: 1.0184 - val_accuracy: 0.4745\n",
      "Epoch 79/300\n",
      "1785/1785 [==============================] - 0s 204us/sample - loss: 0.8657 - accuracy: 0.6207 - val_loss: 1.0180 - val_accuracy: 0.4731\n",
      "Epoch 80/300\n",
      "1785/1785 [==============================] - 0s 185us/sample - loss: 0.8755 - accuracy: 0.6140 - val_loss: 1.0177 - val_accuracy: 0.4717\n",
      "Epoch 81/300\n",
      "1785/1785 [==============================] - 0s 202us/sample - loss: 0.8696 - accuracy: 0.6235 - val_loss: 1.0174 - val_accuracy: 0.4703\n",
      "Epoch 82/300\n",
      "1785/1785 [==============================] - 0s 197us/sample - loss: 0.8712 - accuracy: 0.6218 - val_loss: 1.0171 - val_accuracy: 0.4731\n",
      "Epoch 83/300\n",
      "1785/1785 [==============================] - 0s 212us/sample - loss: 0.8570 - accuracy: 0.6398 - val_loss: 1.0169 - val_accuracy: 0.4745\n",
      "Epoch 84/300\n",
      "1785/1785 [==============================] - 0s 198us/sample - loss: 0.8612 - accuracy: 0.6353 - val_loss: 1.0167 - val_accuracy: 0.4745\n",
      "Epoch 85/300\n",
      "1785/1785 [==============================] - 0s 193us/sample - loss: 0.8582 - accuracy: 0.6286 - val_loss: 1.0164 - val_accuracy: 0.4759\n",
      "Epoch 86/300\n",
      "1785/1785 [==============================] - 0s 196us/sample - loss: 0.8554 - accuracy: 0.6364 - val_loss: 1.0162 - val_accuracy: 0.4759\n",
      "Epoch 87/300\n",
      "1785/1785 [==============================] - 0s 203us/sample - loss: 0.8581 - accuracy: 0.6331 - val_loss: 1.0160 - val_accuracy: 0.4759\n",
      "Epoch 88/300\n",
      "1785/1785 [==============================] - 0s 199us/sample - loss: 0.8504 - accuracy: 0.6347 - val_loss: 1.0157 - val_accuracy: 0.4802\n",
      "Epoch 89/300\n",
      "1785/1785 [==============================] - 0s 188us/sample - loss: 0.8515 - accuracy: 0.6286 - val_loss: 1.0155 - val_accuracy: 0.4816\n",
      "Epoch 90/300\n",
      "1785/1785 [==============================] - 0s 198us/sample - loss: 0.8507 - accuracy: 0.6392 - val_loss: 1.0154 - val_accuracy: 0.4830\n",
      "Epoch 91/300\n",
      "1785/1785 [==============================] - 0s 209us/sample - loss: 0.8366 - accuracy: 0.6471 - val_loss: 1.0151 - val_accuracy: 0.4830\n",
      "Epoch 92/300\n",
      "1785/1785 [==============================] - 0s 190us/sample - loss: 0.8543 - accuracy: 0.6325 - val_loss: 1.0150 - val_accuracy: 0.4830\n",
      "Epoch 93/300\n",
      "1785/1785 [==============================] - 0s 194us/sample - loss: 0.8380 - accuracy: 0.6409 - val_loss: 1.0148 - val_accuracy: 0.4816\n",
      "Epoch 94/300\n",
      "1785/1785 [==============================] - 0s 202us/sample - loss: 0.8427 - accuracy: 0.6431 - val_loss: 1.0147 - val_accuracy: 0.4844\n",
      "Epoch 95/300\n",
      "1785/1785 [==============================] - 0s 205us/sample - loss: 0.8388 - accuracy: 0.6415 - val_loss: 1.0146 - val_accuracy: 0.4858\n",
      "Epoch 96/300\n",
      "1785/1785 [==============================] - 0s 202us/sample - loss: 0.8350 - accuracy: 0.6487 - val_loss: 1.0143 - val_accuracy: 0.4858\n",
      "Epoch 97/300\n",
      "1785/1785 [==============================] - 0s 205us/sample - loss: 0.8290 - accuracy: 0.6571 - val_loss: 1.0142 - val_accuracy: 0.4858\n",
      "Epoch 98/300\n",
      "1785/1785 [==============================] - 0s 193us/sample - loss: 0.8333 - accuracy: 0.6633 - val_loss: 1.0141 - val_accuracy: 0.4844\n",
      "Epoch 99/300\n",
      "1785/1785 [==============================] - 0s 207us/sample - loss: 0.8257 - accuracy: 0.6493 - val_loss: 1.0140 - val_accuracy: 0.4858\n",
      "Epoch 100/300\n",
      "1785/1785 [==============================] - 0s 209us/sample - loss: 0.8278 - accuracy: 0.6521 - val_loss: 1.0140 - val_accuracy: 0.4858\n",
      "Epoch 101/300\n",
      "1785/1785 [==============================] - 0s 208us/sample - loss: 0.8269 - accuracy: 0.6471 - val_loss: 1.0139 - val_accuracy: 0.4858\n",
      "Epoch 102/300\n",
      "1785/1785 [==============================] - 0s 209us/sample - loss: 0.8259 - accuracy: 0.6583 - val_loss: 1.0138 - val_accuracy: 0.4858\n",
      "Epoch 103/300\n",
      "1785/1785 [==============================] - 0s 203us/sample - loss: 0.8253 - accuracy: 0.6555 - val_loss: 1.0138 - val_accuracy: 0.4858\n",
      "Epoch 104/300\n",
      "1785/1785 [==============================] - 0s 217us/sample - loss: 0.8172 - accuracy: 0.6555 - val_loss: 1.0138 - val_accuracy: 0.4873\n",
      "Epoch 105/300\n",
      "1785/1785 [==============================] - 0s 206us/sample - loss: 0.8246 - accuracy: 0.6504 - val_loss: 1.0137 - val_accuracy: 0.4873\n",
      "Epoch 106/300\n",
      "1785/1785 [==============================] - 0s 210us/sample - loss: 0.8157 - accuracy: 0.6588 - val_loss: 1.0137 - val_accuracy: 0.4858\n",
      "Epoch 107/300\n",
      "1785/1785 [==============================] - 0s 214us/sample - loss: 0.8138 - accuracy: 0.6639 - val_loss: 1.0137 - val_accuracy: 0.4844\n",
      "Epoch 108/300\n",
      "1785/1785 [==============================] - 0s 221us/sample - loss: 0.8134 - accuracy: 0.6655 - val_loss: 1.0136 - val_accuracy: 0.4844\n",
      "Epoch 109/300\n",
      "1785/1785 [==============================] - 0s 215us/sample - loss: 0.8093 - accuracy: 0.6627 - val_loss: 1.0136 - val_accuracy: 0.4844\n",
      "Epoch 110/300\n",
      "1785/1785 [==============================] - 0s 210us/sample - loss: 0.8020 - accuracy: 0.6678 - val_loss: 1.0137 - val_accuracy: 0.4844\n",
      "Epoch 111/300\n",
      "1785/1785 [==============================] - 0s 188us/sample - loss: 0.8027 - accuracy: 0.6739 - val_loss: 1.0138 - val_accuracy: 0.4830\n",
      "Epoch 00111: early stopping\n",
      "265/265 [==============================] - 0s 104us/sample - loss: 1.3106 - accuracy: 0.2453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [37:15, 746.74s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:06<00:13,  6.60s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:13<00:06,  6.55s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 586.7265625 steps, validate for 221.546875 steps\n",
      "Epoch 1/300\n",
      "587/586 [==============================] - 19s 32ms/step - loss: 1.1047 - accuracy: 0.4310 - val_loss: 1.1215 - val_accuracy: 0.4119\n",
      "Epoch 2/300\n",
      "587/586 [==============================] - 17s 30ms/step - loss: 1.0096 - accuracy: 0.4863 - val_loss: 1.0966 - val_accuracy: 0.4151\n",
      "Epoch 3/300\n",
      "587/586 [==============================] - 17s 30ms/step - loss: 0.9836 - accuracy: 0.5048 - val_loss: 1.0858 - val_accuracy: 0.4249\n",
      "Epoch 4/300\n",
      "587/586 [==============================] - 17s 30ms/step - loss: 0.9665 - accuracy: 0.5207 - val_loss: 1.0830 - val_accuracy: 0.4259\n",
      "Epoch 5/300\n",
      "587/586 [==============================] - 17s 30ms/step - loss: 0.9526 - accuracy: 0.5317 - val_loss: 1.0805 - val_accuracy: 0.4257\n",
      "Epoch 6/300\n",
      "587/586 [==============================] - 17s 30ms/step - loss: 0.9410 - accuracy: 0.5420 - val_loss: 1.0807 - val_accuracy: 0.4313\n",
      "Epoch 7/300\n",
      "587/586 [==============================] - 18s 30ms/step - loss: 0.9305 - accuracy: 0.5493 - val_loss: 1.0836 - val_accuracy: 0.4296\n",
      "Epoch 8/300\n",
      "587/586 [==============================] - 18s 30ms/step - loss: 0.9210 - accuracy: 0.5556 - val_loss: 1.0836 - val_accuracy: 0.4328\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 586.7265625 steps, validate for 221.546875 steps\n",
      "Epoch 1/300\n",
      "587/586 [==============================] - 40s 68ms/step - loss: 0.9024 - accuracy: 0.5701 - val_loss: 1.0652 - val_accuracy: 0.4487\n",
      "Epoch 2/300\n",
      "587/586 [==============================] - 39s 66ms/step - loss: 0.8723 - accuracy: 0.5919 - val_loss: 1.0659 - val_accuracy: 0.4641\n",
      "Epoch 3/300\n",
      "587/586 [==============================] - 39s 66ms/step - loss: 0.8482 - accuracy: 0.6087 - val_loss: 1.0697 - val_accuracy: 0.4515\n",
      "Epoch 4/300\n",
      "587/586 [==============================] - 39s 66ms/step - loss: 0.8270 - accuracy: 0.6249 - val_loss: 1.1327 - val_accuracy: 0.4369\n",
      "Epoch 00004: early stopping\n",
      "325/325 [==============================] - 0s 990us/sample - loss: 1.1122 - accuracy: 0.4215\n",
      "312/312 [==============================] - 0s 234us/sample - loss: 1.0926 - accuracy: 0.4199\n",
      "308/308 [==============================] - 0s 242us/sample - loss: 1.1052 - accuracy: 0.3799\n",
      "295/295 [==============================] - 0s 214us/sample - loss: 1.1787 - accuracy: 0.3390\n",
      "292/292 [==============================] - 0s 269us/sample - loss: 1.1932 - accuracy: 0.3459\n",
      "287/287 [==============================] - 0s 226us/sample - loss: 1.1918 - accuracy: 0.3449\n",
      "285/285 [==============================] - 0s 255us/sample - loss: 1.1719 - accuracy: 0.3895\n",
      "282/282 [==============================] - 0s 271us/sample - loss: 1.2293 - accuracy: 0.3546\n",
      "282/282 [==============================] - 0s 273us/sample - loss: 1.2669 - accuracy: 0.3262\n",
      "280/280 [==============================] - 0s 258us/sample - loss: 1.2445 - accuracy: 0.3536\n",
      "279/279 [==============================] - 0s 229us/sample - loss: 1.2475 - accuracy: 0.3082\n",
      "278/278 [==============================] - 0s 268us/sample - loss: 1.2617 - accuracy: 0.3237\n",
      "278/278 [==============================] - 0s 251us/sample - loss: 1.2534 - accuracy: 0.3489\n",
      "270/270 [==============================] - 0s 230us/sample - loss: 1.2780 - accuracy: 0.3185\n",
      "269/269 [==============================] - 0s 231us/sample - loss: 1.3116 - accuracy: 0.2862\n",
      "265/265 [==============================] - 0s 231us/sample - loss: 1.2942 - accuracy: 0.3434\n",
      "267/267 [==============================] - 0s 247us/sample - loss: 1.3236 - accuracy: 0.2959\n",
      "269/269 [==============================] - 0s 240us/sample - loss: 1.3062 - accuracy: 0.3123\n",
      "267/267 [==============================] - 0s 249us/sample - loss: 1.3954 - accuracy: 0.2360\n",
      "268/268 [==============================] - 0s 236us/sample - loss: 1.2986 - accuracy: 0.3358\n",
      "265/265 [==============================] - 0s 247us/sample - loss: 1.3385 - accuracy: 0.2868\n",
      "263/263 [==============================] - 0s 269us/sample - loss: 1.3566 - accuracy: 0.2928\n",
      "261/261 [==============================] - 0s 233us/sample - loss: 1.3684 - accuracy: 0.3103\n",
      "264/264 [==============================] - 0s 235us/sample - loss: 1.3404 - accuracy: 0.2803\n",
      "263/263 [==============================] - 0s 236us/sample - loss: 1.2871 - accuracy: 0.3270\n",
      "265/265 [==============================] - 0s 251us/sample - loss: 1.3214 - accuracy: 0.3170\n",
      "262/262 [==============================] - 0s 236us/sample - loss: 1.3167 - accuracy: 0.3206\n",
      "260/260 [==============================] - 0s 233us/sample - loss: 1.2858 - accuracy: 0.3346\n",
      "259/259 [==============================] - 0s 257us/sample - loss: 1.3392 - accuracy: 0.3050\n",
      "255/255 [==============================] - 0s 276us/sample - loss: 1.2725 - accuracy: 0.3569\n",
      "256/256 [==============================] - 0s 1ms/sample - loss: 1.2630 - accuracy: 0.3359\n",
      "253/253 [==============================] - 0s 233us/sample - loss: 1.2820 - accuracy: 0.3518\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 1815 samples, validate on 708 samples\n",
      "Epoch 1/300\n",
      "1815/1815 [==============================] - 2s 1ms/sample - loss: 1.2154 - accuracy: 0.3085 - val_loss: 1.1612 - val_accuracy: 0.3460\n",
      "Epoch 2/300\n",
      "1815/1815 [==============================] - 0s 196us/sample - loss: 1.1825 - accuracy: 0.3344 - val_loss: 1.1510 - val_accuracy: 0.3489\n",
      "Epoch 3/300\n",
      "1815/1815 [==============================] - 0s 195us/sample - loss: 1.1737 - accuracy: 0.3333 - val_loss: 1.1442 - val_accuracy: 0.3602\n",
      "Epoch 4/300\n",
      "1815/1815 [==============================] - 0s 209us/sample - loss: 1.1575 - accuracy: 0.3526 - val_loss: 1.1382 - val_accuracy: 0.3602\n",
      "Epoch 5/300\n",
      "1815/1815 [==============================] - 0s 207us/sample - loss: 1.1477 - accuracy: 0.3504 - val_loss: 1.1336 - val_accuracy: 0.3644\n",
      "Epoch 6/300\n",
      "1815/1815 [==============================] - 0s 208us/sample - loss: 1.1357 - accuracy: 0.3620 - val_loss: 1.1293 - val_accuracy: 0.3630\n",
      "Epoch 7/300\n",
      "1815/1815 [==============================] - 0s 192us/sample - loss: 1.1294 - accuracy: 0.3752 - val_loss: 1.1256 - val_accuracy: 0.3630\n",
      "Epoch 8/300\n",
      "1815/1815 [==============================] - 0s 205us/sample - loss: 1.1372 - accuracy: 0.3598 - val_loss: 1.1223 - val_accuracy: 0.3630\n",
      "Epoch 9/300\n",
      "1815/1815 [==============================] - 0s 206us/sample - loss: 1.1134 - accuracy: 0.3851 - val_loss: 1.1192 - val_accuracy: 0.3630\n",
      "Epoch 10/300\n",
      "1815/1815 [==============================] - 0s 193us/sample - loss: 1.1132 - accuracy: 0.3939 - val_loss: 1.1165 - val_accuracy: 0.3715\n",
      "Epoch 11/300\n",
      "1815/1815 [==============================] - 0s 202us/sample - loss: 1.1143 - accuracy: 0.3857 - val_loss: 1.1140 - val_accuracy: 0.3701\n",
      "Epoch 12/300\n",
      "1815/1815 [==============================] - 0s 202us/sample - loss: 1.1112 - accuracy: 0.3890 - val_loss: 1.1115 - val_accuracy: 0.3729\n",
      "Epoch 13/300\n",
      "1815/1815 [==============================] - 0s 194us/sample - loss: 1.1059 - accuracy: 0.3934 - val_loss: 1.1092 - val_accuracy: 0.3729\n",
      "Epoch 14/300\n",
      "1815/1815 [==============================] - 0s 194us/sample - loss: 1.0972 - accuracy: 0.4033 - val_loss: 1.1072 - val_accuracy: 0.3743\n",
      "Epoch 15/300\n",
      "1815/1815 [==============================] - 0s 191us/sample - loss: 1.1004 - accuracy: 0.4044 - val_loss: 1.1052 - val_accuracy: 0.3729\n",
      "Epoch 16/300\n",
      "1815/1815 [==============================] - 0s 205us/sample - loss: 1.0891 - accuracy: 0.3901 - val_loss: 1.1033 - val_accuracy: 0.3771\n",
      "Epoch 17/300\n",
      "1815/1815 [==============================] - 0s 209us/sample - loss: 1.0819 - accuracy: 0.4281 - val_loss: 1.1015 - val_accuracy: 0.3771\n",
      "Epoch 18/300\n",
      "1815/1815 [==============================] - 0s 198us/sample - loss: 1.0767 - accuracy: 0.4353 - val_loss: 1.0997 - val_accuracy: 0.3799\n",
      "Epoch 19/300\n",
      "1815/1815 [==============================] - 0s 218us/sample - loss: 1.0694 - accuracy: 0.4331 - val_loss: 1.0982 - val_accuracy: 0.3785\n",
      "Epoch 20/300\n",
      "1815/1815 [==============================] - 0s 212us/sample - loss: 1.0728 - accuracy: 0.4380 - val_loss: 1.0965 - val_accuracy: 0.3771\n",
      "Epoch 21/300\n",
      "1815/1815 [==============================] - 0s 213us/sample - loss: 1.0804 - accuracy: 0.4165 - val_loss: 1.0950 - val_accuracy: 0.3814\n",
      "Epoch 22/300\n",
      "1815/1815 [==============================] - 0s 213us/sample - loss: 1.0738 - accuracy: 0.4386 - val_loss: 1.0936 - val_accuracy: 0.3870\n",
      "Epoch 23/300\n",
      "1815/1815 [==============================] - 0s 194us/sample - loss: 1.0654 - accuracy: 0.4364 - val_loss: 1.0922 - val_accuracy: 0.3884\n",
      "Epoch 24/300\n",
      "1815/1815 [==============================] - 0s 212us/sample - loss: 1.0735 - accuracy: 0.4391 - val_loss: 1.0908 - val_accuracy: 0.3898\n",
      "Epoch 25/300\n",
      "1815/1815 [==============================] - 0s 209us/sample - loss: 1.0593 - accuracy: 0.4518 - val_loss: 1.0895 - val_accuracy: 0.3884\n",
      "Epoch 26/300\n",
      "1815/1815 [==============================] - 0s 206us/sample - loss: 1.0519 - accuracy: 0.4468 - val_loss: 1.0882 - val_accuracy: 0.3912\n",
      "Epoch 27/300\n",
      "1815/1815 [==============================] - 0s 196us/sample - loss: 1.0608 - accuracy: 0.4485 - val_loss: 1.0871 - val_accuracy: 0.3870\n",
      "Epoch 28/300\n",
      "1815/1815 [==============================] - 0s 193us/sample - loss: 1.0545 - accuracy: 0.4430 - val_loss: 1.0859 - val_accuracy: 0.3870\n",
      "Epoch 29/300\n",
      "1815/1815 [==============================] - 0s 210us/sample - loss: 1.0505 - accuracy: 0.4507 - val_loss: 1.0848 - val_accuracy: 0.3898\n",
      "Epoch 30/300\n",
      "1815/1815 [==============================] - 0s 190us/sample - loss: 1.0521 - accuracy: 0.4468 - val_loss: 1.0838 - val_accuracy: 0.3898\n",
      "Epoch 31/300\n",
      "1815/1815 [==============================] - 0s 195us/sample - loss: 1.0504 - accuracy: 0.4540 - val_loss: 1.0827 - val_accuracy: 0.3927\n",
      "Epoch 32/300\n",
      "1815/1815 [==============================] - 0s 189us/sample - loss: 1.0361 - accuracy: 0.4716 - val_loss: 1.0817 - val_accuracy: 0.3912\n",
      "Epoch 33/300\n",
      "1815/1815 [==============================] - 0s 196us/sample - loss: 1.0378 - accuracy: 0.4749 - val_loss: 1.0807 - val_accuracy: 0.3941\n",
      "Epoch 34/300\n",
      "1815/1815 [==============================] - 0s 201us/sample - loss: 1.0363 - accuracy: 0.4876 - val_loss: 1.0797 - val_accuracy: 0.3912\n",
      "Epoch 35/300\n",
      "1815/1815 [==============================] - 0s 190us/sample - loss: 1.0293 - accuracy: 0.4727 - val_loss: 1.0788 - val_accuracy: 0.3912\n",
      "Epoch 36/300\n",
      "1815/1815 [==============================] - 0s 195us/sample - loss: 1.0340 - accuracy: 0.4722 - val_loss: 1.0779 - val_accuracy: 0.3912\n",
      "Epoch 37/300\n",
      "1815/1815 [==============================] - 0s 190us/sample - loss: 1.0263 - accuracy: 0.4738 - val_loss: 1.0770 - val_accuracy: 0.3927\n",
      "Epoch 38/300\n",
      "1815/1815 [==============================] - 0s 196us/sample - loss: 1.0328 - accuracy: 0.4711 - val_loss: 1.0760 - val_accuracy: 0.3927\n",
      "Epoch 39/300\n",
      "1815/1815 [==============================] - 0s 196us/sample - loss: 1.0250 - accuracy: 0.4848 - val_loss: 1.0753 - val_accuracy: 0.3927\n",
      "Epoch 40/300\n",
      "1815/1815 [==============================] - 0s 194us/sample - loss: 1.0301 - accuracy: 0.4826 - val_loss: 1.0744 - val_accuracy: 0.3927\n",
      "Epoch 41/300\n",
      "1815/1815 [==============================] - 0s 206us/sample - loss: 1.0233 - accuracy: 0.4799 - val_loss: 1.0736 - val_accuracy: 0.3941\n",
      "Epoch 42/300\n",
      "1815/1815 [==============================] - 0s 190us/sample - loss: 1.0153 - accuracy: 0.4826 - val_loss: 1.0728 - val_accuracy: 0.3969\n",
      "Epoch 43/300\n",
      "1815/1815 [==============================] - 0s 189us/sample - loss: 1.0232 - accuracy: 0.4821 - val_loss: 1.0720 - val_accuracy: 0.3969\n",
      "Epoch 44/300\n",
      "1815/1815 [==============================] - 0s 195us/sample - loss: 1.0327 - accuracy: 0.4804 - val_loss: 1.0713 - val_accuracy: 0.3997\n",
      "Epoch 45/300\n",
      "1815/1815 [==============================] - 0s 184us/sample - loss: 1.0161 - accuracy: 0.4843 - val_loss: 1.0705 - val_accuracy: 0.4025\n",
      "Epoch 46/300\n",
      "1815/1815 [==============================] - 0s 204us/sample - loss: 1.0240 - accuracy: 0.4799 - val_loss: 1.0699 - val_accuracy: 0.4025\n",
      "Epoch 47/300\n",
      "1815/1815 [==============================] - 0s 211us/sample - loss: 1.0129 - accuracy: 0.4937 - val_loss: 1.0691 - val_accuracy: 0.4082\n",
      "Epoch 48/300\n",
      "1815/1815 [==============================] - 0s 210us/sample - loss: 1.0070 - accuracy: 0.4926 - val_loss: 1.0684 - val_accuracy: 0.4082\n",
      "Epoch 49/300\n",
      "1815/1815 [==============================] - 0s 203us/sample - loss: 1.0101 - accuracy: 0.4876 - val_loss: 1.0676 - val_accuracy: 0.4096\n",
      "Epoch 50/300\n",
      "1815/1815 [==============================] - 0s 213us/sample - loss: 1.0096 - accuracy: 0.4992 - val_loss: 1.0669 - val_accuracy: 0.4110\n",
      "Epoch 51/300\n",
      "1815/1815 [==============================] - 0s 212us/sample - loss: 1.0148 - accuracy: 0.4986 - val_loss: 1.0663 - val_accuracy: 0.4124\n",
      "Epoch 52/300\n",
      "1815/1815 [==============================] - 0s 189us/sample - loss: 1.0051 - accuracy: 0.5030 - val_loss: 1.0657 - val_accuracy: 0.4124\n",
      "Epoch 53/300\n",
      "1815/1815 [==============================] - 0s 202us/sample - loss: 1.0093 - accuracy: 0.4887 - val_loss: 1.0649 - val_accuracy: 0.4110\n",
      "Epoch 54/300\n",
      "1815/1815 [==============================] - 0s 204us/sample - loss: 0.9974 - accuracy: 0.5080 - val_loss: 1.0643 - val_accuracy: 0.4138\n",
      "Epoch 55/300\n",
      "1815/1815 [==============================] - 0s 199us/sample - loss: 0.9999 - accuracy: 0.4848 - val_loss: 1.0637 - val_accuracy: 0.4153\n",
      "Epoch 56/300\n",
      "1815/1815 [==============================] - 0s 210us/sample - loss: 0.9965 - accuracy: 0.5019 - val_loss: 1.0631 - val_accuracy: 0.4153\n",
      "Epoch 57/300\n",
      "1815/1815 [==============================] - 0s 201us/sample - loss: 0.9913 - accuracy: 0.5074 - val_loss: 1.0625 - val_accuracy: 0.4153\n",
      "Epoch 58/300\n",
      "1815/1815 [==============================] - 0s 184us/sample - loss: 0.9925 - accuracy: 0.5096 - val_loss: 1.0619 - val_accuracy: 0.4181\n",
      "Epoch 59/300\n",
      "1815/1815 [==============================] - 0s 184us/sample - loss: 0.9932 - accuracy: 0.5019 - val_loss: 1.0614 - val_accuracy: 0.4209\n",
      "Epoch 60/300\n",
      "1815/1815 [==============================] - 0s 201us/sample - loss: 0.9907 - accuracy: 0.5163 - val_loss: 1.0609 - val_accuracy: 0.4209\n",
      "Epoch 61/300\n",
      "1815/1815 [==============================] - 0s 205us/sample - loss: 0.9873 - accuracy: 0.5190 - val_loss: 1.0603 - val_accuracy: 0.4209\n",
      "Epoch 62/300\n",
      "1815/1815 [==============================] - 0s 195us/sample - loss: 0.9783 - accuracy: 0.5157 - val_loss: 1.0599 - val_accuracy: 0.4223\n",
      "Epoch 63/300\n",
      "1815/1815 [==============================] - 0s 198us/sample - loss: 0.9777 - accuracy: 0.5212 - val_loss: 1.0592 - val_accuracy: 0.4266\n",
      "Epoch 64/300\n",
      "1815/1815 [==============================] - 0s 193us/sample - loss: 0.9820 - accuracy: 0.5190 - val_loss: 1.0587 - val_accuracy: 0.4266\n",
      "Epoch 65/300\n",
      "1815/1815 [==============================] - 0s 192us/sample - loss: 0.9802 - accuracy: 0.5234 - val_loss: 1.0582 - val_accuracy: 0.4280\n",
      "Epoch 66/300\n",
      "1815/1815 [==============================] - 0s 206us/sample - loss: 0.9777 - accuracy: 0.5350 - val_loss: 1.0576 - val_accuracy: 0.4280\n",
      "Epoch 67/300\n",
      "1815/1815 [==============================] - 0s 198us/sample - loss: 0.9769 - accuracy: 0.5196 - val_loss: 1.0569 - val_accuracy: 0.4294\n",
      "Epoch 68/300\n",
      "1815/1815 [==============================] - 0s 187us/sample - loss: 0.9781 - accuracy: 0.5284 - val_loss: 1.0565 - val_accuracy: 0.4294\n",
      "Epoch 69/300\n",
      "1815/1815 [==============================] - 0s 202us/sample - loss: 0.9735 - accuracy: 0.5185 - val_loss: 1.0561 - val_accuracy: 0.4294\n",
      "Epoch 70/300\n",
      "1815/1815 [==============================] - 0s 189us/sample - loss: 0.9763 - accuracy: 0.5229 - val_loss: 1.0556 - val_accuracy: 0.4266\n",
      "Epoch 71/300\n",
      "1815/1815 [==============================] - 0s 211us/sample - loss: 0.9697 - accuracy: 0.5218 - val_loss: 1.0551 - val_accuracy: 0.4266\n",
      "Epoch 72/300\n",
      "1815/1815 [==============================] - 0s 213us/sample - loss: 0.9705 - accuracy: 0.5366 - val_loss: 1.0546 - val_accuracy: 0.4294\n",
      "Epoch 73/300\n",
      "1815/1815 [==============================] - 0s 200us/sample - loss: 0.9687 - accuracy: 0.5273 - val_loss: 1.0542 - val_accuracy: 0.4308\n",
      "Epoch 74/300\n",
      "1815/1815 [==============================] - 0s 207us/sample - loss: 0.9692 - accuracy: 0.5262 - val_loss: 1.0538 - val_accuracy: 0.4280\n",
      "Epoch 75/300\n",
      "1815/1815 [==============================] - 0s 185us/sample - loss: 0.9698 - accuracy: 0.5339 - val_loss: 1.0534 - val_accuracy: 0.4280\n",
      "Epoch 76/300\n",
      "1815/1815 [==============================] - 0s 202us/sample - loss: 0.9689 - accuracy: 0.5234 - val_loss: 1.0530 - val_accuracy: 0.4294\n",
      "Epoch 77/300\n",
      "1815/1815 [==============================] - 0s 212us/sample - loss: 0.9542 - accuracy: 0.5493 - val_loss: 1.0525 - val_accuracy: 0.4294\n",
      "Epoch 78/300\n",
      "1815/1815 [==============================] - 0s 212us/sample - loss: 0.9660 - accuracy: 0.5229 - val_loss: 1.0521 - val_accuracy: 0.4294\n",
      "Epoch 79/300\n",
      "1815/1815 [==============================] - 0s 211us/sample - loss: 0.9513 - accuracy: 0.5521 - val_loss: 1.0517 - val_accuracy: 0.4308\n",
      "Epoch 80/300\n",
      "1815/1815 [==============================] - 0s 212us/sample - loss: 0.9566 - accuracy: 0.5438 - val_loss: 1.0513 - val_accuracy: 0.4294\n",
      "Epoch 81/300\n",
      "1815/1815 [==============================] - 0s 200us/sample - loss: 0.9635 - accuracy: 0.5273 - val_loss: 1.0508 - val_accuracy: 0.4294\n",
      "Epoch 82/300\n",
      "1815/1815 [==============================] - 0s 217us/sample - loss: 0.9532 - accuracy: 0.5410 - val_loss: 1.0503 - val_accuracy: 0.4294\n",
      "Epoch 83/300\n",
      "1815/1815 [==============================] - 0s 220us/sample - loss: 0.9462 - accuracy: 0.5433 - val_loss: 1.0500 - val_accuracy: 0.4308\n",
      "Epoch 84/300\n",
      "1815/1815 [==============================] - 0s 211us/sample - loss: 0.9589 - accuracy: 0.5383 - val_loss: 1.0496 - val_accuracy: 0.4322\n",
      "Epoch 85/300\n",
      "1815/1815 [==============================] - 0s 207us/sample - loss: 0.9470 - accuracy: 0.5421 - val_loss: 1.0492 - val_accuracy: 0.4308\n",
      "Epoch 86/300\n",
      "1815/1815 [==============================] - 0s 209us/sample - loss: 0.9424 - accuracy: 0.5537 - val_loss: 1.0489 - val_accuracy: 0.4294\n",
      "Epoch 87/300\n",
      "1815/1815 [==============================] - 0s 199us/sample - loss: 0.9529 - accuracy: 0.5433 - val_loss: 1.0483 - val_accuracy: 0.4308\n",
      "Epoch 88/300\n",
      "1815/1815 [==============================] - 0s 223us/sample - loss: 0.9443 - accuracy: 0.5466 - val_loss: 1.0479 - val_accuracy: 0.4308\n",
      "Epoch 89/300\n",
      "1815/1815 [==============================] - 0s 213us/sample - loss: 0.9419 - accuracy: 0.5515 - val_loss: 1.0476 - val_accuracy: 0.4280\n",
      "Epoch 90/300\n",
      "1815/1815 [==============================] - 0s 201us/sample - loss: 0.9471 - accuracy: 0.5554 - val_loss: 1.0472 - val_accuracy: 0.4322\n",
      "Epoch 91/300\n",
      "1815/1815 [==============================] - 0s 200us/sample - loss: 0.9369 - accuracy: 0.5548 - val_loss: 1.0469 - val_accuracy: 0.4294\n",
      "Epoch 92/300\n",
      "1815/1815 [==============================] - 0s 204us/sample - loss: 0.9412 - accuracy: 0.5559 - val_loss: 1.0465 - val_accuracy: 0.4294\n",
      "Epoch 93/300\n",
      "1815/1815 [==============================] - 0s 207us/sample - loss: 0.9372 - accuracy: 0.5559 - val_loss: 1.0462 - val_accuracy: 0.4308\n",
      "Epoch 94/300\n",
      "1815/1815 [==============================] - 0s 199us/sample - loss: 0.9382 - accuracy: 0.5515 - val_loss: 1.0459 - val_accuracy: 0.4308\n",
      "Epoch 95/300\n",
      "1815/1815 [==============================] - 0s 189us/sample - loss: 0.9441 - accuracy: 0.5543 - val_loss: 1.0456 - val_accuracy: 0.4322\n",
      "Epoch 96/300\n",
      "1815/1815 [==============================] - 0s 196us/sample - loss: 0.9393 - accuracy: 0.5614 - val_loss: 1.0452 - val_accuracy: 0.4350\n",
      "Epoch 97/300\n",
      "1815/1815 [==============================] - 0s 206us/sample - loss: 0.9325 - accuracy: 0.5609 - val_loss: 1.0448 - val_accuracy: 0.4393\n",
      "Epoch 98/300\n",
      "1815/1815 [==============================] - 0s 200us/sample - loss: 0.9306 - accuracy: 0.5603 - val_loss: 1.0447 - val_accuracy: 0.4364\n",
      "Epoch 99/300\n",
      "1815/1815 [==============================] - 0s 198us/sample - loss: 0.9292 - accuracy: 0.5603 - val_loss: 1.0444 - val_accuracy: 0.4393\n",
      "Epoch 100/300\n",
      "1815/1815 [==============================] - 0s 198us/sample - loss: 0.9329 - accuracy: 0.5653 - val_loss: 1.0441 - val_accuracy: 0.4379\n",
      "Epoch 101/300\n",
      "1815/1815 [==============================] - 0s 206us/sample - loss: 0.9339 - accuracy: 0.5537 - val_loss: 1.0437 - val_accuracy: 0.4393\n",
      "Epoch 102/300\n",
      "1815/1815 [==============================] - 0s 212us/sample - loss: 0.9269 - accuracy: 0.5636 - val_loss: 1.0435 - val_accuracy: 0.4393\n",
      "Epoch 103/300\n",
      "1815/1815 [==============================] - 0s 185us/sample - loss: 0.9332 - accuracy: 0.5576 - val_loss: 1.0432 - val_accuracy: 0.4421\n",
      "Epoch 104/300\n",
      "1815/1815 [==============================] - 0s 198us/sample - loss: 0.9196 - accuracy: 0.5647 - val_loss: 1.0429 - val_accuracy: 0.4435\n",
      "Epoch 105/300\n",
      "1815/1815 [==============================] - 0s 190us/sample - loss: 0.9247 - accuracy: 0.5603 - val_loss: 1.0426 - val_accuracy: 0.4421\n",
      "Epoch 106/300\n",
      "1815/1815 [==============================] - 0s 201us/sample - loss: 0.9187 - accuracy: 0.5730 - val_loss: 1.0423 - val_accuracy: 0.4449\n",
      "Epoch 107/300\n",
      "1815/1815 [==============================] - 0s 205us/sample - loss: 0.9215 - accuracy: 0.5686 - val_loss: 1.0419 - val_accuracy: 0.4449\n",
      "Epoch 108/300\n",
      "1815/1815 [==============================] - 0s 196us/sample - loss: 0.9248 - accuracy: 0.5675 - val_loss: 1.0415 - val_accuracy: 0.4449\n",
      "Epoch 109/300\n",
      "1815/1815 [==============================] - 0s 199us/sample - loss: 0.9172 - accuracy: 0.5625 - val_loss: 1.0413 - val_accuracy: 0.4463\n",
      "Epoch 110/300\n",
      "1815/1815 [==============================] - 0s 192us/sample - loss: 0.9166 - accuracy: 0.5642 - val_loss: 1.0410 - val_accuracy: 0.4463\n",
      "Epoch 111/300\n",
      "1815/1815 [==============================] - 0s 198us/sample - loss: 0.9126 - accuracy: 0.5680 - val_loss: 1.0409 - val_accuracy: 0.4463\n",
      "Epoch 112/300\n",
      "1815/1815 [==============================] - 0s 207us/sample - loss: 0.9172 - accuracy: 0.5653 - val_loss: 1.0405 - val_accuracy: 0.4477\n",
      "Epoch 113/300\n",
      "1815/1815 [==============================] - 0s 200us/sample - loss: 0.9068 - accuracy: 0.5857 - val_loss: 1.0402 - val_accuracy: 0.4477\n",
      "Epoch 114/300\n",
      "1815/1815 [==============================] - 0s 207us/sample - loss: 0.9134 - accuracy: 0.5675 - val_loss: 1.0400 - val_accuracy: 0.4477\n",
      "Epoch 115/300\n",
      "1815/1815 [==============================] - 0s 199us/sample - loss: 0.9059 - accuracy: 0.5796 - val_loss: 1.0398 - val_accuracy: 0.4506\n",
      "Epoch 116/300\n",
      "1815/1815 [==============================] - 0s 193us/sample - loss: 0.9159 - accuracy: 0.5774 - val_loss: 1.0395 - val_accuracy: 0.4506\n",
      "Epoch 117/300\n",
      "1815/1815 [==============================] - 0s 202us/sample - loss: 0.9060 - accuracy: 0.5818 - val_loss: 1.0393 - val_accuracy: 0.4477\n",
      "Epoch 118/300\n",
      "1815/1815 [==============================] - 0s 184us/sample - loss: 0.9111 - accuracy: 0.5769 - val_loss: 1.0390 - val_accuracy: 0.4520\n",
      "Epoch 119/300\n",
      "1815/1815 [==============================] - 0s 193us/sample - loss: 0.9054 - accuracy: 0.5802 - val_loss: 1.0388 - val_accuracy: 0.4520\n",
      "Epoch 120/300\n",
      "1815/1815 [==============================] - 0s 181us/sample - loss: 0.9019 - accuracy: 0.5769 - val_loss: 1.0387 - val_accuracy: 0.4520\n",
      "Epoch 121/300\n",
      "1815/1815 [==============================] - 0s 207us/sample - loss: 0.8985 - accuracy: 0.5917 - val_loss: 1.0386 - val_accuracy: 0.4520\n",
      "Epoch 122/300\n",
      "1815/1815 [==============================] - 0s 210us/sample - loss: 0.8985 - accuracy: 0.5884 - val_loss: 1.0384 - val_accuracy: 0.4534\n",
      "Epoch 123/300\n",
      "1815/1815 [==============================] - 0s 188us/sample - loss: 0.8958 - accuracy: 0.5906 - val_loss: 1.0381 - val_accuracy: 0.4534\n",
      "Epoch 124/300\n",
      "1815/1815 [==============================] - 0s 200us/sample - loss: 0.8952 - accuracy: 0.5807 - val_loss: 1.0379 - val_accuracy: 0.4548\n",
      "Epoch 125/300\n",
      "1815/1815 [==============================] - 0s 195us/sample - loss: 0.8992 - accuracy: 0.5945 - val_loss: 1.0377 - val_accuracy: 0.4548\n",
      "Epoch 126/300\n",
      "1815/1815 [==============================] - 0s 206us/sample - loss: 0.8909 - accuracy: 0.6022 - val_loss: 1.0374 - val_accuracy: 0.4534\n",
      "Epoch 127/300\n",
      "1815/1815 [==============================] - 0s 205us/sample - loss: 0.9008 - accuracy: 0.5835 - val_loss: 1.0373 - val_accuracy: 0.4534\n",
      "Epoch 128/300\n",
      "1815/1815 [==============================] - 0s 193us/sample - loss: 0.8912 - accuracy: 0.5763 - val_loss: 1.0369 - val_accuracy: 0.4548\n",
      "Epoch 129/300\n",
      "1815/1815 [==============================] - 0s 195us/sample - loss: 0.8859 - accuracy: 0.5983 - val_loss: 1.0368 - val_accuracy: 0.4520\n",
      "Epoch 130/300\n",
      "1815/1815 [==============================] - 0s 199us/sample - loss: 0.8894 - accuracy: 0.5934 - val_loss: 1.0365 - val_accuracy: 0.4534\n",
      "Epoch 131/300\n",
      "1815/1815 [==============================] - 0s 198us/sample - loss: 0.8918 - accuracy: 0.5950 - val_loss: 1.0363 - val_accuracy: 0.4562\n",
      "Epoch 132/300\n",
      "1815/1815 [==============================] - 0s 206us/sample - loss: 0.8803 - accuracy: 0.5956 - val_loss: 1.0361 - val_accuracy: 0.4562\n",
      "Epoch 133/300\n",
      "1815/1815 [==============================] - 0s 195us/sample - loss: 0.8845 - accuracy: 0.5950 - val_loss: 1.0358 - val_accuracy: 0.4576\n",
      "Epoch 134/300\n",
      "1815/1815 [==============================] - 0s 218us/sample - loss: 0.8823 - accuracy: 0.5917 - val_loss: 1.0357 - val_accuracy: 0.4562\n",
      "Epoch 135/300\n",
      "1815/1815 [==============================] - 0s 194us/sample - loss: 0.8818 - accuracy: 0.6033 - val_loss: 1.0354 - val_accuracy: 0.4562\n",
      "Epoch 136/300\n",
      "1815/1815 [==============================] - 0s 182us/sample - loss: 0.8845 - accuracy: 0.5857 - val_loss: 1.0354 - val_accuracy: 0.4562\n",
      "Epoch 137/300\n",
      "1815/1815 [==============================] - 0s 184us/sample - loss: 0.8866 - accuracy: 0.5857 - val_loss: 1.0352 - val_accuracy: 0.4562\n",
      "Epoch 138/300\n",
      "1815/1815 [==============================] - 0s 211us/sample - loss: 0.8777 - accuracy: 0.5945 - val_loss: 1.0351 - val_accuracy: 0.4562\n",
      "Epoch 139/300\n",
      "1815/1815 [==============================] - 0s 198us/sample - loss: 0.8825 - accuracy: 0.6000 - val_loss: 1.0350 - val_accuracy: 0.4605\n",
      "Epoch 140/300\n",
      "1815/1815 [==============================] - 0s 193us/sample - loss: 0.8723 - accuracy: 0.6055 - val_loss: 1.0349 - val_accuracy: 0.4619\n",
      "Epoch 141/300\n",
      "1815/1815 [==============================] - 0s 192us/sample - loss: 0.8787 - accuracy: 0.5846 - val_loss: 1.0348 - val_accuracy: 0.4633\n",
      "Epoch 142/300\n",
      "1815/1815 [==============================] - 0s 187us/sample - loss: 0.8687 - accuracy: 0.6017 - val_loss: 1.0349 - val_accuracy: 0.4605\n",
      "Epoch 143/300\n",
      "1815/1815 [==============================] - 0s 192us/sample - loss: 0.8722 - accuracy: 0.5994 - val_loss: 1.0347 - val_accuracy: 0.4605\n",
      "Epoch 144/300\n",
      "1815/1815 [==============================] - 0s 211us/sample - loss: 0.8654 - accuracy: 0.6033 - val_loss: 1.0345 - val_accuracy: 0.4619\n",
      "Epoch 145/300\n",
      "1815/1815 [==============================] - 0s 187us/sample - loss: 0.8653 - accuracy: 0.6176 - val_loss: 1.0343 - val_accuracy: 0.4647\n",
      "Epoch 146/300\n",
      "1815/1815 [==============================] - 0s 192us/sample - loss: 0.8701 - accuracy: 0.6099 - val_loss: 1.0344 - val_accuracy: 0.4647\n",
      "Epoch 147/300\n",
      "1815/1815 [==============================] - 0s 193us/sample - loss: 0.8712 - accuracy: 0.6017 - val_loss: 1.0343 - val_accuracy: 0.4661\n",
      "Epoch 148/300\n",
      "1815/1815 [==============================] - 0s 195us/sample - loss: 0.8643 - accuracy: 0.6011 - val_loss: 1.0341 - val_accuracy: 0.4647\n",
      "Epoch 149/300\n",
      "1815/1815 [==============================] - 0s 191us/sample - loss: 0.8695 - accuracy: 0.5994 - val_loss: 1.0340 - val_accuracy: 0.4633\n",
      "Epoch 150/300\n",
      "1815/1815 [==============================] - 0s 187us/sample - loss: 0.8682 - accuracy: 0.6116 - val_loss: 1.0341 - val_accuracy: 0.4633\n",
      "Epoch 151/300\n",
      "1815/1815 [==============================] - 0s 174us/sample - loss: 0.8570 - accuracy: 0.6094 - val_loss: 1.0340 - val_accuracy: 0.4633\n",
      "Epoch 152/300\n",
      "1815/1815 [==============================] - 0s 184us/sample - loss: 0.8573 - accuracy: 0.6160 - val_loss: 1.0340 - val_accuracy: 0.4633\n",
      "Epoch 153/300\n",
      "1815/1815 [==============================] - 0s 203us/sample - loss: 0.8642 - accuracy: 0.6105 - val_loss: 1.0340 - val_accuracy: 0.4647\n",
      "Epoch 154/300\n",
      "1815/1815 [==============================] - 0s 208us/sample - loss: 0.8637 - accuracy: 0.6176 - val_loss: 1.0341 - val_accuracy: 0.4661\n",
      "Epoch 155/300\n",
      "1815/1815 [==============================] - 0s 222us/sample - loss: 0.8577 - accuracy: 0.6264 - val_loss: 1.0339 - val_accuracy: 0.4675\n",
      "Epoch 156/300\n",
      "1815/1815 [==============================] - 0s 208us/sample - loss: 0.8549 - accuracy: 0.6149 - val_loss: 1.0339 - val_accuracy: 0.4661\n",
      "Epoch 157/300\n",
      "1815/1815 [==============================] - 0s 201us/sample - loss: 0.8545 - accuracy: 0.6055 - val_loss: 1.0338 - val_accuracy: 0.4661\n",
      "Epoch 158/300\n",
      "1815/1815 [==============================] - 0s 194us/sample - loss: 0.8564 - accuracy: 0.6160 - val_loss: 1.0338 - val_accuracy: 0.4661\n",
      "Epoch 159/300\n",
      "1815/1815 [==============================] - 0s 196us/sample - loss: 0.8522 - accuracy: 0.6127 - val_loss: 1.0336 - val_accuracy: 0.4661\n",
      "Epoch 160/300\n",
      "1815/1815 [==============================] - 0s 231us/sample - loss: 0.8540 - accuracy: 0.6198 - val_loss: 1.0336 - val_accuracy: 0.4647\n",
      "Epoch 161/300\n",
      "1815/1815 [==============================] - 0s 201us/sample - loss: 0.8486 - accuracy: 0.6253 - val_loss: 1.0336 - val_accuracy: 0.4647\n",
      "Epoch 162/300\n",
      "1815/1815 [==============================] - 0s 213us/sample - loss: 0.8510 - accuracy: 0.6220 - val_loss: 1.0335 - val_accuracy: 0.4619\n",
      "Epoch 163/300\n",
      "1815/1815 [==============================] - 0s 224us/sample - loss: 0.8440 - accuracy: 0.6187 - val_loss: 1.0334 - val_accuracy: 0.4633\n",
      "Epoch 164/300\n",
      "1815/1815 [==============================] - 0s 207us/sample - loss: 0.8474 - accuracy: 0.6088 - val_loss: 1.0333 - val_accuracy: 0.4619\n",
      "Epoch 165/300\n",
      "1815/1815 [==============================] - 0s 222us/sample - loss: 0.8481 - accuracy: 0.6209 - val_loss: 1.0332 - val_accuracy: 0.4619\n",
      "Epoch 166/300\n",
      "1815/1815 [==============================] - 0s 222us/sample - loss: 0.8461 - accuracy: 0.6275 - val_loss: 1.0334 - val_accuracy: 0.4619\n",
      "Epoch 167/300\n",
      "1815/1815 [==============================] - 0s 190us/sample - loss: 0.8410 - accuracy: 0.6242 - val_loss: 1.0333 - val_accuracy: 0.4605\n",
      "Epoch 168/300\n",
      "1815/1815 [==============================] - 0s 193us/sample - loss: 0.8451 - accuracy: 0.6226 - val_loss: 1.0334 - val_accuracy: 0.4590\n",
      "Epoch 00168: early stopping\n",
      "233/233 [==============================] - 0s 99us/sample - loss: 1.0350 - accuracy: 0.4850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [49:30, 743.39s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:06<00:12,  6.28s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:12<00:06,  6.26s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.14s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 599.7109375 steps, validate for 212.1953125 steps\n",
      "Epoch 1/300\n",
      "600/599 [==============================] - 18s 31ms/step - loss: 1.1971 - accuracy: 0.3862 - val_loss: 1.1432 - val_accuracy: 0.4055\n",
      "Epoch 2/300\n",
      "600/599 [==============================] - 17s 29ms/step - loss: 1.0155 - accuracy: 0.4801 - val_loss: 1.1113 - val_accuracy: 0.4170\n",
      "Epoch 3/300\n",
      "600/599 [==============================] - 17s 29ms/step - loss: 0.9845 - accuracy: 0.5030 - val_loss: 1.1019 - val_accuracy: 0.4245\n",
      "Epoch 4/300\n",
      "600/599 [==============================] - 18s 29ms/step - loss: 0.9658 - accuracy: 0.5201 - val_loss: 1.0998 - val_accuracy: 0.4266\n",
      "Epoch 5/300\n",
      "600/599 [==============================] - 18s 29ms/step - loss: 0.9519 - accuracy: 0.5317 - val_loss: 1.1049 - val_accuracy: 0.4237\n",
      "Epoch 6/300\n",
      "600/599 [==============================] - 18s 29ms/step - loss: 0.9399 - accuracy: 0.5420 - val_loss: 1.1002 - val_accuracy: 0.4325\n",
      "Epoch 7/300\n",
      "600/599 [==============================] - 17s 29ms/step - loss: 0.9295 - accuracy: 0.5506 - val_loss: 1.0995 - val_accuracy: 0.4299\n",
      "Epoch 8/300\n",
      "600/599 [==============================] - 17s 29ms/step - loss: 0.9204 - accuracy: 0.5572 - val_loss: 1.1016 - val_accuracy: 0.4273\n",
      "Epoch 9/300\n",
      "600/599 [==============================] - 18s 29ms/step - loss: 0.9116 - accuracy: 0.5654 - val_loss: 1.1079 - val_accuracy: 0.4283\n",
      "Epoch 10/300\n",
      "600/599 [==============================] - 18s 29ms/step - loss: 0.9040 - accuracy: 0.5719 - val_loss: 1.1075 - val_accuracy: 0.4314\n",
      "Epoch 00010: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 599.7109375 steps, validate for 212.1953125 steps\n",
      "Epoch 1/300\n",
      "600/599 [==============================] - 41s 68ms/step - loss: 0.8880 - accuracy: 0.5822 - val_loss: 1.1653 - val_accuracy: 0.4421\n",
      "Epoch 2/300\n",
      "600/599 [==============================] - 39s 66ms/step - loss: 0.8588 - accuracy: 0.6031 - val_loss: 1.1546 - val_accuracy: 0.4184\n",
      "Epoch 3/300\n",
      "600/599 [==============================] - 39s 65ms/step - loss: 0.8362 - accuracy: 0.6202 - val_loss: 1.1033 - val_accuracy: 0.4360\n",
      "Epoch 4/300\n",
      "600/599 [==============================] - 39s 65ms/step - loss: 0.8158 - accuracy: 0.6331 - val_loss: 1.1345 - val_accuracy: 0.4424\n",
      "Epoch 5/300\n",
      "600/599 [==============================] - 39s 66ms/step - loss: 0.7965 - accuracy: 0.6465 - val_loss: 1.1574 - val_accuracy: 0.4313\n",
      "Epoch 6/300\n",
      "600/599 [==============================] - 39s 65ms/step - loss: 0.7786 - accuracy: 0.6588 - val_loss: 1.1582 - val_accuracy: 0.4160\n",
      "Epoch 00006: early stopping\n",
      "318/318 [==============================] - 0s 961us/sample - loss: 1.1095 - accuracy: 0.4591\n",
      "306/306 [==============================] - 0s 248us/sample - loss: 1.0524 - accuracy: 0.4542\n",
      "301/301 [==============================] - 0s 207us/sample - loss: 1.0621 - accuracy: 0.4452\n",
      "301/301 [==============================] - 0s 219us/sample - loss: 1.1234 - accuracy: 0.4219\n",
      "295/295 [==============================] - 0s 229us/sample - loss: 1.0942 - accuracy: 0.4712\n",
      "295/295 [==============================] - 0s 247us/sample - loss: 1.0581 - accuracy: 0.4407\n",
      "286/286 [==============================] - 0s 266us/sample - loss: 1.0962 - accuracy: 0.4685\n",
      "281/281 [==============================] - 0s 263us/sample - loss: 1.0521 - accuracy: 0.4555\n",
      "278/278 [==============================] - 0s 273us/sample - loss: 1.1384 - accuracy: 0.4353\n",
      "276/276 [==============================] - 0s 257us/sample - loss: 1.0908 - accuracy: 0.4638\n",
      "276/276 [==============================] - 0s 228us/sample - loss: 1.0856 - accuracy: 0.4529\n",
      "270/270 [==============================] - 0s 246us/sample - loss: 1.1182 - accuracy: 0.4481\n",
      "268/268 [==============================] - 0s 234us/sample - loss: 1.0815 - accuracy: 0.4776\n",
      "260/260 [==============================] - 0s 238us/sample - loss: 1.0990 - accuracy: 0.4423\n",
      "257/257 [==============================] - 0s 229us/sample - loss: 1.1456 - accuracy: 0.4358\n",
      "253/253 [==============================] - 0s 229us/sample - loss: 1.0225 - accuracy: 0.5178\n",
      "252/252 [==============================] - 0s 231us/sample - loss: 1.0451 - accuracy: 0.4762\n",
      "253/253 [==============================] - 0s 221us/sample - loss: 1.0842 - accuracy: 0.4427\n",
      "252/252 [==============================] - 0s 254us/sample - loss: 1.0927 - accuracy: 0.4802\n",
      "248/248 [==============================] - 0s 241us/sample - loss: 1.0675 - accuracy: 0.5000\n",
      "245/245 [==============================] - 0s 233us/sample - loss: 1.1567 - accuracy: 0.4163\n",
      "243/243 [==============================] - 0s 237us/sample - loss: 1.0946 - accuracy: 0.4897\n",
      "244/244 [==============================] - 0s 255us/sample - loss: 1.0770 - accuracy: 0.4426\n",
      "242/242 [==============================] - 0s 239us/sample - loss: 0.9938 - accuracy: 0.5331\n",
      "237/237 [==============================] - 0s 236us/sample - loss: 1.0542 - accuracy: 0.5063\n",
      "237/237 [==============================] - 0s 236us/sample - loss: 0.9927 - accuracy: 0.5021\n",
      "233/233 [==============================] - 0s 231us/sample - loss: 1.0407 - accuracy: 0.4592\n",
      "228/228 [==============================] - 0s 238us/sample - loss: 1.0484 - accuracy: 0.4956\n",
      "227/227 [==============================] - 0s 247us/sample - loss: 0.9916 - accuracy: 0.5110\n",
      "223/223 [==============================] - 0s 262us/sample - loss: 0.9694 - accuracy: 0.5516\n",
      "220/220 [==============================] - 0s 280us/sample - loss: 0.9956 - accuracy: 0.5182\n",
      "214/214 [==============================] - 0s 258us/sample - loss: 0.9787 - accuracy: 0.5093\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 1872 samples, validate on 687 samples\n",
      "Epoch 1/300\n",
      "1872/1872 [==============================] - 3s 1ms/sample - loss: 1.2213 - accuracy: 0.2740 - val_loss: 1.1762 - val_accuracy: 0.2853\n",
      "Epoch 2/300\n",
      "1872/1872 [==============================] - 0s 189us/sample - loss: 1.1844 - accuracy: 0.3061 - val_loss: 1.1623 - val_accuracy: 0.2882\n",
      "Epoch 3/300\n",
      "1872/1872 [==============================] - 0s 198us/sample - loss: 1.1681 - accuracy: 0.3082 - val_loss: 1.1528 - val_accuracy: 0.3013\n",
      "Epoch 4/300\n",
      "1872/1872 [==============================] - 0s 207us/sample - loss: 1.1588 - accuracy: 0.3226 - val_loss: 1.1452 - val_accuracy: 0.2999\n",
      "Epoch 5/300\n",
      "1872/1872 [==============================] - 0s 227us/sample - loss: 1.1522 - accuracy: 0.3178 - val_loss: 1.1394 - val_accuracy: 0.3100\n",
      "Epoch 6/300\n",
      "1872/1872 [==============================] - 0s 189us/sample - loss: 1.1399 - accuracy: 0.3462 - val_loss: 1.1345 - val_accuracy: 0.3144\n",
      "Epoch 7/300\n",
      "1872/1872 [==============================] - 0s 186us/sample - loss: 1.1384 - accuracy: 0.3387 - val_loss: 1.1299 - val_accuracy: 0.3188\n",
      "Epoch 8/300\n",
      "1872/1872 [==============================] - 0s 202us/sample - loss: 1.1295 - accuracy: 0.3531 - val_loss: 1.1261 - val_accuracy: 0.3217\n",
      "Epoch 9/300\n",
      "1872/1872 [==============================] - 0s 187us/sample - loss: 1.1191 - accuracy: 0.3729 - val_loss: 1.1227 - val_accuracy: 0.3275\n",
      "Epoch 10/300\n",
      "1872/1872 [==============================] - 0s 192us/sample - loss: 1.1130 - accuracy: 0.3777 - val_loss: 1.1194 - val_accuracy: 0.3290\n",
      "Epoch 11/300\n",
      "1872/1872 [==============================] - 0s 189us/sample - loss: 1.1124 - accuracy: 0.3718 - val_loss: 1.1165 - val_accuracy: 0.3362\n",
      "Epoch 12/300\n",
      "1872/1872 [==============================] - 0s 202us/sample - loss: 1.1088 - accuracy: 0.3894 - val_loss: 1.1139 - val_accuracy: 0.3392\n",
      "Epoch 13/300\n",
      "1872/1872 [==============================] - 0s 199us/sample - loss: 1.1068 - accuracy: 0.3649 - val_loss: 1.1114 - val_accuracy: 0.3392\n",
      "Epoch 14/300\n",
      "1872/1872 [==============================] - 0s 208us/sample - loss: 1.1026 - accuracy: 0.3889 - val_loss: 1.1090 - val_accuracy: 0.3479\n",
      "Epoch 15/300\n",
      "1872/1872 [==============================] - 0s 197us/sample - loss: 1.0917 - accuracy: 0.3969 - val_loss: 1.1068 - val_accuracy: 0.3552\n",
      "Epoch 16/300\n",
      "1872/1872 [==============================] - 0s 203us/sample - loss: 1.0917 - accuracy: 0.3916 - val_loss: 1.1048 - val_accuracy: 0.3610\n",
      "Epoch 17/300\n",
      "1872/1872 [==============================] - 0s 187us/sample - loss: 1.0859 - accuracy: 0.4097 - val_loss: 1.1029 - val_accuracy: 0.3697\n",
      "Epoch 18/300\n",
      "1872/1872 [==============================] - 0s 195us/sample - loss: 1.0852 - accuracy: 0.4097 - val_loss: 1.1011 - val_accuracy: 0.3712\n",
      "Epoch 19/300\n",
      "1872/1872 [==============================] - 0s 197us/sample - loss: 1.0844 - accuracy: 0.4193 - val_loss: 1.0993 - val_accuracy: 0.3741\n",
      "Epoch 20/300\n",
      "1872/1872 [==============================] - 0s 189us/sample - loss: 1.0840 - accuracy: 0.4145 - val_loss: 1.0976 - val_accuracy: 0.3726\n",
      "Epoch 21/300\n",
      "1872/1872 [==============================] - 0s 200us/sample - loss: 1.0709 - accuracy: 0.4306 - val_loss: 1.0959 - val_accuracy: 0.3770\n",
      "Epoch 22/300\n",
      "1872/1872 [==============================] - 0s 195us/sample - loss: 1.0659 - accuracy: 0.4375 - val_loss: 1.0945 - val_accuracy: 0.3770\n",
      "Epoch 23/300\n",
      "1872/1872 [==============================] - 0s 184us/sample - loss: 1.0714 - accuracy: 0.4225 - val_loss: 1.0930 - val_accuracy: 0.3770\n",
      "Epoch 24/300\n",
      "1872/1872 [==============================] - 0s 196us/sample - loss: 1.0663 - accuracy: 0.4295 - val_loss: 1.0916 - val_accuracy: 0.3843\n",
      "Epoch 25/300\n",
      "1872/1872 [==============================] - 0s 193us/sample - loss: 1.0641 - accuracy: 0.4188 - val_loss: 1.0902 - val_accuracy: 0.3872\n",
      "Epoch 26/300\n",
      "1872/1872 [==============================] - 0s 206us/sample - loss: 1.0505 - accuracy: 0.4455 - val_loss: 1.0889 - val_accuracy: 0.3916\n",
      "Epoch 27/300\n",
      "1872/1872 [==============================] - 0s 206us/sample - loss: 1.0611 - accuracy: 0.4241 - val_loss: 1.0877 - val_accuracy: 0.3959\n",
      "Epoch 28/300\n",
      "1872/1872 [==============================] - 0s 215us/sample - loss: 1.0555 - accuracy: 0.4418 - val_loss: 1.0864 - val_accuracy: 0.3945\n",
      "Epoch 29/300\n",
      "1872/1872 [==============================] - 0s 212us/sample - loss: 1.0533 - accuracy: 0.4503 - val_loss: 1.0853 - val_accuracy: 0.3930\n",
      "Epoch 30/300\n",
      "1872/1872 [==============================] - 0s 193us/sample - loss: 1.0513 - accuracy: 0.4396 - val_loss: 1.0841 - val_accuracy: 0.3930\n",
      "Epoch 31/300\n",
      "1872/1872 [==============================] - 0s 186us/sample - loss: 1.0400 - accuracy: 0.4525 - val_loss: 1.0830 - val_accuracy: 0.3945\n",
      "Epoch 32/300\n",
      "1872/1872 [==============================] - 0s 217us/sample - loss: 1.0446 - accuracy: 0.4546 - val_loss: 1.0819 - val_accuracy: 0.3974\n",
      "Epoch 33/300\n",
      "1872/1872 [==============================] - 0s 196us/sample - loss: 1.0459 - accuracy: 0.4418 - val_loss: 1.0808 - val_accuracy: 0.4003\n",
      "Epoch 34/300\n",
      "1872/1872 [==============================] - 0s 189us/sample - loss: 1.0333 - accuracy: 0.4551 - val_loss: 1.0799 - val_accuracy: 0.4003\n",
      "Epoch 35/300\n",
      "1872/1872 [==============================] - 0s 193us/sample - loss: 1.0345 - accuracy: 0.4690 - val_loss: 1.0788 - val_accuracy: 0.3959\n",
      "Epoch 36/300\n",
      "1872/1872 [==============================] - 0s 188us/sample - loss: 1.0294 - accuracy: 0.4760 - val_loss: 1.0779 - val_accuracy: 0.3974\n",
      "Epoch 37/300\n",
      "1872/1872 [==============================] - 0s 206us/sample - loss: 1.0358 - accuracy: 0.4744 - val_loss: 1.0769 - val_accuracy: 0.4032\n",
      "Epoch 38/300\n",
      "1872/1872 [==============================] - 0s 205us/sample - loss: 1.0300 - accuracy: 0.4776 - val_loss: 1.0760 - val_accuracy: 0.4061\n",
      "Epoch 39/300\n",
      "1872/1872 [==============================] - 0s 209us/sample - loss: 1.0360 - accuracy: 0.4631 - val_loss: 1.0751 - val_accuracy: 0.4032\n",
      "Epoch 40/300\n",
      "1872/1872 [==============================] - 0s 194us/sample - loss: 1.0243 - accuracy: 0.4717 - val_loss: 1.0743 - val_accuracy: 0.4047\n",
      "Epoch 41/300\n",
      "1872/1872 [==============================] - 0s 197us/sample - loss: 1.0260 - accuracy: 0.4781 - val_loss: 1.0734 - val_accuracy: 0.4047\n",
      "Epoch 42/300\n",
      "1872/1872 [==============================] - 0s 199us/sample - loss: 1.0220 - accuracy: 0.4754 - val_loss: 1.0726 - val_accuracy: 0.4047\n",
      "Epoch 43/300\n",
      "1872/1872 [==============================] - 0s 192us/sample - loss: 1.0227 - accuracy: 0.4754 - val_loss: 1.0718 - val_accuracy: 0.4134\n",
      "Epoch 44/300\n",
      "1872/1872 [==============================] - 0s 204us/sample - loss: 1.0203 - accuracy: 0.4754 - val_loss: 1.0710 - val_accuracy: 0.4148\n",
      "Epoch 45/300\n",
      "1872/1872 [==============================] - 0s 188us/sample - loss: 1.0202 - accuracy: 0.4760 - val_loss: 1.0702 - val_accuracy: 0.4163\n",
      "Epoch 46/300\n",
      "1872/1872 [==============================] - 0s 186us/sample - loss: 1.0063 - accuracy: 0.4808 - val_loss: 1.0694 - val_accuracy: 0.4105\n",
      "Epoch 47/300\n",
      "1872/1872 [==============================] - 0s 193us/sample - loss: 1.0079 - accuracy: 0.4941 - val_loss: 1.0687 - val_accuracy: 0.4105\n",
      "Epoch 48/300\n",
      "1872/1872 [==============================] - 0s 206us/sample - loss: 1.0089 - accuracy: 0.4776 - val_loss: 1.0679 - val_accuracy: 0.4090\n",
      "Epoch 49/300\n",
      "1872/1872 [==============================] - 0s 197us/sample - loss: 1.0104 - accuracy: 0.4979 - val_loss: 1.0673 - val_accuracy: 0.4134\n",
      "Epoch 50/300\n",
      "1872/1872 [==============================] - 0s 202us/sample - loss: 1.0056 - accuracy: 0.4925 - val_loss: 1.0666 - val_accuracy: 0.4105\n",
      "Epoch 51/300\n",
      "1872/1872 [==============================] - 0s 193us/sample - loss: 1.0010 - accuracy: 0.4845 - val_loss: 1.0659 - val_accuracy: 0.4105\n",
      "Epoch 52/300\n",
      "1872/1872 [==============================] - 0s 198us/sample - loss: 1.0016 - accuracy: 0.5005 - val_loss: 1.0653 - val_accuracy: 0.4119\n",
      "Epoch 53/300\n",
      "1872/1872 [==============================] - 0s 205us/sample - loss: 0.9932 - accuracy: 0.5176 - val_loss: 1.0646 - val_accuracy: 0.4105\n",
      "Epoch 54/300\n",
      "1872/1872 [==============================] - 0s 206us/sample - loss: 1.0015 - accuracy: 0.4968 - val_loss: 1.0640 - val_accuracy: 0.4134\n",
      "Epoch 55/300\n",
      "1872/1872 [==============================] - 0s 206us/sample - loss: 0.9991 - accuracy: 0.5064 - val_loss: 1.0634 - val_accuracy: 0.4163\n",
      "Epoch 56/300\n",
      "1872/1872 [==============================] - 0s 206us/sample - loss: 1.0041 - accuracy: 0.5032 - val_loss: 1.0627 - val_accuracy: 0.4148\n",
      "Epoch 57/300\n",
      "1872/1872 [==============================] - 0s 196us/sample - loss: 0.9935 - accuracy: 0.5021 - val_loss: 1.0622 - val_accuracy: 0.4134\n",
      "Epoch 58/300\n",
      "1872/1872 [==============================] - 0s 202us/sample - loss: 0.9886 - accuracy: 0.5064 - val_loss: 1.0614 - val_accuracy: 0.4134\n",
      "Epoch 59/300\n",
      "1872/1872 [==============================] - 0s 205us/sample - loss: 0.9842 - accuracy: 0.5203 - val_loss: 1.0609 - val_accuracy: 0.4148\n",
      "Epoch 60/300\n",
      "1872/1872 [==============================] - 0s 195us/sample - loss: 0.9820 - accuracy: 0.5288 - val_loss: 1.0603 - val_accuracy: 0.4178\n",
      "Epoch 61/300\n",
      "1872/1872 [==============================] - 0s 203us/sample - loss: 0.9840 - accuracy: 0.5128 - val_loss: 1.0597 - val_accuracy: 0.4192\n",
      "Epoch 62/300\n",
      "1872/1872 [==============================] - 0s 196us/sample - loss: 0.9783 - accuracy: 0.5246 - val_loss: 1.0591 - val_accuracy: 0.4207\n",
      "Epoch 63/300\n",
      "1872/1872 [==============================] - 0s 184us/sample - loss: 0.9767 - accuracy: 0.5283 - val_loss: 1.0586 - val_accuracy: 0.4236\n",
      "Epoch 64/300\n",
      "1872/1872 [==============================] - 0s 185us/sample - loss: 0.9786 - accuracy: 0.5091 - val_loss: 1.0580 - val_accuracy: 0.4236\n",
      "Epoch 65/300\n",
      "1872/1872 [==============================] - 0s 199us/sample - loss: 0.9754 - accuracy: 0.5219 - val_loss: 1.0575 - val_accuracy: 0.4250\n",
      "Epoch 66/300\n",
      "1872/1872 [==============================] - 0s 194us/sample - loss: 0.9767 - accuracy: 0.5401 - val_loss: 1.0570 - val_accuracy: 0.4250\n",
      "Epoch 67/300\n",
      "1872/1872 [==============================] - 0s 193us/sample - loss: 0.9761 - accuracy: 0.5176 - val_loss: 1.0565 - val_accuracy: 0.4236\n",
      "Epoch 68/300\n",
      "1872/1872 [==============================] - 0s 195us/sample - loss: 0.9742 - accuracy: 0.5358 - val_loss: 1.0559 - val_accuracy: 0.4207\n",
      "Epoch 69/300\n",
      "1872/1872 [==============================] - 0s 206us/sample - loss: 0.9688 - accuracy: 0.5358 - val_loss: 1.0554 - val_accuracy: 0.4207\n",
      "Epoch 70/300\n",
      "1872/1872 [==============================] - 0s 198us/sample - loss: 0.9652 - accuracy: 0.5251 - val_loss: 1.0550 - val_accuracy: 0.4221\n",
      "Epoch 71/300\n",
      "1872/1872 [==============================] - 0s 199us/sample - loss: 0.9675 - accuracy: 0.5321 - val_loss: 1.0545 - val_accuracy: 0.4294\n",
      "Epoch 72/300\n",
      "1872/1872 [==============================] - 0s 198us/sample - loss: 0.9686 - accuracy: 0.5411 - val_loss: 1.0541 - val_accuracy: 0.4279\n",
      "Epoch 73/300\n",
      "1872/1872 [==============================] - 0s 191us/sample - loss: 0.9580 - accuracy: 0.5470 - val_loss: 1.0536 - val_accuracy: 0.4294\n",
      "Epoch 74/300\n",
      "1872/1872 [==============================] - 0s 202us/sample - loss: 0.9636 - accuracy: 0.5481 - val_loss: 1.0532 - val_accuracy: 0.4294\n",
      "Epoch 75/300\n",
      "1872/1872 [==============================] - 0s 204us/sample - loss: 0.9670 - accuracy: 0.5256 - val_loss: 1.0527 - val_accuracy: 0.4309\n",
      "Epoch 76/300\n",
      "1872/1872 [==============================] - 0s 205us/sample - loss: 0.9583 - accuracy: 0.5609 - val_loss: 1.0522 - val_accuracy: 0.4279\n",
      "Epoch 77/300\n",
      "1872/1872 [==============================] - 0s 203us/sample - loss: 0.9583 - accuracy: 0.5374 - val_loss: 1.0518 - val_accuracy: 0.4279\n",
      "Epoch 78/300\n",
      "1872/1872 [==============================] - 0s 209us/sample - loss: 0.9561 - accuracy: 0.5491 - val_loss: 1.0514 - val_accuracy: 0.4294\n",
      "Epoch 79/300\n",
      "1872/1872 [==============================] - 0s 207us/sample - loss: 0.9559 - accuracy: 0.5609 - val_loss: 1.0509 - val_accuracy: 0.4294\n",
      "Epoch 80/300\n",
      "1872/1872 [==============================] - 0s 215us/sample - loss: 0.9591 - accuracy: 0.5518 - val_loss: 1.0505 - val_accuracy: 0.4279\n",
      "Epoch 81/300\n",
      "1872/1872 [==============================] - 0s 202us/sample - loss: 0.9535 - accuracy: 0.5540 - val_loss: 1.0500 - val_accuracy: 0.4294\n",
      "Epoch 82/300\n",
      "1872/1872 [==============================] - 0s 191us/sample - loss: 0.9447 - accuracy: 0.5593 - val_loss: 1.0496 - val_accuracy: 0.4309\n",
      "Epoch 83/300\n",
      "1872/1872 [==============================] - 0s 195us/sample - loss: 0.9475 - accuracy: 0.5625 - val_loss: 1.0492 - val_accuracy: 0.4294\n",
      "Epoch 84/300\n",
      "1872/1872 [==============================] - 0s 198us/sample - loss: 0.9441 - accuracy: 0.5561 - val_loss: 1.0488 - val_accuracy: 0.4294\n",
      "Epoch 85/300\n",
      "1872/1872 [==============================] - 0s 188us/sample - loss: 0.9440 - accuracy: 0.5582 - val_loss: 1.0484 - val_accuracy: 0.4309\n",
      "Epoch 86/300\n",
      "1872/1872 [==============================] - 0s 198us/sample - loss: 0.9376 - accuracy: 0.5700 - val_loss: 1.0481 - val_accuracy: 0.4323\n",
      "Epoch 87/300\n",
      "1872/1872 [==============================] - 0s 191us/sample - loss: 0.9372 - accuracy: 0.5721 - val_loss: 1.0477 - val_accuracy: 0.4323\n",
      "Epoch 88/300\n",
      "1872/1872 [==============================] - 0s 200us/sample - loss: 0.9430 - accuracy: 0.5625 - val_loss: 1.0474 - val_accuracy: 0.4352\n",
      "Epoch 89/300\n",
      "1872/1872 [==============================] - 0s 213us/sample - loss: 0.9355 - accuracy: 0.5678 - val_loss: 1.0470 - val_accuracy: 0.4381\n",
      "Epoch 90/300\n",
      "1872/1872 [==============================] - 0s 200us/sample - loss: 0.9343 - accuracy: 0.5791 - val_loss: 1.0466 - val_accuracy: 0.4352\n",
      "Epoch 91/300\n",
      "1872/1872 [==============================] - 0s 192us/sample - loss: 0.9268 - accuracy: 0.5753 - val_loss: 1.0462 - val_accuracy: 0.4367\n",
      "Epoch 92/300\n",
      "1872/1872 [==============================] - 0s 188us/sample - loss: 0.9305 - accuracy: 0.5844 - val_loss: 1.0459 - val_accuracy: 0.4367\n",
      "Epoch 93/300\n",
      "1872/1872 [==============================] - 0s 185us/sample - loss: 0.9324 - accuracy: 0.5732 - val_loss: 1.0455 - val_accuracy: 0.4367\n",
      "Epoch 94/300\n",
      "1872/1872 [==============================] - 0s 189us/sample - loss: 0.9286 - accuracy: 0.5753 - val_loss: 1.0453 - val_accuracy: 0.4367\n",
      "Epoch 95/300\n",
      "1872/1872 [==============================] - 0s 199us/sample - loss: 0.9218 - accuracy: 0.5753 - val_loss: 1.0449 - val_accuracy: 0.4367\n",
      "Epoch 96/300\n",
      "1872/1872 [==============================] - 0s 196us/sample - loss: 0.9279 - accuracy: 0.5662 - val_loss: 1.0446 - val_accuracy: 0.4352\n",
      "Epoch 97/300\n",
      "1872/1872 [==============================] - 0s 209us/sample - loss: 0.9184 - accuracy: 0.5769 - val_loss: 1.0442 - val_accuracy: 0.4352\n",
      "Epoch 98/300\n",
      "1872/1872 [==============================] - 0s 196us/sample - loss: 0.9187 - accuracy: 0.5748 - val_loss: 1.0439 - val_accuracy: 0.4367\n",
      "Epoch 99/300\n",
      "1872/1872 [==============================] - 0s 216us/sample - loss: 0.9202 - accuracy: 0.5823 - val_loss: 1.0436 - val_accuracy: 0.4352\n",
      "Epoch 100/300\n",
      "1872/1872 [==============================] - 0s 203us/sample - loss: 0.9099 - accuracy: 0.5796 - val_loss: 1.0433 - val_accuracy: 0.4396\n",
      "Epoch 101/300\n",
      "1872/1872 [==============================] - 0s 207us/sample - loss: 0.9124 - accuracy: 0.5833 - val_loss: 1.0430 - val_accuracy: 0.4396\n",
      "Epoch 102/300\n",
      "1872/1872 [==============================] - 0s 201us/sample - loss: 0.9135 - accuracy: 0.5823 - val_loss: 1.0427 - val_accuracy: 0.4440\n",
      "Epoch 103/300\n",
      "1872/1872 [==============================] - 0s 202us/sample - loss: 0.9135 - accuracy: 0.5860 - val_loss: 1.0424 - val_accuracy: 0.4454\n",
      "Epoch 104/300\n",
      "1872/1872 [==============================] - 0s 195us/sample - loss: 0.9134 - accuracy: 0.5785 - val_loss: 1.0422 - val_accuracy: 0.4454\n",
      "Epoch 105/300\n",
      "1872/1872 [==============================] - 0s 204us/sample - loss: 0.9091 - accuracy: 0.6015 - val_loss: 1.0419 - val_accuracy: 0.4469\n",
      "Epoch 106/300\n",
      "1872/1872 [==============================] - 0s 203us/sample - loss: 0.9051 - accuracy: 0.5860 - val_loss: 1.0417 - val_accuracy: 0.4469\n",
      "Epoch 107/300\n",
      "1872/1872 [==============================] - 0s 192us/sample - loss: 0.9071 - accuracy: 0.5919 - val_loss: 1.0415 - val_accuracy: 0.4498\n",
      "Epoch 108/300\n",
      "1872/1872 [==============================] - 0s 192us/sample - loss: 0.9084 - accuracy: 0.5978 - val_loss: 1.0412 - val_accuracy: 0.4498\n",
      "Epoch 109/300\n",
      "1872/1872 [==============================] - 0s 203us/sample - loss: 0.9048 - accuracy: 0.5801 - val_loss: 1.0410 - val_accuracy: 0.4498\n",
      "Epoch 110/300\n",
      "1872/1872 [==============================] - 0s 205us/sample - loss: 0.9010 - accuracy: 0.5881 - val_loss: 1.0407 - val_accuracy: 0.4512\n",
      "Epoch 111/300\n",
      "1872/1872 [==============================] - 0s 215us/sample - loss: 0.8988 - accuracy: 0.5924 - val_loss: 1.0405 - val_accuracy: 0.4512\n",
      "Epoch 112/300\n",
      "1872/1872 [==============================] - 0s 194us/sample - loss: 0.8980 - accuracy: 0.5951 - val_loss: 1.0403 - val_accuracy: 0.4512\n",
      "Epoch 113/300\n",
      "1872/1872 [==============================] - 0s 190us/sample - loss: 0.8950 - accuracy: 0.6015 - val_loss: 1.0401 - val_accuracy: 0.4527\n",
      "Epoch 114/300\n",
      "1872/1872 [==============================] - 0s 204us/sample - loss: 0.8955 - accuracy: 0.5978 - val_loss: 1.0399 - val_accuracy: 0.4541\n",
      "Epoch 115/300\n",
      "1872/1872 [==============================] - 0s 225us/sample - loss: 0.8922 - accuracy: 0.6015 - val_loss: 1.0397 - val_accuracy: 0.4541\n",
      "Epoch 116/300\n",
      "1872/1872 [==============================] - 0s 208us/sample - loss: 0.8930 - accuracy: 0.5978 - val_loss: 1.0395 - val_accuracy: 0.4527\n",
      "Epoch 117/300\n",
      "1872/1872 [==============================] - 0s 205us/sample - loss: 0.8858 - accuracy: 0.6015 - val_loss: 1.0394 - val_accuracy: 0.4527\n",
      "Epoch 118/300\n",
      "1872/1872 [==============================] - 0s 215us/sample - loss: 0.8935 - accuracy: 0.6084 - val_loss: 1.0392 - val_accuracy: 0.4541\n",
      "Epoch 119/300\n",
      "1872/1872 [==============================] - 0s 205us/sample - loss: 0.8867 - accuracy: 0.6122 - val_loss: 1.0390 - val_accuracy: 0.4541\n",
      "Epoch 120/300\n",
      "1872/1872 [==============================] - 0s 209us/sample - loss: 0.8790 - accuracy: 0.6197 - val_loss: 1.0388 - val_accuracy: 0.4541\n",
      "Epoch 121/300\n",
      "1872/1872 [==============================] - 0s 205us/sample - loss: 0.8814 - accuracy: 0.6132 - val_loss: 1.0387 - val_accuracy: 0.4571\n",
      "Epoch 122/300\n",
      "1872/1872 [==============================] - 0s 200us/sample - loss: 0.8811 - accuracy: 0.6207 - val_loss: 1.0385 - val_accuracy: 0.4585\n",
      "Epoch 123/300\n",
      "1872/1872 [==============================] - 0s 205us/sample - loss: 0.8843 - accuracy: 0.6245 - val_loss: 1.0384 - val_accuracy: 0.4571\n",
      "Epoch 124/300\n",
      "1872/1872 [==============================] - 0s 207us/sample - loss: 0.8808 - accuracy: 0.6127 - val_loss: 1.0382 - val_accuracy: 0.4571\n",
      "Epoch 125/300\n",
      "1872/1872 [==============================] - 0s 209us/sample - loss: 0.8760 - accuracy: 0.6261 - val_loss: 1.0382 - val_accuracy: 0.4585\n",
      "Epoch 126/300\n",
      "1872/1872 [==============================] - 0s 210us/sample - loss: 0.8715 - accuracy: 0.6298 - val_loss: 1.0379 - val_accuracy: 0.4600\n",
      "Epoch 127/300\n",
      "1872/1872 [==============================] - 0s 194us/sample - loss: 0.8698 - accuracy: 0.6341 - val_loss: 1.0379 - val_accuracy: 0.4600\n",
      "Epoch 128/300\n",
      "1872/1872 [==============================] - 0s 213us/sample - loss: 0.8685 - accuracy: 0.6223 - val_loss: 1.0378 - val_accuracy: 0.4585\n",
      "Epoch 129/300\n",
      "1872/1872 [==============================] - 0s 222us/sample - loss: 0.8699 - accuracy: 0.6261 - val_loss: 1.0376 - val_accuracy: 0.4585\n",
      "Epoch 130/300\n",
      "1872/1872 [==============================] - 0s 221us/sample - loss: 0.8649 - accuracy: 0.6282 - val_loss: 1.0376 - val_accuracy: 0.4571\n",
      "Epoch 131/300\n",
      "1872/1872 [==============================] - 0s 215us/sample - loss: 0.8695 - accuracy: 0.6223 - val_loss: 1.0374 - val_accuracy: 0.4571\n",
      "Epoch 132/300\n",
      "1872/1872 [==============================] - 0s 190us/sample - loss: 0.8612 - accuracy: 0.6453 - val_loss: 1.0374 - val_accuracy: 0.4571\n",
      "Epoch 133/300\n",
      "1872/1872 [==============================] - 0s 209us/sample - loss: 0.8645 - accuracy: 0.6373 - val_loss: 1.0372 - val_accuracy: 0.4556\n",
      "Epoch 134/300\n",
      "1872/1872 [==============================] - 0s 213us/sample - loss: 0.8615 - accuracy: 0.6239 - val_loss: 1.0371 - val_accuracy: 0.4556\n",
      "Epoch 135/300\n",
      "1872/1872 [==============================] - 0s 199us/sample - loss: 0.8639 - accuracy: 0.6229 - val_loss: 1.0371 - val_accuracy: 0.4556\n",
      "Epoch 136/300\n",
      "1872/1872 [==============================] - 0s 195us/sample - loss: 0.8593 - accuracy: 0.6298 - val_loss: 1.0371 - val_accuracy: 0.4556\n",
      "Epoch 137/300\n",
      "1872/1872 [==============================] - 0s 198us/sample - loss: 0.8589 - accuracy: 0.6368 - val_loss: 1.0369 - val_accuracy: 0.4571\n",
      "Epoch 138/300\n",
      "1872/1872 [==============================] - 0s 192us/sample - loss: 0.8571 - accuracy: 0.6351 - val_loss: 1.0370 - val_accuracy: 0.4571\n",
      "Epoch 139/300\n",
      "1872/1872 [==============================] - 0s 196us/sample - loss: 0.8528 - accuracy: 0.6335 - val_loss: 1.0371 - val_accuracy: 0.4571\n",
      "Epoch 140/300\n",
      "1872/1872 [==============================] - 0s 172us/sample - loss: 0.8490 - accuracy: 0.6351 - val_loss: 1.0370 - val_accuracy: 0.4571\n",
      "Epoch 00140: early stopping\n",
      "197/197 [==============================] - 0s 131us/sample - loss: 0.9461 - accuracy: 0.5888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [1:03:32, 762.44s/it]\n",
      "0it [00:00, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:06<00:12,  6.17s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:12<00:06,  6.18s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.08s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 618.1015625 steps, validate for 180.4453125 steps\n",
      "Epoch 1/300\n",
      "619/618 [==============================] - 18s 30ms/step - loss: 1.0752 - accuracy: 0.4577 - val_loss: 1.0933 - val_accuracy: 0.4395\n",
      "Epoch 2/300\n",
      "619/618 [==============================] - 17s 28ms/step - loss: 0.9921 - accuracy: 0.5119 - val_loss: 1.0845 - val_accuracy: 0.4439\n",
      "Epoch 3/300\n",
      "619/618 [==============================] - 17s 28ms/step - loss: 0.9678 - accuracy: 0.5299 - val_loss: 1.0766 - val_accuracy: 0.4482\n",
      "Epoch 4/300\n",
      "619/618 [==============================] - 17s 28ms/step - loss: 0.9508 - accuracy: 0.5422 - val_loss: 1.0690 - val_accuracy: 0.4522\n",
      "Epoch 5/300\n",
      "619/618 [==============================] - 17s 28ms/step - loss: 0.9369 - accuracy: 0.5530 - val_loss: 1.0734 - val_accuracy: 0.4531\n",
      "Epoch 6/300\n",
      "619/618 [==============================] - 17s 28ms/step - loss: 0.9244 - accuracy: 0.5615 - val_loss: 1.0720 - val_accuracy: 0.4540\n",
      "Epoch 7/300\n",
      "619/618 [==============================] - 17s 28ms/step - loss: 0.9138 - accuracy: 0.5685 - val_loss: 1.0753 - val_accuracy: 0.4522\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 618.1015625 steps, validate for 180.4453125 steps\n",
      "Epoch 1/300\n",
      "619/618 [==============================] - 41s 66ms/step - loss: 0.8944 - accuracy: 0.5806 - val_loss: 1.1168 - val_accuracy: 0.4158\n",
      "Epoch 2/300\n",
      "619/618 [==============================] - 40s 64ms/step - loss: 0.8644 - accuracy: 0.6025 - val_loss: 1.0788 - val_accuracy: 0.4604\n",
      "Epoch 3/300\n",
      "619/618 [==============================] - 40s 64ms/step - loss: 0.8412 - accuracy: 0.6186 - val_loss: 1.1270 - val_accuracy: 0.4340\n",
      "Epoch 4/300\n",
      "619/618 [==============================] - 40s 64ms/step - loss: 0.8206 - accuracy: 0.6309 - val_loss: 1.0984 - val_accuracy: 0.4662\n",
      "Epoch 5/300\n",
      "619/618 [==============================] - 40s 64ms/step - loss: 0.8024 - accuracy: 0.6441 - val_loss: 1.0717 - val_accuracy: 0.4834\n",
      "Epoch 6/300\n",
      "619/618 [==============================] - 40s 64ms/step - loss: 0.7849 - accuracy: 0.6556 - val_loss: 1.1038 - val_accuracy: 0.4667\n",
      "Epoch 7/300\n",
      "619/618 [==============================] - 40s 64ms/step - loss: 0.7685 - accuracy: 0.6653 - val_loss: 1.1382 - val_accuracy: 0.4354\n",
      "Epoch 8/300\n",
      "619/618 [==============================] - 40s 64ms/step - loss: 0.7524 - accuracy: 0.6742 - val_loss: 1.1218 - val_accuracy: 0.4619\n",
      "Epoch 00008: early stopping\n",
      "380/380 [==============================] - 0s 790us/sample - loss: 1.5943 - accuracy: 0.2263\n",
      "369/369 [==============================] - 0s 217us/sample - loss: 1.5838 - accuracy: 0.2249\n",
      "364/364 [==============================] - 0s 220us/sample - loss: 1.5417 - accuracy: 0.2527\n",
      "360/360 [==============================] - 0s 228us/sample - loss: 1.5804 - accuracy: 0.2306\n",
      "355/355 [==============================] - 0s 221us/sample - loss: 1.5577 - accuracy: 0.2028\n",
      "346/346 [==============================] - 0s 230us/sample - loss: 1.5574 - accuracy: 0.2630\n",
      "343/343 [==============================] - 0s 224us/sample - loss: 1.5643 - accuracy: 0.2274\n",
      "336/336 [==============================] - 0s 249us/sample - loss: 1.5575 - accuracy: 0.2470\n",
      "332/332 [==============================] - 0s 236us/sample - loss: 1.5230 - accuracy: 0.2500\n",
      "328/328 [==============================] - 0s 239us/sample - loss: 1.5971 - accuracy: 0.2195\n",
      "323/323 [==============================] - 0s 242us/sample - loss: 1.5813 - accuracy: 0.2384\n",
      "322/322 [==============================] - 0s 613us/sample - loss: 1.6628 - accuracy: 0.1925\n",
      "313/313 [==============================] - 0s 269us/sample - loss: 1.5402 - accuracy: 0.2428\n",
      "312/312 [==============================] - 0s 255us/sample - loss: 1.6185 - accuracy: 0.2340\n",
      "310/310 [==============================] - 0s 237us/sample - loss: 1.6634 - accuracy: 0.2097\n",
      "308/308 [==============================] - 0s 230us/sample - loss: 1.6685 - accuracy: 0.2045\n",
      "305/305 [==============================] - 0s 238us/sample - loss: 1.6163 - accuracy: 0.2131\n",
      "301/301 [==============================] - 0s 231us/sample - loss: 1.6257 - accuracy: 0.2126\n",
      "299/299 [==============================] - 0s 239us/sample - loss: 1.6080 - accuracy: 0.2642\n",
      "298/298 [==============================] - 0s 231us/sample - loss: 1.6622 - accuracy: 0.2181\n",
      "293/293 [==============================] - 0s 252us/sample - loss: 1.5223 - accuracy: 0.2628\n",
      "291/291 [==============================] - 0s 267us/sample - loss: 1.5990 - accuracy: 0.2302\n",
      "291/291 [==============================] - 0s 266us/sample - loss: 1.5440 - accuracy: 0.2371\n",
      "291/291 [==============================] - 0s 266us/sample - loss: 1.5726 - accuracy: 0.2646\n",
      "292/292 [==============================] - 0s 249us/sample - loss: 1.6036 - accuracy: 0.2671\n",
      "287/287 [==============================] - 0s 235us/sample - loss: 1.6450 - accuracy: 0.2334\n",
      "286/286 [==============================] - 0s 237us/sample - loss: 1.5569 - accuracy: 0.2343\n",
      "286/286 [==============================] - 0s 249us/sample - loss: 1.5534 - accuracy: 0.2552\n",
      "279/279 [==============================] - 0s 251us/sample - loss: 1.4700 - accuracy: 0.2545\n",
      "277/277 [==============================] - 0s 236us/sample - loss: 1.6028 - accuracy: 0.2527\n",
      "277/277 [==============================] - 0s 291us/sample - loss: 1.5311 - accuracy: 0.2780\n",
      "275/275 [==============================] - 0s 286us/sample - loss: 1.6948 - accuracy: 0.1927\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 1955 samples, validate on 554 samples\n",
      "Epoch 1/300\n",
      "1955/1955 [==============================] - 2s 1ms/sample - loss: 1.1925 - accuracy: 0.3463 - val_loss: 1.1782 - val_accuracy: 0.3213\n",
      "Epoch 2/300\n",
      "1955/1955 [==============================] - 0s 196us/sample - loss: 1.1514 - accuracy: 0.3801 - val_loss: 1.1604 - val_accuracy: 0.3321\n",
      "Epoch 3/300\n",
      "1955/1955 [==============================] - 0s 187us/sample - loss: 1.1358 - accuracy: 0.3954 - val_loss: 1.1477 - val_accuracy: 0.3339\n",
      "Epoch 4/300\n",
      "1955/1955 [==============================] - 0s 177us/sample - loss: 1.1187 - accuracy: 0.3934 - val_loss: 1.1378 - val_accuracy: 0.3484\n",
      "Epoch 5/300\n",
      "1955/1955 [==============================] - 0s 195us/sample - loss: 1.1135 - accuracy: 0.4010 - val_loss: 1.1298 - val_accuracy: 0.3538\n",
      "Epoch 6/300\n",
      "1955/1955 [==============================] - 0s 188us/sample - loss: 1.0939 - accuracy: 0.4118 - val_loss: 1.1227 - val_accuracy: 0.3718\n",
      "Epoch 7/300\n",
      "1955/1955 [==============================] - 0s 191us/sample - loss: 1.0857 - accuracy: 0.4215 - val_loss: 1.1170 - val_accuracy: 0.3791\n",
      "Epoch 8/300\n",
      "1955/1955 [==============================] - 0s 206us/sample - loss: 1.0808 - accuracy: 0.4276 - val_loss: 1.1119 - val_accuracy: 0.3881\n",
      "Epoch 9/300\n",
      "1955/1955 [==============================] - 0s 214us/sample - loss: 1.0741 - accuracy: 0.4312 - val_loss: 1.1072 - val_accuracy: 0.4007\n",
      "Epoch 10/300\n",
      "1955/1955 [==============================] - 0s 197us/sample - loss: 1.0632 - accuracy: 0.4547 - val_loss: 1.1029 - val_accuracy: 0.4079\n",
      "Epoch 11/300\n",
      "1955/1955 [==============================] - 0s 203us/sample - loss: 1.0609 - accuracy: 0.4414 - val_loss: 1.0994 - val_accuracy: 0.4152\n",
      "Epoch 12/300\n",
      "1955/1955 [==============================] - 0s 197us/sample - loss: 1.0604 - accuracy: 0.4532 - val_loss: 1.0959 - val_accuracy: 0.4188\n",
      "Epoch 13/300\n",
      "1955/1955 [==============================] - 0s 196us/sample - loss: 1.0471 - accuracy: 0.4578 - val_loss: 1.0925 - val_accuracy: 0.4224\n",
      "Epoch 14/300\n",
      "1955/1955 [==============================] - 0s 180us/sample - loss: 1.0393 - accuracy: 0.4747 - val_loss: 1.0892 - val_accuracy: 0.4224\n",
      "Epoch 15/300\n",
      "1955/1955 [==============================] - 0s 195us/sample - loss: 1.0403 - accuracy: 0.4706 - val_loss: 1.0863 - val_accuracy: 0.4314\n",
      "Epoch 16/300\n",
      "1955/1955 [==============================] - 0s 193us/sample - loss: 1.0287 - accuracy: 0.4752 - val_loss: 1.0836 - val_accuracy: 0.4314\n",
      "Epoch 17/300\n",
      "1955/1955 [==============================] - 0s 183us/sample - loss: 1.0295 - accuracy: 0.4675 - val_loss: 1.0811 - val_accuracy: 0.4368\n",
      "Epoch 18/300\n",
      "1955/1955 [==============================] - 0s 189us/sample - loss: 1.0183 - accuracy: 0.4829 - val_loss: 1.0786 - val_accuracy: 0.4404\n",
      "Epoch 19/300\n",
      "1955/1955 [==============================] - 0s 192us/sample - loss: 1.0233 - accuracy: 0.4824 - val_loss: 1.0764 - val_accuracy: 0.4477\n",
      "Epoch 20/300\n",
      "1955/1955 [==============================] - 0s 189us/sample - loss: 1.0150 - accuracy: 0.4926 - val_loss: 1.0742 - val_accuracy: 0.4477\n",
      "Epoch 21/300\n",
      "1955/1955 [==============================] - 0s 198us/sample - loss: 1.0104 - accuracy: 0.4905 - val_loss: 1.0720 - val_accuracy: 0.4495\n",
      "Epoch 22/300\n",
      "1955/1955 [==============================] - 0s 187us/sample - loss: 1.0028 - accuracy: 0.5013 - val_loss: 1.0701 - val_accuracy: 0.4585\n",
      "Epoch 23/300\n",
      "1955/1955 [==============================] - 0s 202us/sample - loss: 0.9989 - accuracy: 0.4997 - val_loss: 1.0682 - val_accuracy: 0.4549\n",
      "Epoch 24/300\n",
      "1955/1955 [==============================] - 0s 188us/sample - loss: 0.9992 - accuracy: 0.4977 - val_loss: 1.0666 - val_accuracy: 0.4549\n",
      "Epoch 25/300\n",
      "1955/1955 [==============================] - 0s 204us/sample - loss: 1.0013 - accuracy: 0.4864 - val_loss: 1.0648 - val_accuracy: 0.4513\n",
      "Epoch 26/300\n",
      "1955/1955 [==============================] - 0s 207us/sample - loss: 0.9874 - accuracy: 0.5090 - val_loss: 1.0630 - val_accuracy: 0.4513\n",
      "Epoch 27/300\n",
      "1955/1955 [==============================] - 0s 198us/sample - loss: 0.9853 - accuracy: 0.5223 - val_loss: 1.0617 - val_accuracy: 0.4477\n",
      "Epoch 28/300\n",
      "1955/1955 [==============================] - 0s 176us/sample - loss: 0.9881 - accuracy: 0.5207 - val_loss: 1.0603 - val_accuracy: 0.4477\n",
      "Epoch 29/300\n",
      "1955/1955 [==============================] - 0s 193us/sample - loss: 0.9797 - accuracy: 0.5243 - val_loss: 1.0587 - val_accuracy: 0.4531\n",
      "Epoch 30/300\n",
      "1955/1955 [==============================] - 0s 191us/sample - loss: 0.9762 - accuracy: 0.5330 - val_loss: 1.0575 - val_accuracy: 0.4603\n",
      "Epoch 31/300\n",
      "1955/1955 [==============================] - 0s 210us/sample - loss: 0.9674 - accuracy: 0.5335 - val_loss: 1.0563 - val_accuracy: 0.4603\n",
      "Epoch 32/300\n",
      "1955/1955 [==============================] - 0s 190us/sample - loss: 0.9710 - accuracy: 0.5304 - val_loss: 1.0550 - val_accuracy: 0.4621\n",
      "Epoch 33/300\n",
      "1955/1955 [==============================] - 0s 180us/sample - loss: 0.9690 - accuracy: 0.5146 - val_loss: 1.0539 - val_accuracy: 0.4621\n",
      "Epoch 34/300\n",
      "1955/1955 [==============================] - 0s 197us/sample - loss: 0.9615 - accuracy: 0.5355 - val_loss: 1.0527 - val_accuracy: 0.4621\n",
      "Epoch 35/300\n",
      "1955/1955 [==============================] - 0s 205us/sample - loss: 0.9685 - accuracy: 0.5458 - val_loss: 1.0516 - val_accuracy: 0.4675\n",
      "Epoch 36/300\n",
      "1955/1955 [==============================] - 0s 202us/sample - loss: 0.9587 - accuracy: 0.5468 - val_loss: 1.0506 - val_accuracy: 0.4657\n",
      "Epoch 37/300\n",
      "1955/1955 [==============================] - 0s 192us/sample - loss: 0.9584 - accuracy: 0.5402 - val_loss: 1.0498 - val_accuracy: 0.4711\n",
      "Epoch 38/300\n",
      "1955/1955 [==============================] - 0s 179us/sample - loss: 0.9621 - accuracy: 0.5509 - val_loss: 1.0488 - val_accuracy: 0.4783\n",
      "Epoch 39/300\n",
      "1955/1955 [==============================] - 0s 200us/sample - loss: 0.9538 - accuracy: 0.5504 - val_loss: 1.0479 - val_accuracy: 0.4765\n",
      "Epoch 40/300\n",
      "1955/1955 [==============================] - 0s 189us/sample - loss: 0.9458 - accuracy: 0.5524 - val_loss: 1.0472 - val_accuracy: 0.4765\n",
      "Epoch 41/300\n",
      "1955/1955 [==============================] - 0s 205us/sample - loss: 0.9529 - accuracy: 0.5499 - val_loss: 1.0464 - val_accuracy: 0.4711\n",
      "Epoch 42/300\n",
      "1955/1955 [==============================] - 0s 196us/sample - loss: 0.9501 - accuracy: 0.5535 - val_loss: 1.0455 - val_accuracy: 0.4747\n",
      "Epoch 43/300\n",
      "1955/1955 [==============================] - 0s 188us/sample - loss: 0.9405 - accuracy: 0.5586 - val_loss: 1.0447 - val_accuracy: 0.4729\n",
      "Epoch 44/300\n",
      "1955/1955 [==============================] - 0s 190us/sample - loss: 0.9404 - accuracy: 0.5570 - val_loss: 1.0442 - val_accuracy: 0.4729\n",
      "Epoch 45/300\n",
      "1955/1955 [==============================] - 0s 187us/sample - loss: 0.9300 - accuracy: 0.5662 - val_loss: 1.0435 - val_accuracy: 0.4729\n",
      "Epoch 46/300\n",
      "1955/1955 [==============================] - 0s 195us/sample - loss: 0.9377 - accuracy: 0.5596 - val_loss: 1.0428 - val_accuracy: 0.4711\n",
      "Epoch 47/300\n",
      "1955/1955 [==============================] - 0s 187us/sample - loss: 0.9276 - accuracy: 0.5668 - val_loss: 1.0422 - val_accuracy: 0.4729\n",
      "Epoch 48/300\n",
      "1955/1955 [==============================] - 0s 205us/sample - loss: 0.9239 - accuracy: 0.5698 - val_loss: 1.0416 - val_accuracy: 0.4729\n",
      "Epoch 49/300\n",
      "1955/1955 [==============================] - 0s 198us/sample - loss: 0.9267 - accuracy: 0.5770 - val_loss: 1.0410 - val_accuracy: 0.4747\n",
      "Epoch 50/300\n",
      "1955/1955 [==============================] - 0s 194us/sample - loss: 0.9193 - accuracy: 0.5760 - val_loss: 1.0406 - val_accuracy: 0.4747\n",
      "Epoch 51/300\n",
      "1955/1955 [==============================] - 0s 207us/sample - loss: 0.9238 - accuracy: 0.5749 - val_loss: 1.0400 - val_accuracy: 0.4747\n",
      "Epoch 52/300\n",
      "1955/1955 [==============================] - 0s 194us/sample - loss: 0.9189 - accuracy: 0.5862 - val_loss: 1.0395 - val_accuracy: 0.4765\n",
      "Epoch 53/300\n",
      "1955/1955 [==============================] - 0s 200us/sample - loss: 0.9193 - accuracy: 0.5811 - val_loss: 1.0390 - val_accuracy: 0.4747\n",
      "Epoch 54/300\n",
      "1955/1955 [==============================] - 0s 197us/sample - loss: 0.9074 - accuracy: 0.5867 - val_loss: 1.0387 - val_accuracy: 0.4747\n",
      "Epoch 55/300\n",
      "1955/1955 [==============================] - 0s 200us/sample - loss: 0.9132 - accuracy: 0.5852 - val_loss: 1.0383 - val_accuracy: 0.4729\n",
      "Epoch 56/300\n",
      "1955/1955 [==============================] - 0s 196us/sample - loss: 0.9082 - accuracy: 0.5816 - val_loss: 1.0380 - val_accuracy: 0.4747\n",
      "Epoch 57/300\n",
      "1955/1955 [==============================] - 0s 196us/sample - loss: 0.8988 - accuracy: 0.5821 - val_loss: 1.0376 - val_accuracy: 0.4747\n",
      "Epoch 58/300\n",
      "1955/1955 [==============================] - 0s 208us/sample - loss: 0.9070 - accuracy: 0.5949 - val_loss: 1.0373 - val_accuracy: 0.4711\n",
      "Epoch 59/300\n",
      "1955/1955 [==============================] - 0s 189us/sample - loss: 0.9015 - accuracy: 0.6061 - val_loss: 1.0371 - val_accuracy: 0.4693\n",
      "Epoch 60/300\n",
      "1955/1955 [==============================] - 0s 184us/sample - loss: 0.8988 - accuracy: 0.5908 - val_loss: 1.0368 - val_accuracy: 0.4693\n",
      "Epoch 61/300\n",
      "1955/1955 [==============================] - 0s 188us/sample - loss: 0.8944 - accuracy: 0.6000 - val_loss: 1.0365 - val_accuracy: 0.4693\n",
      "Epoch 62/300\n",
      "1955/1955 [==============================] - 0s 192us/sample - loss: 0.8973 - accuracy: 0.5939 - val_loss: 1.0364 - val_accuracy: 0.4711\n",
      "Epoch 63/300\n",
      "1955/1955 [==============================] - 0s 188us/sample - loss: 0.8907 - accuracy: 0.5954 - val_loss: 1.0362 - val_accuracy: 0.4657\n",
      "Epoch 64/300\n",
      "1955/1955 [==============================] - 0s 187us/sample - loss: 0.8896 - accuracy: 0.6051 - val_loss: 1.0360 - val_accuracy: 0.4711\n",
      "Epoch 65/300\n",
      "1955/1955 [==============================] - 0s 193us/sample - loss: 0.8866 - accuracy: 0.6087 - val_loss: 1.0357 - val_accuracy: 0.4693\n",
      "Epoch 66/300\n",
      "1955/1955 [==============================] - 0s 180us/sample - loss: 0.8910 - accuracy: 0.5990 - val_loss: 1.0355 - val_accuracy: 0.4729\n",
      "Epoch 67/300\n",
      "1955/1955 [==============================] - 0s 211us/sample - loss: 0.8824 - accuracy: 0.6056 - val_loss: 1.0356 - val_accuracy: 0.4747\n",
      "Epoch 68/300\n",
      "1955/1955 [==============================] - 0s 199us/sample - loss: 0.8803 - accuracy: 0.6077 - val_loss: 1.0353 - val_accuracy: 0.4747\n",
      "Epoch 69/300\n",
      "1955/1955 [==============================] - 0s 206us/sample - loss: 0.8776 - accuracy: 0.6056 - val_loss: 1.0352 - val_accuracy: 0.4729\n",
      "Epoch 70/300\n",
      "1955/1955 [==============================] - 0s 189us/sample - loss: 0.8737 - accuracy: 0.6153 - val_loss: 1.0352 - val_accuracy: 0.4729\n",
      "Epoch 71/300\n",
      "1955/1955 [==============================] - 0s 197us/sample - loss: 0.8762 - accuracy: 0.6123 - val_loss: 1.0352 - val_accuracy: 0.4729\n",
      "Epoch 72/300\n",
      "1955/1955 [==============================] - 0s 201us/sample - loss: 0.8710 - accuracy: 0.6118 - val_loss: 1.0350 - val_accuracy: 0.4747\n",
      "Epoch 73/300\n",
      "1955/1955 [==============================] - 0s 190us/sample - loss: 0.8584 - accuracy: 0.6297 - val_loss: 1.0351 - val_accuracy: 0.4747\n",
      "Epoch 74/300\n",
      "1955/1955 [==============================] - 0s 189us/sample - loss: 0.8646 - accuracy: 0.6184 - val_loss: 1.0350 - val_accuracy: 0.4711\n",
      "Epoch 75/300\n",
      "1955/1955 [==============================] - 0s 183us/sample - loss: 0.8644 - accuracy: 0.6184 - val_loss: 1.0349 - val_accuracy: 0.4747\n",
      "Epoch 76/300\n",
      "1955/1955 [==============================] - 0s 200us/sample - loss: 0.8667 - accuracy: 0.6189 - val_loss: 1.0349 - val_accuracy: 0.4765\n",
      "Epoch 77/300\n",
      "1955/1955 [==============================] - 0s 182us/sample - loss: 0.8631 - accuracy: 0.6174 - val_loss: 1.0348 - val_accuracy: 0.4783\n",
      "Epoch 78/300\n",
      "1955/1955 [==============================] - 0s 180us/sample - loss: 0.8576 - accuracy: 0.6230 - val_loss: 1.0347 - val_accuracy: 0.4801\n",
      "Epoch 79/300\n",
      "1955/1955 [==============================] - 0s 187us/sample - loss: 0.8563 - accuracy: 0.6246 - val_loss: 1.0347 - val_accuracy: 0.4783\n",
      "Epoch 80/300\n",
      "1955/1955 [==============================] - 0s 189us/sample - loss: 0.8486 - accuracy: 0.6266 - val_loss: 1.0349 - val_accuracy: 0.4783\n",
      "Epoch 81/300\n",
      "1955/1955 [==============================] - 0s 189us/sample - loss: 0.8474 - accuracy: 0.6363 - val_loss: 1.0349 - val_accuracy: 0.4783\n",
      "Epoch 82/300\n",
      "1955/1955 [==============================] - 0s 196us/sample - loss: 0.8469 - accuracy: 0.6343 - val_loss: 1.0350 - val_accuracy: 0.4765\n",
      "Epoch 00082: early stopping\n",
      "247/247 [==============================] - 0s 89us/sample - loss: 1.4119 - accuracy: 0.2146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [14:14, 854.98s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:06<00:12,  6.39s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:12<00:06,  6.32s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.26s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 633.7890625 steps, validate for 175.703125 steps\n",
      "Epoch 1/300\n",
      "634/633 [==============================] - 19s 30ms/step - loss: 1.1559 - accuracy: 0.4328 - val_loss: 1.1414 - val_accuracy: 0.4074\n",
      "Epoch 2/300\n",
      "634/633 [==============================] - 18s 28ms/step - loss: 1.0208 - accuracy: 0.4862 - val_loss: 1.1173 - val_accuracy: 0.4237\n",
      "Epoch 3/300\n",
      "634/633 [==============================] - 18s 28ms/step - loss: 0.9901 - accuracy: 0.5091 - val_loss: 1.0992 - val_accuracy: 0.4247\n",
      "Epoch 4/300\n",
      "634/633 [==============================] - 18s 28ms/step - loss: 0.9703 - accuracy: 0.5246 - val_loss: 1.1007 - val_accuracy: 0.4253\n",
      "Epoch 5/300\n",
      "634/633 [==============================] - 18s 28ms/step - loss: 0.9557 - accuracy: 0.5360 - val_loss: 1.0933 - val_accuracy: 0.4257\n",
      "Epoch 6/300\n",
      "634/633 [==============================] - 18s 28ms/step - loss: 0.9438 - accuracy: 0.5457 - val_loss: 1.1008 - val_accuracy: 0.4300\n",
      "Epoch 7/300\n",
      "634/633 [==============================] - 18s 28ms/step - loss: 0.9336 - accuracy: 0.5526 - val_loss: 1.0974 - val_accuracy: 0.4281\n",
      "Epoch 8/300\n",
      "634/633 [==============================] - 18s 28ms/step - loss: 0.9247 - accuracy: 0.5601 - val_loss: 1.1016 - val_accuracy: 0.4296\n",
      "Epoch 00008: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 633.7890625 steps, validate for 175.703125 steps\n",
      "Epoch 1/300\n",
      "634/633 [==============================] - 42s 66ms/step - loss: 0.9073 - accuracy: 0.5717 - val_loss: 1.1288 - val_accuracy: 0.3888\n",
      "Epoch 2/300\n",
      "634/633 [==============================] - 41s 64ms/step - loss: 0.8795 - accuracy: 0.5928 - val_loss: 1.1313 - val_accuracy: 0.4126\n",
      "Epoch 3/300\n",
      "634/633 [==============================] - 40s 64ms/step - loss: 0.8570 - accuracy: 0.6081 - val_loss: 1.1266 - val_accuracy: 0.4209\n",
      "Epoch 4/300\n",
      "634/633 [==============================] - 41s 64ms/step - loss: 0.8373 - accuracy: 0.6208 - val_loss: 1.1175 - val_accuracy: 0.4437\n",
      "Epoch 5/300\n",
      "634/633 [==============================] - 40s 64ms/step - loss: 0.8184 - accuracy: 0.6331 - val_loss: 1.1553 - val_accuracy: 0.4150\n",
      "Epoch 6/300\n",
      "634/633 [==============================] - 40s 64ms/step - loss: 0.8008 - accuracy: 0.6447 - val_loss: 1.1725 - val_accuracy: 0.4452\n",
      "Epoch 7/300\n",
      "634/633 [==============================] - 41s 64ms/step - loss: 0.7838 - accuracy: 0.6558 - val_loss: 1.1540 - val_accuracy: 0.4307\n",
      "Epoch 00007: early stopping\n",
      "414/414 [==============================] - 0s 698us/sample - loss: 0.8886 - accuracy: 0.5121\n",
      "392/392 [==============================] - 0s 212us/sample - loss: 0.9030 - accuracy: 0.5204\n",
      "382/382 [==============================] - 0s 226us/sample - loss: 0.8717 - accuracy: 0.5419\n",
      "356/356 [==============================] - 0s 237us/sample - loss: 0.9055 - accuracy: 0.5393\n",
      "330/330 [==============================] - 0s 261us/sample - loss: 0.8802 - accuracy: 0.5545\n",
      "317/317 [==============================] - 0s 250us/sample - loss: 0.9075 - accuracy: 0.5047\n",
      "311/311 [==============================] - 0s 288us/sample - loss: 0.9606 - accuracy: 0.4662\n",
      "309/309 [==============================] - 0s 284us/sample - loss: 0.9635 - accuracy: 0.4725\n",
      "294/294 [==============================] - 0s 234us/sample - loss: 0.9585 - accuracy: 0.5034\n",
      "289/289 [==============================] - 0s 268us/sample - loss: 1.0194 - accuracy: 0.4567\n",
      "269/269 [==============================] - 0s 260us/sample - loss: 0.9934 - accuracy: 0.4796\n",
      "262/262 [==============================] - 0s 244us/sample - loss: 1.0420 - accuracy: 0.4504\n",
      "254/254 [==============================] - 0s 220us/sample - loss: 1.0414 - accuracy: 0.4370\n",
      "253/253 [==============================] - 0s 247us/sample - loss: 1.0536 - accuracy: 0.4269\n",
      "252/252 [==============================] - 0s 241us/sample - loss: 1.0848 - accuracy: 0.4325\n",
      "249/249 [==============================] - 0s 241us/sample - loss: 1.0708 - accuracy: 0.4177\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 1.0836 - accuracy: 0.3984\n",
      "244/244 [==============================] - 0s 232us/sample - loss: 1.1000 - accuracy: 0.4426\n",
      "243/243 [==============================] - 0s 241us/sample - loss: 1.1167 - accuracy: 0.3992\n",
      "241/241 [==============================] - 0s 265us/sample - loss: 1.1537 - accuracy: 0.3527\n",
      "239/239 [==============================] - 0s 249us/sample - loss: 1.1892 - accuracy: 0.3431\n",
      "236/236 [==============================] - 0s 241us/sample - loss: 1.1749 - accuracy: 0.3771\n",
      "235/235 [==============================] - 0s 244us/sample - loss: 1.1092 - accuracy: 0.3915\n",
      "229/229 [==============================] - 0s 249us/sample - loss: 1.0990 - accuracy: 0.3974\n",
      "229/229 [==============================] - 0s 251us/sample - loss: 1.1599 - accuracy: 0.3755\n",
      "225/225 [==============================] - 0s 297us/sample - loss: 1.1461 - accuracy: 0.4000\n",
      "223/223 [==============================] - 0s 258us/sample - loss: 1.1122 - accuracy: 0.3901\n",
      "222/222 [==============================] - 0s 223us/sample - loss: 1.1007 - accuracy: 0.4279\n",
      "222/222 [==============================] - 0s 249us/sample - loss: 1.1263 - accuracy: 0.3964\n",
      "223/223 [==============================] - 0s 245us/sample - loss: 1.0898 - accuracy: 0.4126\n",
      "222/222 [==============================] - 0s 262us/sample - loss: 1.1168 - accuracy: 0.4189\n",
      "216/216 [==============================] - 0s 247us/sample - loss: 1.1411 - accuracy: 0.3796\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 2022 samples, validate on 540 samples\n",
      "Epoch 1/300\n",
      "2022/2022 [==============================] - 2s 1ms/sample - loss: 1.1239 - accuracy: 0.3808 - val_loss: 1.1753 - val_accuracy: 0.2944\n",
      "Epoch 2/300\n",
      "2022/2022 [==============================] - 0s 187us/sample - loss: 1.0965 - accuracy: 0.4001 - val_loss: 1.1607 - val_accuracy: 0.2926\n",
      "Epoch 3/300\n",
      "2022/2022 [==============================] - 0s 192us/sample - loss: 1.0832 - accuracy: 0.4006 - val_loss: 1.1505 - val_accuracy: 0.2944\n",
      "Epoch 4/300\n",
      "2022/2022 [==============================] - 0s 197us/sample - loss: 1.0768 - accuracy: 0.4135 - val_loss: 1.1429 - val_accuracy: 0.3148\n",
      "Epoch 5/300\n",
      "2022/2022 [==============================] - 0s 184us/sample - loss: 1.0612 - accuracy: 0.4327 - val_loss: 1.1368 - val_accuracy: 0.3278\n",
      "Epoch 6/300\n",
      "2022/2022 [==============================] - 0s 188us/sample - loss: 1.0530 - accuracy: 0.4451 - val_loss: 1.1316 - val_accuracy: 0.3426\n",
      "Epoch 7/300\n",
      "2022/2022 [==============================] - 0s 180us/sample - loss: 1.0512 - accuracy: 0.4377 - val_loss: 1.1272 - val_accuracy: 0.3537\n",
      "Epoch 8/300\n",
      "2022/2022 [==============================] - 0s 199us/sample - loss: 1.0496 - accuracy: 0.4372 - val_loss: 1.1234 - val_accuracy: 0.3481\n",
      "Epoch 9/300\n",
      "2022/2022 [==============================] - 0s 192us/sample - loss: 1.0361 - accuracy: 0.4644 - val_loss: 1.1199 - val_accuracy: 0.3481\n",
      "Epoch 10/300\n",
      "2022/2022 [==============================] - 0s 181us/sample - loss: 1.0310 - accuracy: 0.4664 - val_loss: 1.1166 - val_accuracy: 0.3481\n",
      "Epoch 11/300\n",
      "2022/2022 [==============================] - 0s 184us/sample - loss: 1.0347 - accuracy: 0.4555 - val_loss: 1.1137 - val_accuracy: 0.3611\n",
      "Epoch 12/300\n",
      "2022/2022 [==============================] - 0s 174us/sample - loss: 1.0254 - accuracy: 0.4812 - val_loss: 1.1113 - val_accuracy: 0.3630\n",
      "Epoch 13/300\n",
      "2022/2022 [==============================] - 0s 175us/sample - loss: 1.0182 - accuracy: 0.4876 - val_loss: 1.1088 - val_accuracy: 0.3704\n",
      "Epoch 14/300\n",
      "2022/2022 [==============================] - 0s 189us/sample - loss: 1.0140 - accuracy: 0.4946 - val_loss: 1.1066 - val_accuracy: 0.3741\n",
      "Epoch 15/300\n",
      "2022/2022 [==============================] - 0s 190us/sample - loss: 1.0140 - accuracy: 0.4817 - val_loss: 1.1044 - val_accuracy: 0.3741\n",
      "Epoch 16/300\n",
      "2022/2022 [==============================] - 0s 197us/sample - loss: 1.0116 - accuracy: 0.4807 - val_loss: 1.1025 - val_accuracy: 0.3759\n",
      "Epoch 17/300\n",
      "2022/2022 [==============================] - 0s 183us/sample - loss: 1.0014 - accuracy: 0.4965 - val_loss: 1.1010 - val_accuracy: 0.3778\n",
      "Epoch 18/300\n",
      "2022/2022 [==============================] - 0s 176us/sample - loss: 0.9986 - accuracy: 0.5040 - val_loss: 1.0994 - val_accuracy: 0.3815\n",
      "Epoch 19/300\n",
      "2022/2022 [==============================] - 0s 181us/sample - loss: 0.9978 - accuracy: 0.4990 - val_loss: 1.0979 - val_accuracy: 0.3833\n",
      "Epoch 20/300\n",
      "2022/2022 [==============================] - 0s 202us/sample - loss: 0.9980 - accuracy: 0.4941 - val_loss: 1.0965 - val_accuracy: 0.3870\n",
      "Epoch 21/300\n",
      "2022/2022 [==============================] - 0s 180us/sample - loss: 0.9875 - accuracy: 0.5134 - val_loss: 1.0952 - val_accuracy: 0.3889\n",
      "Epoch 22/300\n",
      "2022/2022 [==============================] - 0s 183us/sample - loss: 0.9907 - accuracy: 0.5138 - val_loss: 1.0940 - val_accuracy: 0.3944\n",
      "Epoch 23/300\n",
      "2022/2022 [==============================] - 0s 215us/sample - loss: 0.9871 - accuracy: 0.5153 - val_loss: 1.0930 - val_accuracy: 0.3963\n",
      "Epoch 24/300\n",
      "2022/2022 [==============================] - 0s 207us/sample - loss: 0.9839 - accuracy: 0.5069 - val_loss: 1.0922 - val_accuracy: 0.4019\n",
      "Epoch 25/300\n",
      "2022/2022 [==============================] - 0s 192us/sample - loss: 0.9833 - accuracy: 0.5227 - val_loss: 1.0913 - val_accuracy: 0.4037\n",
      "Epoch 26/300\n",
      "2022/2022 [==============================] - 0s 183us/sample - loss: 0.9738 - accuracy: 0.5267 - val_loss: 1.0903 - val_accuracy: 0.4056\n",
      "Epoch 27/300\n",
      "2022/2022 [==============================] - 0s 180us/sample - loss: 0.9709 - accuracy: 0.5317 - val_loss: 1.0896 - val_accuracy: 0.4111\n",
      "Epoch 28/300\n",
      "2022/2022 [==============================] - 0s 169us/sample - loss: 0.9754 - accuracy: 0.5312 - val_loss: 1.0887 - val_accuracy: 0.4167\n",
      "Epoch 29/300\n",
      "2022/2022 [==============================] - 0s 178us/sample - loss: 0.9686 - accuracy: 0.5371 - val_loss: 1.0880 - val_accuracy: 0.4148\n",
      "Epoch 30/300\n",
      "2022/2022 [==============================] - 0s 180us/sample - loss: 0.9713 - accuracy: 0.5420 - val_loss: 1.0873 - val_accuracy: 0.4167\n",
      "Epoch 31/300\n",
      "2022/2022 [==============================] - 0s 183us/sample - loss: 0.9665 - accuracy: 0.5376 - val_loss: 1.0867 - val_accuracy: 0.4204\n",
      "Epoch 32/300\n",
      "2022/2022 [==============================] - 0s 184us/sample - loss: 0.9632 - accuracy: 0.5376 - val_loss: 1.0861 - val_accuracy: 0.4204\n",
      "Epoch 33/300\n",
      "2022/2022 [==============================] - 0s 197us/sample - loss: 0.9672 - accuracy: 0.5336 - val_loss: 1.0856 - val_accuracy: 0.4241\n",
      "Epoch 34/300\n",
      "2022/2022 [==============================] - 0s 191us/sample - loss: 0.9620 - accuracy: 0.5435 - val_loss: 1.0851 - val_accuracy: 0.4241\n",
      "Epoch 35/300\n",
      "2022/2022 [==============================] - 0s 178us/sample - loss: 0.9574 - accuracy: 0.5425 - val_loss: 1.0845 - val_accuracy: 0.4241\n",
      "Epoch 36/300\n",
      "2022/2022 [==============================] - 0s 185us/sample - loss: 0.9558 - accuracy: 0.5504 - val_loss: 1.0843 - val_accuracy: 0.4278\n",
      "Epoch 37/300\n",
      "2022/2022 [==============================] - 0s 182us/sample - loss: 0.9460 - accuracy: 0.5613 - val_loss: 1.0841 - val_accuracy: 0.4333\n",
      "Epoch 38/300\n",
      "2022/2022 [==============================] - 0s 182us/sample - loss: 0.9536 - accuracy: 0.5391 - val_loss: 1.0835 - val_accuracy: 0.4315\n",
      "Epoch 39/300\n",
      "2022/2022 [==============================] - 0s 190us/sample - loss: 0.9476 - accuracy: 0.5490 - val_loss: 1.0831 - val_accuracy: 0.4333\n",
      "Epoch 40/300\n",
      "2022/2022 [==============================] - 0s 177us/sample - loss: 0.9443 - accuracy: 0.5519 - val_loss: 1.0829 - val_accuracy: 0.4333\n",
      "Epoch 41/300\n",
      "2022/2022 [==============================] - 0s 194us/sample - loss: 0.9408 - accuracy: 0.5569 - val_loss: 1.0827 - val_accuracy: 0.4315\n",
      "Epoch 42/300\n",
      "2022/2022 [==============================] - 0s 183us/sample - loss: 0.9408 - accuracy: 0.5613 - val_loss: 1.0823 - val_accuracy: 0.4352\n",
      "Epoch 43/300\n",
      "2022/2022 [==============================] - 0s 198us/sample - loss: 0.9445 - accuracy: 0.5589 - val_loss: 1.0823 - val_accuracy: 0.4352\n",
      "Epoch 44/300\n",
      "2022/2022 [==============================] - 0s 195us/sample - loss: 0.9409 - accuracy: 0.5524 - val_loss: 1.0822 - val_accuracy: 0.4352\n",
      "Epoch 45/300\n",
      "2022/2022 [==============================] - 0s 197us/sample - loss: 0.9397 - accuracy: 0.5653 - val_loss: 1.0822 - val_accuracy: 0.4333\n",
      "Epoch 46/300\n",
      "2022/2022 [==============================] - 0s 187us/sample - loss: 0.9363 - accuracy: 0.5707 - val_loss: 1.0821 - val_accuracy: 0.4315\n",
      "Epoch 47/300\n",
      "2022/2022 [==============================] - 0s 193us/sample - loss: 0.9317 - accuracy: 0.5707 - val_loss: 1.0821 - val_accuracy: 0.4352\n",
      "Epoch 48/300\n",
      "2022/2022 [==============================] - 0s 186us/sample - loss: 0.9359 - accuracy: 0.5549 - val_loss: 1.0821 - val_accuracy: 0.4333\n",
      "Epoch 49/300\n",
      "2022/2022 [==============================] - 0s 194us/sample - loss: 0.9302 - accuracy: 0.5702 - val_loss: 1.0820 - val_accuracy: 0.4296\n",
      "Epoch 50/300\n",
      "2022/2022 [==============================] - 0s 179us/sample - loss: 0.9267 - accuracy: 0.5737 - val_loss: 1.0820 - val_accuracy: 0.4296\n",
      "Epoch 51/300\n",
      "2022/2022 [==============================] - 0s 182us/sample - loss: 0.9273 - accuracy: 0.5752 - val_loss: 1.0820 - val_accuracy: 0.4241\n",
      "Epoch 52/300\n",
      "2022/2022 [==============================] - 0s 190us/sample - loss: 0.9252 - accuracy: 0.5727 - val_loss: 1.0821 - val_accuracy: 0.4222\n",
      "Epoch 00052: early stopping\n",
      "194/194 [==============================] - 0s 101us/sample - loss: 1.1619 - accuracy: 0.2320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [27:58, 845.42s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:05<00:11,  5.68s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:12<00:05,  5.87s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:14<00:00,  4.95s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 624.1328125 steps, validate for 168.0546875 steps\n",
      "Epoch 1/300\n",
      "625/624 [==============================] - 18s 29ms/step - loss: 1.1418 - accuracy: 0.4274 - val_loss: 1.1368 - val_accuracy: 0.4210\n",
      "Epoch 2/300\n",
      "625/624 [==============================] - 17s 28ms/step - loss: 1.0122 - accuracy: 0.4851 - val_loss: 1.1163 - val_accuracy: 0.4115\n",
      "Epoch 3/300\n",
      "625/624 [==============================] - 17s 28ms/step - loss: 0.9815 - accuracy: 0.5095 - val_loss: 1.1103 - val_accuracy: 0.4163\n",
      "Epoch 4/300\n",
      "625/624 [==============================] - 17s 27ms/step - loss: 0.9619 - accuracy: 0.5276 - val_loss: 1.1068 - val_accuracy: 0.4176\n",
      "Epoch 5/300\n",
      "625/624 [==============================] - 17s 28ms/step - loss: 0.9465 - accuracy: 0.5413 - val_loss: 1.1105 - val_accuracy: 0.4086\n",
      "Epoch 6/300\n",
      "625/624 [==============================] - 17s 28ms/step - loss: 0.9341 - accuracy: 0.5503 - val_loss: 1.1102 - val_accuracy: 0.4116\n",
      "Epoch 7/300\n",
      "625/624 [==============================] - 17s 28ms/step - loss: 0.9229 - accuracy: 0.5581 - val_loss: 1.1070 - val_accuracy: 0.4212\n",
      "Epoch 00007: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 624.1328125 steps, validate for 168.0546875 steps\n",
      "Epoch 1/300\n",
      "625/624 [==============================] - 41s 65ms/step - loss: 0.9028 - accuracy: 0.5738 - val_loss: 1.2117 - val_accuracy: 0.3796\n",
      "Epoch 2/300\n",
      "625/624 [==============================] - 40s 64ms/step - loss: 0.8719 - accuracy: 0.5962 - val_loss: 1.1594 - val_accuracy: 0.4196\n",
      "Epoch 3/300\n",
      "625/624 [==============================] - 40s 64ms/step - loss: 0.8474 - accuracy: 0.6145 - val_loss: 1.1377 - val_accuracy: 0.4366\n",
      "Epoch 4/300\n",
      "625/624 [==============================] - 40s 64ms/step - loss: 0.8260 - accuracy: 0.6274 - val_loss: 1.1444 - val_accuracy: 0.4316\n",
      "Epoch 5/300\n",
      "625/624 [==============================] - 40s 64ms/step - loss: 0.8059 - accuracy: 0.6396 - val_loss: 1.1385 - val_accuracy: 0.4349\n",
      "Epoch 6/300\n",
      "625/624 [==============================] - 40s 63ms/step - loss: 0.7871 - accuracy: 0.6510 - val_loss: 1.1626 - val_accuracy: 0.4216\n",
      "Epoch 00006: early stopping\n",
      "410/410 [==============================] - 0s 779us/sample - loss: 1.1282 - accuracy: 0.4073\n",
      "390/390 [==============================] - 0s 219us/sample - loss: 1.1254 - accuracy: 0.3872\n",
      "387/387 [==============================] - 0s 229us/sample - loss: 1.2306 - accuracy: 0.3282\n",
      "380/380 [==============================] - 0s 209us/sample - loss: 1.1871 - accuracy: 0.3605\n",
      "378/378 [==============================] - 0s 253us/sample - loss: 1.1677 - accuracy: 0.3757\n",
      "371/371 [==============================] - 0s 253us/sample - loss: 1.1871 - accuracy: 0.3693\n",
      "369/369 [==============================] - 0s 251us/sample - loss: 1.2828 - accuracy: 0.3035\n",
      "363/363 [==============================] - 0s 233us/sample - loss: 1.2817 - accuracy: 0.3306\n",
      "354/354 [==============================] - 0s 257us/sample - loss: 1.3512 - accuracy: 0.2712\n",
      "353/353 [==============================] - 0s 241us/sample - loss: 1.2502 - accuracy: 0.3371\n",
      "345/345 [==============================] - 0s 230us/sample - loss: 1.3661 - accuracy: 0.3188\n",
      "336/336 [==============================] - 0s 260us/sample - loss: 1.3862 - accuracy: 0.2976\n",
      "336/336 [==============================] - 0s 242us/sample - loss: 1.3667 - accuracy: 0.2589\n",
      "335/335 [==============================] - 0s 236us/sample - loss: 1.3834 - accuracy: 0.2716\n",
      "337/337 [==============================] - 0s 235us/sample - loss: 1.3559 - accuracy: 0.2997\n",
      "338/338 [==============================] - 0s 254us/sample - loss: 1.3159 - accuracy: 0.2988\n",
      "333/333 [==============================] - 0s 246us/sample - loss: 1.4084 - accuracy: 0.2763\n",
      "330/330 [==============================] - 0s 262us/sample - loss: 1.4691 - accuracy: 0.2455\n",
      "331/331 [==============================] - 0s 273us/sample - loss: 1.4371 - accuracy: 0.2447\n",
      "328/328 [==============================] - 0s 249us/sample - loss: 1.4519 - accuracy: 0.2256\n",
      "324/324 [==============================] - 0s 269us/sample - loss: 1.4362 - accuracy: 0.2531\n",
      "322/322 [==============================] - 0s 232us/sample - loss: 1.4305 - accuracy: 0.2795\n",
      "324/324 [==============================] - 0s 291us/sample - loss: 1.4908 - accuracy: 0.2377\n",
      "323/323 [==============================] - 0s 242us/sample - loss: 1.4405 - accuracy: 0.2910\n",
      "318/318 [==============================] - 0s 220us/sample - loss: 1.4920 - accuracy: 0.1950\n",
      "314/314 [==============================] - 0s 238us/sample - loss: 1.4972 - accuracy: 0.2389\n",
      "313/313 [==============================] - 0s 243us/sample - loss: 1.5024 - accuracy: 0.2268\n",
      "304/304 [==============================] - 0s 251us/sample - loss: 1.5311 - accuracy: 0.2007\n",
      "301/301 [==============================] - 0s 260us/sample - loss: 1.5556 - accuracy: 0.1860\n",
      "299/299 [==============================] - 0s 238us/sample - loss: 1.4232 - accuracy: 0.2609\n",
      "301/301 [==============================] - 0s 269us/sample - loss: 1.4621 - accuracy: 0.2492\n",
      "296/296 [==============================] - 0s 248us/sample - loss: 1.5193 - accuracy: 0.2432\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 1978 samples, validate on 513 samples\n",
      "Epoch 1/300\n",
      "1978/1978 [==============================] - 2s 1ms/sample - loss: 1.1352 - accuracy: 0.3675 - val_loss: 1.1144 - val_accuracy: 0.3801\n",
      "Epoch 2/300\n",
      "1978/1978 [==============================] - 0s 185us/sample - loss: 1.1108 - accuracy: 0.3933 - val_loss: 1.1025 - val_accuracy: 0.3918\n",
      "Epoch 3/300\n",
      "1978/1978 [==============================] - 0s 186us/sample - loss: 1.0907 - accuracy: 0.4004 - val_loss: 1.0947 - val_accuracy: 0.3957\n",
      "Epoch 4/300\n",
      "1978/1978 [==============================] - 0s 187us/sample - loss: 1.0762 - accuracy: 0.4221 - val_loss: 1.0880 - val_accuracy: 0.3996\n",
      "Epoch 5/300\n",
      "1978/1978 [==============================] - 0s 185us/sample - loss: 1.0815 - accuracy: 0.4151 - val_loss: 1.0828 - val_accuracy: 0.4074\n",
      "Epoch 6/300\n",
      "1978/1978 [==============================] - 0s 184us/sample - loss: 1.0582 - accuracy: 0.4479 - val_loss: 1.0783 - val_accuracy: 0.4152\n",
      "Epoch 7/300\n",
      "1978/1978 [==============================] - 0s 187us/sample - loss: 1.0575 - accuracy: 0.4353 - val_loss: 1.0748 - val_accuracy: 0.4230\n",
      "Epoch 8/300\n",
      "1978/1978 [==============================] - 0s 189us/sample - loss: 1.0458 - accuracy: 0.4439 - val_loss: 1.0711 - val_accuracy: 0.4211\n",
      "Epoch 9/300\n",
      "1978/1978 [==============================] - 0s 175us/sample - loss: 1.0460 - accuracy: 0.4540 - val_loss: 1.0681 - val_accuracy: 0.4269\n",
      "Epoch 10/300\n",
      "1978/1978 [==============================] - 0s 178us/sample - loss: 1.0354 - accuracy: 0.4692 - val_loss: 1.0654 - val_accuracy: 0.4269\n",
      "Epoch 11/300\n",
      "1978/1978 [==============================] - 0s 184us/sample - loss: 1.0279 - accuracy: 0.4737 - val_loss: 1.0631 - val_accuracy: 0.4327\n",
      "Epoch 12/300\n",
      "1978/1978 [==============================] - 0s 188us/sample - loss: 1.0232 - accuracy: 0.4848 - val_loss: 1.0609 - val_accuracy: 0.4386\n",
      "Epoch 13/300\n",
      "1978/1978 [==============================] - 0s 195us/sample - loss: 1.0198 - accuracy: 0.4838 - val_loss: 1.0587 - val_accuracy: 0.4386\n",
      "Epoch 14/300\n",
      "1978/1978 [==============================] - 0s 168us/sample - loss: 1.0206 - accuracy: 0.4853 - val_loss: 1.0568 - val_accuracy: 0.4425\n",
      "Epoch 15/300\n",
      "1978/1978 [==============================] - 0s 181us/sample - loss: 1.0106 - accuracy: 0.4869 - val_loss: 1.0551 - val_accuracy: 0.4405\n",
      "Epoch 16/300\n",
      "1978/1978 [==============================] - 0s 191us/sample - loss: 1.0032 - accuracy: 0.4939 - val_loss: 1.0536 - val_accuracy: 0.4425\n",
      "Epoch 17/300\n",
      "1978/1978 [==============================] - 0s 190us/sample - loss: 0.9950 - accuracy: 0.5101 - val_loss: 1.0521 - val_accuracy: 0.4444\n",
      "Epoch 18/300\n",
      "1978/1978 [==============================] - 0s 185us/sample - loss: 0.9958 - accuracy: 0.5071 - val_loss: 1.0509 - val_accuracy: 0.4425\n",
      "Epoch 19/300\n",
      "1978/1978 [==============================] - 0s 189us/sample - loss: 0.9900 - accuracy: 0.5207 - val_loss: 1.0496 - val_accuracy: 0.4444\n",
      "Epoch 20/300\n",
      "1978/1978 [==============================] - 0s 184us/sample - loss: 0.9908 - accuracy: 0.5101 - val_loss: 1.0486 - val_accuracy: 0.4464\n",
      "Epoch 21/300\n",
      "1978/1978 [==============================] - 0s 194us/sample - loss: 0.9832 - accuracy: 0.5192 - val_loss: 1.0476 - val_accuracy: 0.4464\n",
      "Epoch 22/300\n",
      "1978/1978 [==============================] - 0s 201us/sample - loss: 0.9824 - accuracy: 0.5101 - val_loss: 1.0466 - val_accuracy: 0.4483\n",
      "Epoch 23/300\n",
      "1978/1978 [==============================] - 0s 199us/sample - loss: 0.9779 - accuracy: 0.5278 - val_loss: 1.0457 - val_accuracy: 0.4503\n",
      "Epoch 24/300\n",
      "1978/1978 [==============================] - 0s 210us/sample - loss: 0.9652 - accuracy: 0.5410 - val_loss: 1.0447 - val_accuracy: 0.4503\n",
      "Epoch 25/300\n",
      "1978/1978 [==============================] - 0s 195us/sample - loss: 0.9714 - accuracy: 0.5293 - val_loss: 1.0439 - val_accuracy: 0.4503\n",
      "Epoch 26/300\n",
      "1978/1978 [==============================] - 0s 181us/sample - loss: 0.9652 - accuracy: 0.5334 - val_loss: 1.0432 - val_accuracy: 0.4522\n",
      "Epoch 27/300\n",
      "1978/1978 [==============================] - 0s 186us/sample - loss: 0.9692 - accuracy: 0.5384 - val_loss: 1.0427 - val_accuracy: 0.4561\n",
      "Epoch 28/300\n",
      "1978/1978 [==============================] - 0s 191us/sample - loss: 0.9666 - accuracy: 0.5359 - val_loss: 1.0420 - val_accuracy: 0.4542\n",
      "Epoch 29/300\n",
      "1978/1978 [==============================] - 0s 182us/sample - loss: 0.9537 - accuracy: 0.5485 - val_loss: 1.0414 - val_accuracy: 0.4542\n",
      "Epoch 30/300\n",
      "1978/1978 [==============================] - 0s 197us/sample - loss: 0.9515 - accuracy: 0.5415 - val_loss: 1.0408 - val_accuracy: 0.4581\n",
      "Epoch 31/300\n",
      "1978/1978 [==============================] - 0s 199us/sample - loss: 0.9532 - accuracy: 0.5465 - val_loss: 1.0404 - val_accuracy: 0.4581\n",
      "Epoch 32/300\n",
      "1978/1978 [==============================] - 0s 188us/sample - loss: 0.9546 - accuracy: 0.5334 - val_loss: 1.0400 - val_accuracy: 0.4600\n",
      "Epoch 33/300\n",
      "1978/1978 [==============================] - 0s 193us/sample - loss: 0.9475 - accuracy: 0.5354 - val_loss: 1.0396 - val_accuracy: 0.4600\n",
      "Epoch 34/300\n",
      "1978/1978 [==============================] - 0s 207us/sample - loss: 0.9383 - accuracy: 0.5516 - val_loss: 1.0391 - val_accuracy: 0.4620\n",
      "Epoch 35/300\n",
      "1978/1978 [==============================] - 0s 185us/sample - loss: 0.9438 - accuracy: 0.5556 - val_loss: 1.0388 - val_accuracy: 0.4639\n",
      "Epoch 36/300\n",
      "1978/1978 [==============================] - 0s 180us/sample - loss: 0.9414 - accuracy: 0.5450 - val_loss: 1.0387 - val_accuracy: 0.4639\n",
      "Epoch 37/300\n",
      "1978/1978 [==============================] - 0s 205us/sample - loss: 0.9390 - accuracy: 0.5561 - val_loss: 1.0383 - val_accuracy: 0.4659\n",
      "Epoch 38/300\n",
      "1978/1978 [==============================] - 0s 182us/sample - loss: 0.9317 - accuracy: 0.5556 - val_loss: 1.0381 - val_accuracy: 0.4620\n",
      "Epoch 39/300\n",
      "1978/1978 [==============================] - 0s 196us/sample - loss: 0.9333 - accuracy: 0.5647 - val_loss: 1.0380 - val_accuracy: 0.4620\n",
      "Epoch 40/300\n",
      "1978/1978 [==============================] - 0s 186us/sample - loss: 0.9279 - accuracy: 0.5571 - val_loss: 1.0376 - val_accuracy: 0.4678\n",
      "Epoch 41/300\n",
      "1978/1978 [==============================] - 0s 187us/sample - loss: 0.9299 - accuracy: 0.5607 - val_loss: 1.0374 - val_accuracy: 0.4659\n",
      "Epoch 42/300\n",
      "1978/1978 [==============================] - 0s 191us/sample - loss: 0.9219 - accuracy: 0.5698 - val_loss: 1.0373 - val_accuracy: 0.4659\n",
      "Epoch 43/300\n",
      "1978/1978 [==============================] - 0s 194us/sample - loss: 0.9160 - accuracy: 0.5753 - val_loss: 1.0372 - val_accuracy: 0.4659\n",
      "Epoch 44/300\n",
      "1978/1978 [==============================] - 0s 190us/sample - loss: 0.9152 - accuracy: 0.5647 - val_loss: 1.0373 - val_accuracy: 0.4659\n",
      "Epoch 45/300\n",
      "1978/1978 [==============================] - 0s 184us/sample - loss: 0.9087 - accuracy: 0.5794 - val_loss: 1.0372 - val_accuracy: 0.4659\n",
      "Epoch 46/300\n",
      "1978/1978 [==============================] - 0s 201us/sample - loss: 0.9121 - accuracy: 0.5693 - val_loss: 1.0372 - val_accuracy: 0.4659\n",
      "Epoch 47/300\n",
      "1978/1978 [==============================] - 0s 185us/sample - loss: 0.9116 - accuracy: 0.5844 - val_loss: 1.0373 - val_accuracy: 0.4639\n",
      "Epoch 48/300\n",
      "1978/1978 [==============================] - 0s 183us/sample - loss: 0.9136 - accuracy: 0.5723 - val_loss: 1.0372 - val_accuracy: 0.4659\n",
      "Epoch 00048: early stopping\n",
      "265/265 [==============================] - 0s 134us/sample - loss: 1.3072 - accuracy: 0.2340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [40:37, 819.59s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:06<00:12,  6.47s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:12<00:06,  6.43s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.22s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 631.3203125 steps, validate for 176.953125 steps\n",
      "Epoch 1/300\n",
      "632/631 [==============================] - 19s 29ms/step - loss: 1.1637 - accuracy: 0.3964 - val_loss: 1.1151 - val_accuracy: 0.4002\n",
      "Epoch 2/300\n",
      "632/631 [==============================] - 18s 28ms/step - loss: 1.0190 - accuracy: 0.4790 - val_loss: 1.1002 - val_accuracy: 0.4019\n",
      "Epoch 3/300\n",
      "632/631 [==============================] - 18s 28ms/step - loss: 0.9882 - accuracy: 0.5061 - val_loss: 1.0904 - val_accuracy: 0.4073\n",
      "Epoch 4/300\n",
      "632/631 [==============================] - 18s 28ms/step - loss: 0.9680 - accuracy: 0.5232 - val_loss: 1.0978 - val_accuracy: 0.3979\n",
      "Epoch 5/300\n",
      "632/631 [==============================] - 18s 28ms/step - loss: 0.9526 - accuracy: 0.5347 - val_loss: 1.0966 - val_accuracy: 0.4009\n",
      "Epoch 6/300\n",
      "632/631 [==============================] - 18s 28ms/step - loss: 0.9398 - accuracy: 0.5439 - val_loss: 1.1026 - val_accuracy: 0.4019\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 631.3203125 steps, validate for 176.953125 steps\n",
      "Epoch 1/300\n",
      "632/631 [==============================] - 41s 66ms/step - loss: 0.9202 - accuracy: 0.5575 - val_loss: 1.1243 - val_accuracy: 0.4107\n",
      "Epoch 2/300\n",
      "632/631 [==============================] - 40s 64ms/step - loss: 0.8909 - accuracy: 0.5799 - val_loss: 1.1721 - val_accuracy: 0.4231\n",
      "Epoch 3/300\n",
      "632/631 [==============================] - 40s 64ms/step - loss: 0.8685 - accuracy: 0.5964 - val_loss: 1.1314 - val_accuracy: 0.4019\n",
      "Epoch 4/300\n",
      "632/631 [==============================] - 41s 64ms/step - loss: 0.8473 - accuracy: 0.6109 - val_loss: 1.1589 - val_accuracy: 0.3738\n",
      "Epoch 00004: early stopping\n",
      "325/325 [==============================] - 0s 917us/sample - loss: 1.1360 - accuracy: 0.3723\n",
      "312/312 [==============================] - 0s 231us/sample - loss: 1.1051 - accuracy: 0.4295\n",
      "308/308 [==============================] - 0s 255us/sample - loss: 1.1193 - accuracy: 0.4091\n",
      "295/295 [==============================] - 0s 239us/sample - loss: 1.0408 - accuracy: 0.4441\n",
      "292/292 [==============================] - 0s 230us/sample - loss: 0.9930 - accuracy: 0.5171\n",
      "287/287 [==============================] - 0s 249us/sample - loss: 1.0014 - accuracy: 0.4878\n",
      "285/285 [==============================] - 0s 220us/sample - loss: 0.9351 - accuracy: 0.5158\n",
      "282/282 [==============================] - 0s 276us/sample - loss: 0.9719 - accuracy: 0.5248\n",
      "282/282 [==============================] - 0s 233us/sample - loss: 0.9883 - accuracy: 0.4858\n",
      "280/280 [==============================] - 0s 222us/sample - loss: 0.9997 - accuracy: 0.4857\n",
      "279/279 [==============================] - 0s 226us/sample - loss: 0.9938 - accuracy: 0.4839\n",
      "278/278 [==============================] - 0s 233us/sample - loss: 0.9941 - accuracy: 0.4640\n",
      "278/278 [==============================] - 0s 227us/sample - loss: 0.9908 - accuracy: 0.4856\n",
      "270/270 [==============================] - 0s 282us/sample - loss: 0.9896 - accuracy: 0.5074\n",
      "269/269 [==============================] - 0s 246us/sample - loss: 0.9715 - accuracy: 0.4870\n",
      "265/265 [==============================] - 0s 265us/sample - loss: 0.9704 - accuracy: 0.5019\n",
      "267/267 [==============================] - 0s 237us/sample - loss: 1.0387 - accuracy: 0.4644\n",
      "269/269 [==============================] - 0s 233us/sample - loss: 0.9952 - accuracy: 0.4796\n",
      "267/267 [==============================] - 0s 242us/sample - loss: 1.0316 - accuracy: 0.4419\n",
      "268/268 [==============================] - 0s 255us/sample - loss: 0.9947 - accuracy: 0.4664\n",
      "265/265 [==============================] - 0s 235us/sample - loss: 1.0247 - accuracy: 0.4491\n",
      "263/263 [==============================] - 0s 223us/sample - loss: 1.0740 - accuracy: 0.4144\n",
      "261/261 [==============================] - 0s 246us/sample - loss: 1.0382 - accuracy: 0.4521\n",
      "264/264 [==============================] - 0s 236us/sample - loss: 1.0171 - accuracy: 0.4583\n",
      "263/263 [==============================] - 0s 260us/sample - loss: 1.0099 - accuracy: 0.4487\n",
      "265/265 [==============================] - 0s 250us/sample - loss: 1.0561 - accuracy: 0.4189\n",
      "262/262 [==============================] - 0s 247us/sample - loss: 1.0564 - accuracy: 0.4198\n",
      "260/260 [==============================] - 0s 239us/sample - loss: 1.0371 - accuracy: 0.4269\n",
      "259/259 [==============================] - 0s 240us/sample - loss: 1.0646 - accuracy: 0.4247\n",
      "255/255 [==============================] - 0s 232us/sample - loss: 1.0419 - accuracy: 0.4784\n",
      "256/256 [==============================] - 0s 988us/sample - loss: 0.9919 - accuracy: 0.4922\n",
      "253/253 [==============================] - 0s 229us/sample - loss: 1.0040 - accuracy: 0.4545\n",
      "Saved_feature_train\n",
      "Saved_feature_test\n",
      "Saved_feature_validation\n",
      "Train on 1980 samples, validate on 543 samples\n",
      "Epoch 1/300\n",
      "1980/1980 [==============================] - 3s 1ms/sample - loss: 1.1480 - accuracy: 0.3596 - val_loss: 1.0708 - val_accuracy: 0.3996\n",
      "Epoch 2/300\n",
      "1980/1980 [==============================] - 0s 190us/sample - loss: 1.1214 - accuracy: 0.3753 - val_loss: 1.0651 - val_accuracy: 0.4125\n",
      "Epoch 3/300\n",
      "1980/1980 [==============================] - 0s 181us/sample - loss: 1.1200 - accuracy: 0.3631 - val_loss: 1.0612 - val_accuracy: 0.4199\n",
      "Epoch 4/300\n",
      "1980/1980 [==============================] - 0s 202us/sample - loss: 1.1021 - accuracy: 0.3778 - val_loss: 1.0585 - val_accuracy: 0.4162\n",
      "Epoch 5/300\n",
      "1980/1980 [==============================] - 0s 200us/sample - loss: 1.1032 - accuracy: 0.3843 - val_loss: 1.0567 - val_accuracy: 0.4125\n",
      "Epoch 6/300\n",
      "1980/1980 [==============================] - 0s 183us/sample - loss: 1.0899 - accuracy: 0.4066 - val_loss: 1.0552 - val_accuracy: 0.4180\n",
      "Epoch 7/300\n",
      "1980/1980 [==============================] - 0s 196us/sample - loss: 1.0815 - accuracy: 0.4035 - val_loss: 1.0540 - val_accuracy: 0.4217\n",
      "Epoch 8/300\n",
      "1980/1980 [==============================] - 0s 189us/sample - loss: 1.0731 - accuracy: 0.4207 - val_loss: 1.0532 - val_accuracy: 0.4180\n",
      "Epoch 9/300\n",
      "1980/1980 [==============================] - 0s 192us/sample - loss: 1.0759 - accuracy: 0.3965 - val_loss: 1.0524 - val_accuracy: 0.4180\n",
      "Epoch 10/300\n",
      "1980/1980 [==============================] - 0s 192us/sample - loss: 1.0702 - accuracy: 0.4258 - val_loss: 1.0519 - val_accuracy: 0.4199\n",
      "Epoch 11/300\n",
      "1980/1980 [==============================] - 0s 221us/sample - loss: 1.0684 - accuracy: 0.4162 - val_loss: 1.0513 - val_accuracy: 0.4162\n",
      "Epoch 12/300\n",
      "1980/1980 [==============================] - 0s 194us/sample - loss: 1.0577 - accuracy: 0.4177 - val_loss: 1.0508 - val_accuracy: 0.4180\n",
      "Epoch 13/300\n",
      "1980/1980 [==============================] - 0s 191us/sample - loss: 1.0540 - accuracy: 0.4364 - val_loss: 1.0504 - val_accuracy: 0.4180\n",
      "Epoch 14/300\n",
      "1980/1980 [==============================] - 0s 183us/sample - loss: 1.0527 - accuracy: 0.4394 - val_loss: 1.0502 - val_accuracy: 0.4180\n",
      "Epoch 15/300\n",
      "1980/1980 [==============================] - 0s 179us/sample - loss: 1.0558 - accuracy: 0.4131 - val_loss: 1.0500 - val_accuracy: 0.4162\n",
      "Epoch 16/300\n",
      "1980/1980 [==============================] - 0s 181us/sample - loss: 1.0444 - accuracy: 0.4404 - val_loss: 1.0500 - val_accuracy: 0.4199\n",
      "Epoch 17/300\n",
      "1980/1980 [==============================] - 0s 180us/sample - loss: 1.0385 - accuracy: 0.4434 - val_loss: 1.0499 - val_accuracy: 0.4199\n",
      "Epoch 18/300\n",
      "1980/1980 [==============================] - 0s 179us/sample - loss: 1.0475 - accuracy: 0.4439 - val_loss: 1.0499 - val_accuracy: 0.4199\n",
      "Epoch 19/300\n",
      "1980/1980 [==============================] - 0s 182us/sample - loss: 1.0393 - accuracy: 0.4505 - val_loss: 1.0499 - val_accuracy: 0.4273\n",
      "Epoch 20/300\n",
      "1980/1980 [==============================] - 0s 176us/sample - loss: 1.0282 - accuracy: 0.4692 - val_loss: 1.0498 - val_accuracy: 0.4254\n",
      "Epoch 21/300\n",
      "1980/1980 [==============================] - 0s 178us/sample - loss: 1.0302 - accuracy: 0.4505 - val_loss: 1.0499 - val_accuracy: 0.4236\n",
      "Epoch 22/300\n",
      "1980/1980 [==============================] - 0s 187us/sample - loss: 1.0280 - accuracy: 0.4722 - val_loss: 1.0500 - val_accuracy: 0.4144\n",
      "Epoch 23/300\n",
      "1980/1980 [==============================] - 0s 186us/sample - loss: 1.0251 - accuracy: 0.4611 - val_loss: 1.0502 - val_accuracy: 0.4180\n",
      "Epoch 00023: early stopping\n",
      "233/233 [==============================] - 0s 98us/sample - loss: 1.0168 - accuracy: 0.4206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [51:57, 777.73s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:06<00:12,  6.44s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [00:12<00:06,  6.44s/it]\u001b[A\n",
      "100%|██████████| 3/3 [00:15<00:00,  5.25s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "for num_ix, rand_num in enumerate(rand):\n",
    "    for index_t_well, _ in tqdm.tqdm(enumerate(tot_well)):\n",
    "\n",
    "        time_points = list(map(str, range(1,97,3)))\n",
    "\n",
    "        new_time = []\n",
    "        for i in time_points:\n",
    "            r = '_' + i + '.'\n",
    "            new_time.append(r)\n",
    "\n",
    "\n",
    "\n",
    "        path_test = '/home/jovyan/DATA_MASTER_PROJECT/Check_DIFF_T0_T97/{}_cropped/'.format(a)\n",
    "\n",
    "        # NAME OF THE WELLS CORRESPONDING TO THE DRUG THAT YOU WANT IN THE TEST SET \n",
    "\n",
    "        wells_drug = [tot_well[index_t_well][0], tot_well[index_t_well][1]] \n",
    "\n",
    "        test = []\n",
    "\n",
    "        for _,_, filenames in os.walk(path_test):\n",
    "\n",
    "            for filename in sorted(filenames, key = natural_keys):\n",
    "\n",
    "                for w in wells_drug:\n",
    "                    for t in new_time:\n",
    "                        if '{}'.format(w) in filename and '{}tiff'.format(t) in filename:\n",
    "                            test.append(filename)\n",
    "\n",
    "        groups_list = ['{}'.format(a), '{}'.format(b), '{}'.format(c)]\n",
    "\n",
    "        fileds_of_view = ['1','2','3','4']\n",
    "\n",
    "        field_train, field_val = train_test_split(fileds_of_view, test_size=0.2, random_state=rand_num)\n",
    "\n",
    "\n",
    "        train = []\n",
    "\n",
    "        validation = []\n",
    "\n",
    "        group_compounds = []\n",
    "\n",
    "        for group in tqdm.tqdm(groups_list):\n",
    "\n",
    "            pa = '/home/jovyan/DATA_MASTER_PROJECT/Check_DIFF_T0_T97/{}_cropped/'.format(group)\n",
    "\n",
    "            for _,_, filenames in os.walk(pa):\n",
    "\n",
    "                for filename in sorted(filenames, key = natural_keys):\n",
    "\n",
    "                    for t in new_time:\n",
    "\n",
    "                        if '_{}-'.format(wells_drug[0]) not in filename and '_{}-'.format(wells_drug[1]) not in filename and '{}tiff'.format(t) in filename:\n",
    "\n",
    "                            group_compounds.append(filename)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for i in group_compounds:\n",
    "\n",
    "            for f in field_train:\n",
    "                if '-{}_'.format(f) in i:\n",
    "                    train.append(i)\n",
    "\n",
    "\n",
    "            for v in field_val:\n",
    "                if '-{}_'.format(v) in i:\n",
    "                    validation.append(i)\n",
    "\n",
    "\n",
    "        x_train = loadImages(train)\n",
    "        y_train = make_labels(train)\n",
    "\n",
    "\n",
    "\n",
    "        x_val = loadImages(validation)\n",
    "        y_val = make_labels(validation)\n",
    "\n",
    "\n",
    "\n",
    "        x_train = resize(x_train)\n",
    "\n",
    "\n",
    "        x_val = resize(x_val)\n",
    "\n",
    "\n",
    "        weights = class_weight.compute_class_weight('balanced', np.unique(y_train),y_train)\n",
    "\n",
    "\n",
    "        x_train = preprocess_input(x_train)\n",
    "\n",
    "        x_val = preprocess_input(x_val)\n",
    "        \n",
    "        y_train = keras.utils.to_categorical(y_train)\n",
    "        y_val = keras.utils.to_categorical(y_val)\n",
    "\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=3)\n",
    "\n",
    "        pretrained_model = VGG16(weights='imagenet',include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "        base_model = Model(inputs=pretrained_model.input, outputs=pretrained_model.get_layer('block3_pool').output)\n",
    "\n",
    "        batch_size = 128\n",
    "\n",
    "        datagen = ImageDataGenerator()\n",
    "\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "        train_gen = datagen.flow(x_train, y_train,batch_size=batch_size )\n",
    "\n",
    "        dat_val = ImageDataGenerator()\n",
    "\n",
    "        dat_val.fit(x_val)\n",
    "\n",
    "        val_gen = dat_val.flow(x_val, y_val,batch_size=batch_size)\n",
    "\n",
    "        m4 = Sequential()\n",
    "        m4.add(base_model)\n",
    "\n",
    "\n",
    "        m4.add(BatchNormalization())\n",
    "        m4.add(GlobalAveragePooling2D())\n",
    "        m4.add(Dense(64, activation='relu'))\n",
    "        m4.add(BatchNormalization())\n",
    "        m4.add(Activation('relu'))\n",
    "        m4.add(Dense(3,activation='softmax'))\n",
    "\n",
    "\n",
    "        base_model.trainable = False\n",
    "\n",
    "        opt = keras.optimizers.Adam(lr=1e-4)\n",
    "\n",
    "        m4.compile(loss= keras.losses.categorical_crossentropy, optimizer=opt, metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "        epochs = 300\n",
    "\n",
    "        m4_h = m4.fit(train_gen,\n",
    "                        steps_per_epoch=(len(x_train)/batch_size),\n",
    "                        callbacks = [es],\n",
    "                        epochs=epochs,\n",
    "                        validation_data = (val_gen), \n",
    "                        validation_steps = (len(x_val)/batch_size),\n",
    "                        class_weight = weights,\n",
    "                         verbose = 1)\n",
    "\n",
    "        base_model.trainable = True\n",
    "\n",
    "        opt = keras.optimizers.Adam(lr=1e-5)\n",
    "\n",
    "        m4.compile(loss= keras.losses.categorical_crossentropy, optimizer=opt, metrics = ['accuracy'])\n",
    "\n",
    "        epochs = 300\n",
    "\n",
    "        m4_h = m4.fit(train_gen,\n",
    "                        steps_per_epoch=(len(x_train)/batch_size),\n",
    "                        callbacks = [es],\n",
    "                        epochs=epochs,\n",
    "                        validation_data = val_gen, \n",
    "                        validation_steps = (len(x_val)/batch_size),\n",
    "                        class_weight = weights,\n",
    "                        verbose = 1)\n",
    "\n",
    "        l = []\n",
    "        for t in new_time:\n",
    "            for i in test:\n",
    "                if t in i:\n",
    "                    l.append((i))\n",
    "\n",
    "\n",
    "        grouped = {}\n",
    "        for elem in l:\n",
    "            key = elem.split('.tiff')[0].split('_')[5]\n",
    "            grouped.setdefault(key, []).append(elem)\n",
    "        grouped = grouped.values()\n",
    "\n",
    "        test_data = list(grouped)\n",
    "\n",
    "        r = []\n",
    "\n",
    "        for ix ,_ in enumerate(test_data):\n",
    "            r.append(time_step_acc(test_data[ix],m4))\n",
    "\n",
    "        plt.plot(time_points,r)\n",
    "        plt.savefig('/home/jovyan/{}_accuracy.png'.format(string_well[index_t_well]))\n",
    "\n",
    "        tot_results_accuracy.append(r)\n",
    "        \n",
    "        for i, layer in enumerate(m4.layers):\n",
    "            layer._name = 'layer_' + str(i)\n",
    "\n",
    "\n",
    "\n",
    "        lstm_model = Model(inputs=m4.input, outputs=m4.get_layer('layer_4').output)\n",
    "\n",
    "        data_name = [train,test,validation]\n",
    "\n",
    "        feat_name = ['train', 'test', 'validation']\n",
    "\n",
    "        for index_name, _ in enumerate(data_name):\n",
    "\n",
    "            path =  data_name[index_name]\n",
    "\n",
    "            name_well = []\n",
    "\n",
    "            for i in path:\n",
    "                name_well.append(i.split('_id')[0])\n",
    "\n",
    "            wells = list(set(name_well))\n",
    "            wells\n",
    "\n",
    "            for w in wells:\n",
    "\n",
    "                time = []\n",
    "\n",
    "\n",
    "                for filename in sorted(path, key = natural_keys):\n",
    "                    if w in filename: #PAY ATTENTION ID THE IMAGE IS A TIFF OR PNG IMAGE #########\n",
    "                        time.append(filename)\n",
    "\n",
    "                data_id = {}\n",
    "                n_id = []\n",
    "                w_n = []\n",
    "\n",
    "                for i in time:\n",
    "                    t = i.split('_id_')[1].split('time_')[0]\n",
    "                    f = i.split('_id_')[0].split('time_')[0]\n",
    "                    n_id.append(t)\n",
    "                    w_n.append(f)\n",
    "\n",
    "                id_cell = set(n_id)\n",
    "\n",
    "\n",
    "                for ix, i in enumerate(sorted(id_cell, key = natural_keys)):\n",
    "\n",
    "                    id_name = []\n",
    "                    dict_1 = {}\n",
    "\n",
    "                    for t in time:\n",
    "                        if 'id_{}'.format(i) in t:\n",
    "                            id_name.append(t)\n",
    "\n",
    "                    d = {'id':id_name}\n",
    "                    data = pd.DataFrame(d)\n",
    "\n",
    "                    dict_1[ix]=data \n",
    "                    data_id.update(dict_1) \n",
    "\n",
    "                delete = [i for i, j in data_id.items() if len(j) < 32] # 9 or the length of time span you are traning on \n",
    "                for i in delete : del data_id[i]\n",
    "\n",
    "                len_id = [i for i, j in data_id.items()]\n",
    "\n",
    "                for le in len_id:    \n",
    "\n",
    "\n",
    "                    e = pd.DataFrame(data_id[le])\n",
    "\n",
    "                    coords = e.values.tolist()\n",
    "                    id_cells = []\n",
    "                    for i in coords:\n",
    "                        for j in i:\n",
    "                            id_cells.append(j)\n",
    "\n",
    "                    x_orig = loadImages(id_cells)\n",
    "                    x_orig = resize(x_orig)\n",
    "\n",
    "                    x_orig = preprocess_input(x_orig)\n",
    "                    output = lstm_model.predict(x_orig)\n",
    "                    np.save('/home/jovyan/DATA_MASTER_PROJECT/LSTM//FEAT_FOLDERS/features_{}/features_well_{}_id_{}.npy'.format(feat_name[index_name],w_n[0], le), output)\n",
    "            print('Saved_feature_{}'.format(feat_name[index_name]))\n",
    "\n",
    "\n",
    "        x_train_lstm = loadImages_LSTM(train_data)\n",
    "        y_train_lstm = make_labels_LSTM(y_tra_path)\n",
    "\n",
    "        x_test_lstm = loadImages_LSTM(tes_data)\n",
    "        y_test_lstm = make_labels_LSTM(y_tes_path)\n",
    "\n",
    "        x_val_lstm = loadImages_LSTM(val_data)\n",
    "        y_val_lstm = make_labels_LSTM(y_val_path)\n",
    "        \n",
    "        weights_lstm = class_weight.compute_class_weight('balanced', np.unique(y_train_lstm),y_train_lstm)\n",
    "        \n",
    "        y_train_lstm = keras.utils.to_categorical(y_train_lstm)\n",
    "        y_test_1_lstm = keras.utils.to_categorical(y_test_lstm, num_classes= 3)\n",
    "        y_val_lstm = keras.utils.to_categorical(y_val_lstm)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        m = Sequential()\n",
    "        m.add(LSTM(32, input_shape = (x_train_lstm.shape[1],x_train_lstm.shape[2])))\n",
    "        m.add(Dropout(0.2))\n",
    "        m.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "        opt_lstm = keras.optimizers.Adam(lr=1e-4)\n",
    "\n",
    "        m.compile(loss= keras.losses.categorical_crossentropy, optimizer=opt, metrics = ['accuracy'])\n",
    "\n",
    "        epochs = 300\n",
    "\n",
    "        m_h = m.fit(x_train_lstm,y_train_lstm,\n",
    "\n",
    "                         callbacks = [es],\n",
    "\n",
    "                        epochs=epochs,\n",
    "                        validation_data = (x_val_lstm,y_val_lstm), \n",
    "\n",
    "                        class_weight = weights_lstm)\n",
    "\n",
    "\n",
    "        scores_lstm = m.evaluate(x_test_lstm, y_test_1_lstm)\n",
    "        results_lstm.append([scores_lstm[1]*100, string_well[index_t_well]])\n",
    "\n",
    "        # DELITE FILES IN FEATURE VECTOR FOLDERS\n",
    "\n",
    "        folders = glob.glob('/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/*')\n",
    "\n",
    "        for fo in folders:\n",
    "            file = glob.glob(f'{fo}/*')\n",
    "            for f in file:\n",
    "                os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCURACY SCORE AVERAGE FOR CNN\n",
    "cv_s = cv_mean_acc(tot_results_accuracy, string_well)\n",
    "cv_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT OF MEAN ACCURACY FOR EVERY TIME POINT CNN\n",
    "\n",
    "l_drug = string_well*3\n",
    "\n",
    "acc_plot = []\n",
    "\n",
    "for i in tot_results_accuracy:\n",
    "    acc_plot.append(i)\n",
    "\n",
    "cv_plot = list(zip(acc_plot, l_drug))\n",
    "\n",
    "res_plot = sorted(cv_plot, key = lambda x: x[1])\n",
    "\n",
    "a , b = zip(*res_plot)\n",
    "    \n",
    "a = list(a)\n",
    "\n",
    "s = list(np.array_split(a, 5))\n",
    "\n",
    "cv_plot = []\n",
    "\n",
    "for ix, i in enumerate(s):\n",
    "    s1 = list(s[ix])\n",
    "    \n",
    "    cv_plot.append(np.average(s1, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "for i in cv_plot:\n",
    "    \n",
    "    plt.plot(time_points, i)\n",
    "    plt.show\n",
    "    plt.savefig('/home/jovyan/cv_score.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lstm = sorted(results_lstm, key=lambda x: x[1])\n",
    "r_lstm , _ = zip(*results_lstm)\n",
    "    \n",
    "r_lstm = list(r_lstm)\n",
    "\n",
    "re_lstm = list(np.array_split(r_lstm, 5))\n",
    "\n",
    "cv_lstm = []\n",
    "\n",
    "for ix, i in enumerate(re_lstm):\n",
    "    r1 = list(re_lstm[ix])\n",
    "    cv_lstm.append(np.mean(r1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_lstm = list(zip(cv_lstm, string_well))\n",
    "cv_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE RUNNING AGAIN\n",
    "\n",
    "folders = glob.glob('/home/jovyan/DATA_MASTER_PROJECT/LSTM/FEAT_FOLDERS/*')\n",
    "\n",
    "for fo in folders:\n",
    "    file = glob.glob(f'{fo}/*')\n",
    "    for f in file:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
